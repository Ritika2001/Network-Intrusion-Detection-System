{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN, Bidirectional\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "traindata = pd.read_csv('/home/ritika/NetworkIntrusionDetection/UNSW-NB15 - CSV Files/a part of training and testing set/UNSW_NB15_training-set.csv')\n",
    "testdata = pd.read_csv('/home/ritika/NetworkIntrusionDetection/UNSW-NB15 - CSV Files/a part of training and testing set/UNSW_NB15_testing-set.csv')\n",
    "\n",
    "traindata=traindata.drop('id',axis=1)\n",
    "testdata=testdata.drop('id',axis=1)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb_make = LabelEncoder()\n",
    "\n",
    "traindata['proto'] = lb_make.fit_transform(traindata['proto'])\n",
    "traindata['service'] = lb_make.fit_transform(traindata['service'])\n",
    "traindata['state'] = lb_make.fit_transform(traindata['state'])\n",
    "\n",
    "testdata['proto'] = lb_make.fit_transform(testdata['proto'])\n",
    "testdata['service'] = lb_make.fit_transform(testdata['service'])\n",
    "testdata['state'] = lb_make.fit_transform(testdata['state'])\n",
    "\n",
    "X = traindata.iloc[:,1:42]\n",
    "Y = traindata.iloc[:,-1]\n",
    "x = testdata.iloc[:,1:42]\n",
    "y = testdata.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arr = X.values\n",
    "arr = x.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "Arr_Delta = librosa.feature.delta(Arr)\n",
    "arr_delta = librosa.feature.delta(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Delta1 = pd.DataFrame(Arr_Delta)\n",
    "delta1 = pd.DataFrame(arr_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Normalizer().fit(Delta1)\n",
    "Delta1 = scaler.transform(Delta1)\n",
    "scaler = Normalizer().fit(delta1)\n",
    "delta1 = scaler.transform(delta1)\n",
    "\n",
    "Y = np.array(Y)\n",
    "y = np.array(y)\n",
    "\n",
    "Delta1 = np.reshape(Delta1, (Delta1.shape[0],Delta1.shape[1],1))\n",
    "delta1 = np.reshape(delta1, (delta1.shape[0],delta1.shape[1],1))\n",
    "\n",
    "\n",
    "scaler = Normalizer().fit(X)\n",
    "X = scaler.transform(X)\n",
    "scaler = Normalizer().fit(x)\n",
    "x = scaler.transform(x)\n",
    "\n",
    "Y = np.array(Y)\n",
    "y = np.array(y)\n",
    "\n",
    "X = np.reshape(X, (X.shape[0],X.shape[1],1))\n",
    "x = np.reshape(x, (x.shape[0],x.shape[1],1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82332, 41, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritika/.virtualenvs/project/lib/python3.5/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", padding=\"same\")`\n",
      "  \n",
      "/home/ritika/.virtualenvs/project/lib/python3.5/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "  if __name__ == '__main__':\n",
      "/home/ritika/.virtualenvs/project/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", padding=\"same\")`\n",
      "  if sys.path[0] == '':\n",
      "/home/ritika/.virtualenvs/project/lib/python3.5/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 41, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 41, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 41, 64)       256         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 41, 64)       256         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 20, 64)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 20, 64)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 20, 128)      0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 140)          111440      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 140)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            141         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 112,093\n",
      "Trainable params: 112,093\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 175341 samples, validate on 82332 samples\n",
      "Epoch 1/200\n",
      "175341/175341 [==============================] - 177s 1ms/step - loss: 0.4137 - accuracy: 0.7671 - val_loss: 0.5516 - val_accuracy: 0.6860\n",
      "Epoch 2/200\n",
      "175341/175341 [==============================] - 176s 1ms/step - loss: 0.3747 - accuracy: 0.8148 - val_loss: 0.5475 - val_accuracy: 0.6885\n",
      "Epoch 3/200\n",
      "175341/175341 [==============================] - 176s 1ms/step - loss: 0.3645 - accuracy: 0.8216 - val_loss: 0.5620 - val_accuracy: 0.6911\n",
      "Epoch 4/200\n",
      "175341/175341 [==============================] - 177s 1ms/step - loss: 0.3621 - accuracy: 0.8225 - val_loss: 0.5535 - val_accuracy: 0.6880\n",
      "Epoch 5/200\n",
      "175341/175341 [==============================] - 177s 1ms/step - loss: 0.3594 - accuracy: 0.8244 - val_loss: 0.5430 - val_accuracy: 0.7202\n",
      "Epoch 6/200\n",
      "175341/175341 [==============================] - 176s 1ms/step - loss: 0.3577 - accuracy: 0.8253 - val_loss: 0.5554 - val_accuracy: 0.7299\n",
      "Epoch 7/200\n",
      "175341/175341 [==============================] - 178s 1ms/step - loss: 0.3558 - accuracy: 0.8261 - val_loss: 0.5424 - val_accuracy: 0.7203\n",
      "Epoch 8/200\n",
      "175341/175341 [==============================] - 178s 1ms/step - loss: 0.3536 - accuracy: 0.8280 - val_loss: 0.5412 - val_accuracy: 0.7186\n",
      "Epoch 9/200\n",
      "175341/175341 [==============================] - 178s 1ms/step - loss: 0.3479 - accuracy: 0.8341 - val_loss: 0.5422 - val_accuracy: 0.7230\n",
      "Epoch 10/200\n",
      "175341/175341 [==============================] - 178s 1ms/step - loss: 0.3278 - accuracy: 0.8524 - val_loss: 0.5268 - val_accuracy: 0.7220\n",
      "Epoch 11/200\n",
      "175341/175341 [==============================] - 179s 1ms/step - loss: 0.3189 - accuracy: 0.8593 - val_loss: 0.5220 - val_accuracy: 0.7293\n",
      "Epoch 12/200\n",
      "175341/175341 [==============================] - 179s 1ms/step - loss: 0.3140 - accuracy: 0.8625 - val_loss: 0.5274 - val_accuracy: 0.7275\n",
      "Epoch 13/200\n",
      "175341/175341 [==============================] - 180s 1ms/step - loss: 0.3100 - accuracy: 0.8650 - val_loss: 0.5240 - val_accuracy: 0.7303\n",
      "Epoch 14/200\n",
      "175341/175341 [==============================] - 179s 1ms/step - loss: 0.3048 - accuracy: 0.8662 - val_loss: 0.5101 - val_accuracy: 0.7341\n",
      "Epoch 15/200\n",
      "175341/175341 [==============================] - 179s 1ms/step - loss: 0.3009 - accuracy: 0.8670 - val_loss: 0.5322 - val_accuracy: 0.7152\n",
      "Epoch 16/200\n",
      "175341/175341 [==============================] - 179s 1ms/step - loss: 0.2980 - accuracy: 0.8678 - val_loss: 0.4993 - val_accuracy: 0.7270\n",
      "Epoch 17/200\n",
      "175341/175341 [==============================] - 180s 1ms/step - loss: 0.2965 - accuracy: 0.8677 - val_loss: 0.5163 - val_accuracy: 0.7310\n",
      "Epoch 18/200\n",
      "175341/175341 [==============================] - 180s 1ms/step - loss: 0.2943 - accuracy: 0.8686 - val_loss: 0.4950 - val_accuracy: 0.7288\n",
      "Epoch 19/200\n",
      "175341/175341 [==============================] - 181s 1ms/step - loss: 0.2937 - accuracy: 0.8683 - val_loss: 0.5079 - val_accuracy: 0.7289\n",
      "Epoch 20/200\n",
      "175341/175341 [==============================] - 181s 1ms/step - loss: 0.2919 - accuracy: 0.8692 - val_loss: 0.5265 - val_accuracy: 0.7277\n",
      "Epoch 21/200\n",
      "175341/175341 [==============================] - 179s 1ms/step - loss: 0.2908 - accuracy: 0.8692 - val_loss: 0.5212 - val_accuracy: 0.7281\n",
      "Epoch 22/200\n",
      "175341/175341 [==============================] - 178s 1ms/step - loss: 0.2897 - accuracy: 0.8697 - val_loss: 0.5099 - val_accuracy: 0.7335\n",
      "Epoch 23/200\n",
      "175341/175341 [==============================] - 176s 1ms/step - loss: 0.2885 - accuracy: 0.8702 - val_loss: 0.5229 - val_accuracy: 0.7334\n",
      "Epoch 24/200\n",
      "175341/175341 [==============================] - 176s 1ms/step - loss: 0.2882 - accuracy: 0.8701 - val_loss: 0.5152 - val_accuracy: 0.7314\n",
      "Epoch 25/200\n",
      "175341/175341 [==============================] - 176s 1ms/step - loss: 0.2869 - accuracy: 0.8702 - val_loss: 0.4976 - val_accuracy: 0.7355\n",
      "Epoch 26/200\n",
      "175341/175341 [==============================] - 176s 1ms/step - loss: 0.2869 - accuracy: 0.8713 - val_loss: 0.5246 - val_accuracy: 0.7319\n",
      "Epoch 27/200\n",
      "175341/175341 [==============================] - 177s 1ms/step - loss: 0.2867 - accuracy: 0.8708 - val_loss: 0.5200 - val_accuracy: 0.7316\n",
      "Epoch 28/200\n",
      "175341/175341 [==============================] - 176s 1ms/step - loss: 0.2856 - accuracy: 0.8716 - val_loss: 0.5028 - val_accuracy: 0.7256\n",
      "Epoch 29/200\n",
      "175341/175341 [==============================] - 176s 1ms/step - loss: 0.2842 - accuracy: 0.8722 - val_loss: 0.4936 - val_accuracy: 0.7280\n",
      "Epoch 30/200\n",
      "175341/175341 [==============================] - 177s 1ms/step - loss: 0.2849 - accuracy: 0.8721 - val_loss: 0.5046 - val_accuracy: 0.7348\n",
      "Epoch 31/200\n",
      "175341/175341 [==============================] - 177s 1ms/step - loss: 0.2843 - accuracy: 0.8719 - val_loss: 0.4906 - val_accuracy: 0.7324\n",
      "Epoch 32/200\n",
      "175341/175341 [==============================] - 176s 1ms/step - loss: 0.2840 - accuracy: 0.8719 - val_loss: 0.5009 - val_accuracy: 0.7342\n",
      "Epoch 33/200\n",
      "175341/175341 [==============================] - 177s 1ms/step - loss: 0.2833 - accuracy: 0.8720 - val_loss: 0.5256 - val_accuracy: 0.7322\n",
      "Epoch 34/200\n",
      "175341/175341 [==============================] - 176s 1ms/step - loss: 0.2836 - accuracy: 0.8723 - val_loss: 0.5143 - val_accuracy: 0.7347\n",
      "Epoch 35/200\n",
      "175341/175341 [==============================] - 161s 916us/step - loss: 0.2832 - accuracy: 0.8723 - val_loss: 0.5228 - val_accuracy: 0.7281\n",
      "Epoch 36/200\n",
      "175341/175341 [==============================] - 161s 916us/step - loss: 0.2821 - accuracy: 0.8728 - val_loss: 0.4956 - val_accuracy: 0.7323\n",
      "Epoch 37/200\n",
      "175341/175341 [==============================] - 161s 917us/step - loss: 0.2818 - accuracy: 0.8727 - val_loss: 0.5014 - val_accuracy: 0.7354\n",
      "Epoch 38/200\n",
      "175341/175341 [==============================] - 160s 915us/step - loss: 0.2821 - accuracy: 0.8726 - val_loss: 0.5049 - val_accuracy: 0.7297\n",
      "Epoch 39/200\n",
      "175341/175341 [==============================] - 161s 921us/step - loss: 0.2819 - accuracy: 0.8719 - val_loss: 0.4954 - val_accuracy: 0.7344\n",
      "Epoch 40/200\n",
      "175341/175341 [==============================] - 163s 930us/step - loss: 0.2818 - accuracy: 0.8724 - val_loss: 0.5049 - val_accuracy: 0.7356\n",
      "Epoch 41/200\n",
      "175341/175341 [==============================] - 169s 962us/step - loss: 0.2812 - accuracy: 0.8729 - val_loss: 0.4925 - val_accuracy: 0.7316\n",
      "Epoch 42/200\n",
      "175341/175341 [==============================] - 169s 965us/step - loss: 0.2809 - accuracy: 0.8732 - val_loss: 0.5001 - val_accuracy: 0.7352\n",
      "Epoch 43/200\n",
      "175341/175341 [==============================] - 171s 972us/step - loss: 0.2805 - accuracy: 0.8728 - val_loss: 0.4895 - val_accuracy: 0.7348\n",
      "Epoch 44/200\n",
      "175341/175341 [==============================] - 170s 972us/step - loss: 0.2808 - accuracy: 0.8729 - val_loss: 0.5085 - val_accuracy: 0.7267\n",
      "Epoch 45/200\n",
      "175341/175341 [==============================] - 171s 976us/step - loss: 0.2802 - accuracy: 0.8730 - val_loss: 0.4958 - val_accuracy: 0.7340\n",
      "Epoch 46/200\n",
      "175341/175341 [==============================] - 172s 979us/step - loss: 0.2798 - accuracy: 0.8732 - val_loss: 0.4808 - val_accuracy: 0.7356\n",
      "Epoch 47/200\n",
      "175341/175341 [==============================] - 172s 982us/step - loss: 0.2792 - accuracy: 0.8733 - val_loss: 0.5148 - val_accuracy: 0.7291\n",
      "Epoch 48/200\n",
      "175341/175341 [==============================] - 173s 984us/step - loss: 0.2791 - accuracy: 0.8740 - val_loss: 0.5045 - val_accuracy: 0.7358\n",
      "Epoch 49/200\n",
      "175341/175341 [==============================] - 173s 985us/step - loss: 0.2795 - accuracy: 0.8731 - val_loss: 0.4864 - val_accuracy: 0.7351\n",
      "Epoch 50/200\n",
      "175341/175341 [==============================] - 173s 988us/step - loss: 0.2790 - accuracy: 0.8740 - val_loss: 0.5151 - val_accuracy: 0.7348\n",
      "Epoch 51/200\n",
      "175341/175341 [==============================] - 174s 992us/step - loss: 0.2787 - accuracy: 0.8741 - val_loss: 0.5027 - val_accuracy: 0.7282\n",
      "Epoch 52/200\n",
      "175341/175341 [==============================] - 174s 992us/step - loss: 0.2786 - accuracy: 0.8734 - val_loss: 0.5056 - val_accuracy: 0.7307\n",
      "Epoch 53/200\n",
      "175341/175341 [==============================] - 170s 969us/step - loss: 0.2790 - accuracy: 0.8740 - val_loss: 0.4993 - val_accuracy: 0.7358\n",
      "Epoch 54/200\n",
      "175341/175341 [==============================] - 155s 883us/step - loss: 0.2782 - accuracy: 0.8737 - val_loss: 0.5196 - val_accuracy: 0.7338\n",
      "Epoch 55/200\n",
      "175341/175341 [==============================] - 157s 898us/step - loss: 0.2785 - accuracy: 0.8736 - val_loss: 0.4942 - val_accuracy: 0.7347\n",
      "Epoch 56/200\n",
      "175341/175341 [==============================] - 196s 1ms/step - loss: 0.2782 - accuracy: 0.8741 - val_loss: 0.4976 - val_accuracy: 0.7351\n",
      "Epoch 57/200\n",
      "175341/175341 [==============================] - 169s 963us/step - loss: 0.2769 - accuracy: 0.8739 - val_loss: 0.4944 - val_accuracy: 0.7354\n",
      "Epoch 58/200\n",
      "175341/175341 [==============================] - 206s 1ms/step - loss: 0.2777 - accuracy: 0.8736 - val_loss: 0.5011 - val_accuracy: 0.7355\n",
      "Epoch 59/200\n",
      "175341/175341 [==============================] - 211s 1ms/step - loss: 0.2767 - accuracy: 0.8736 - val_loss: 0.4858 - val_accuracy: 0.7341\n",
      "Epoch 60/200\n",
      "175341/175341 [==============================] - 225s 1ms/step - loss: 0.2757 - accuracy: 0.8737 - val_loss: 0.4900 - val_accuracy: 0.7323\n",
      "Epoch 61/200\n",
      "175341/175341 [==============================] - 223s 1ms/step - loss: 0.2756 - accuracy: 0.8741 - val_loss: 0.4851 - val_accuracy: 0.7355\n",
      "Epoch 62/200\n",
      "175341/175341 [==============================] - 266s 2ms/step - loss: 0.2744 - accuracy: 0.8742 - val_loss: 0.5088 - val_accuracy: 0.7288\n",
      "Epoch 63/200\n",
      "175341/175341 [==============================] - 286s 2ms/step - loss: 0.2737 - accuracy: 0.8747 - val_loss: 0.5152 - val_accuracy: 0.7355\n",
      "Epoch 64/200\n",
      "175341/175341 [==============================] - 300s 2ms/step - loss: 0.2739 - accuracy: 0.8739 - val_loss: 0.4714 - val_accuracy: 0.7353\n",
      "Epoch 65/200\n",
      "175341/175341 [==============================] - 306s 2ms/step - loss: 0.2735 - accuracy: 0.8741 - val_loss: 0.4858 - val_accuracy: 0.7352\n",
      "Epoch 66/200\n",
      "175341/175341 [==============================] - 297s 2ms/step - loss: 0.2729 - accuracy: 0.8746 - val_loss: 0.4478 - val_accuracy: 0.7342\n",
      "Epoch 67/200\n",
      "175341/175341 [==============================] - 295s 2ms/step - loss: 0.2731 - accuracy: 0.8740 - val_loss: 0.4419 - val_accuracy: 0.7423\n",
      "Epoch 68/200\n",
      "175341/175341 [==============================] - 298s 2ms/step - loss: 0.2728 - accuracy: 0.8748 - val_loss: 0.4713 - val_accuracy: 0.7357\n",
      "Epoch 69/200\n",
      "175341/175341 [==============================] - 267s 2ms/step - loss: 0.2714 - accuracy: 0.8752 - val_loss: 0.4628 - val_accuracy: 0.7335\n",
      "Epoch 70/200\n",
      "175341/175341 [==============================] - 268s 2ms/step - loss: 0.2706 - accuracy: 0.8756 - val_loss: 0.4711 - val_accuracy: 0.7369\n",
      "Epoch 71/200\n",
      "175341/175341 [==============================] - 258s 1ms/step - loss: 0.2713 - accuracy: 0.8746 - val_loss: 0.4458 - val_accuracy: 0.7312\n",
      "Epoch 72/200\n",
      "175341/175341 [==============================] - 254s 1ms/step - loss: 0.2706 - accuracy: 0.8750 - val_loss: 0.4528 - val_accuracy: 0.7318\n",
      "Epoch 73/200\n",
      "175341/175341 [==============================] - 293s 2ms/step - loss: 0.2706 - accuracy: 0.8750 - val_loss: 0.4743 - val_accuracy: 0.7353\n",
      "Epoch 74/200\n",
      "175341/175341 [==============================] - 299s 2ms/step - loss: 0.2695 - accuracy: 0.8751 - val_loss: 0.4603 - val_accuracy: 0.7349\n",
      "Epoch 75/200\n",
      "175341/175341 [==============================] - 277s 2ms/step - loss: 0.2702 - accuracy: 0.8753 - val_loss: 0.4380 - val_accuracy: 0.7365\n",
      "Epoch 76/200\n",
      "175341/175341 [==============================] - 204s 1ms/step - loss: 0.2708 - accuracy: 0.8748 - val_loss: 0.4344 - val_accuracy: 0.7417\n",
      "Epoch 77/200\n",
      "175341/175341 [==============================] - 181s 1ms/step - loss: 0.2687 - accuracy: 0.8762 - val_loss: 0.4923 - val_accuracy: 0.7355\n",
      "Epoch 78/200\n",
      "175341/175341 [==============================] - 160s 910us/step - loss: 0.2692 - accuracy: 0.8757 - val_loss: 0.4562 - val_accuracy: 0.7432\n",
      "Epoch 79/200\n",
      "175341/175341 [==============================] - 183s 1ms/step - loss: 0.2685 - accuracy: 0.8753 - val_loss: 0.4257 - val_accuracy: 0.7522\n",
      "Epoch 80/200\n",
      "175341/175341 [==============================] - 188s 1ms/step - loss: 0.2683 - accuracy: 0.8758 - val_loss: 0.4802 - val_accuracy: 0.7404\n",
      "Epoch 81/200\n",
      "175341/175341 [==============================] - 186s 1ms/step - loss: 0.2688 - accuracy: 0.8753 - val_loss: 0.4516 - val_accuracy: 0.7420\n",
      "Epoch 82/200\n",
      "175341/175341 [==============================] - 187s 1ms/step - loss: 0.2678 - accuracy: 0.8759 - val_loss: 0.4366 - val_accuracy: 0.7467\n",
      "Epoch 83/200\n",
      "175341/175341 [==============================] - 185s 1ms/step - loss: 0.2685 - accuracy: 0.8761 - val_loss: 0.4231 - val_accuracy: 0.7541\n",
      "Epoch 84/200\n",
      "175341/175341 [==============================] - 185s 1ms/step - loss: 0.2673 - accuracy: 0.8761 - val_loss: 0.4302 - val_accuracy: 0.7548\n",
      "Epoch 85/200\n",
      "175341/175341 [==============================] - 185s 1ms/step - loss: 0.2669 - accuracy: 0.8762 - val_loss: 0.4239 - val_accuracy: 0.7528\n",
      "Epoch 86/200\n",
      "175341/175341 [==============================] - 183s 1ms/step - loss: 0.2674 - accuracy: 0.8763 - val_loss: 0.4265 - val_accuracy: 0.7538\n",
      "Epoch 87/200\n",
      "175341/175341 [==============================] - 185s 1ms/step - loss: 0.2679 - accuracy: 0.8753 - val_loss: 0.4587 - val_accuracy: 0.7477\n",
      "Epoch 88/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175341/175341 [==============================] - 168s 956us/step - loss: 0.2670 - accuracy: 0.8761 - val_loss: 0.4528 - val_accuracy: 0.7418\n",
      "Epoch 89/200\n",
      "175341/175341 [==============================] - 168s 961us/step - loss: 0.2666 - accuracy: 0.8767 - val_loss: 0.4741 - val_accuracy: 0.7333\n",
      "Epoch 90/200\n",
      "175341/175341 [==============================] - 169s 963us/step - loss: 0.2671 - accuracy: 0.8767 - val_loss: 0.4446 - val_accuracy: 0.7431\n",
      "Epoch 91/200\n",
      "175341/175341 [==============================] - 170s 968us/step - loss: 0.2663 - accuracy: 0.8768 - val_loss: 0.4570 - val_accuracy: 0.7549\n",
      "Epoch 92/200\n",
      "175341/175341 [==============================] - 170s 967us/step - loss: 0.2670 - accuracy: 0.8760 - val_loss: 0.4310 - val_accuracy: 0.7358\n",
      "Epoch 93/200\n",
      "175341/175341 [==============================] - 170s 969us/step - loss: 0.2666 - accuracy: 0.8765 - val_loss: 0.4447 - val_accuracy: 0.7544\n",
      "Epoch 94/200\n",
      "175341/175341 [==============================] - 170s 968us/step - loss: 0.2664 - accuracy: 0.8761 - val_loss: 0.4323 - val_accuracy: 0.7559\n",
      "Epoch 95/200\n",
      "175341/175341 [==============================] - 171s 976us/step - loss: 0.2673 - accuracy: 0.8759 - val_loss: 0.4404 - val_accuracy: 0.7435\n",
      "Epoch 96/200\n",
      "175341/175341 [==============================] - 171s 973us/step - loss: 0.2666 - accuracy: 0.8768 - val_loss: 0.4675 - val_accuracy: 0.7431\n",
      "Epoch 97/200\n",
      "175341/175341 [==============================] - 172s 982us/step - loss: 0.2666 - accuracy: 0.8766 - val_loss: 0.4491 - val_accuracy: 0.7418\n",
      "Epoch 98/200\n",
      "175341/175341 [==============================] - 174s 992us/step - loss: 0.2662 - accuracy: 0.8767 - val_loss: 0.4273 - val_accuracy: 0.7563\n",
      "Epoch 99/200\n",
      "175341/175341 [==============================] - 176s 1ms/step - loss: 0.2664 - accuracy: 0.8767 - val_loss: 0.4526 - val_accuracy: 0.7549\n",
      "Epoch 100/200\n",
      "175341/175341 [==============================] - 175s 1ms/step - loss: 0.2655 - accuracy: 0.8767 - val_loss: 0.4330 - val_accuracy: 0.7564\n",
      "Epoch 101/200\n",
      "175341/175341 [==============================] - 175s 1000us/step - loss: 0.2658 - accuracy: 0.8765 - val_loss: 0.4559 - val_accuracy: 0.7430\n",
      "Epoch 102/200\n",
      "175341/175341 [==============================] - 177s 1ms/step - loss: 0.2652 - accuracy: 0.8773 - val_loss: 0.4325 - val_accuracy: 0.7571\n",
      "Epoch 103/200\n",
      "175341/175341 [==============================] - 177s 1ms/step - loss: 0.2645 - accuracy: 0.8775 - val_loss: 0.4131 - val_accuracy: 0.7559\n",
      "Epoch 104/200\n",
      "175341/175341 [==============================] - 177s 1ms/step - loss: 0.2663 - accuracy: 0.8763 - val_loss: 0.4514 - val_accuracy: 0.7327\n",
      "Epoch 105/200\n",
      "175341/175341 [==============================] - 178s 1ms/step - loss: 0.2650 - accuracy: 0.8775 - val_loss: 0.4391 - val_accuracy: 0.7419\n",
      "Epoch 106/200\n",
      "175341/175341 [==============================] - 178s 1ms/step - loss: 0.2650 - accuracy: 0.8771 - val_loss: 0.4429 - val_accuracy: 0.7450\n",
      "Epoch 107/200\n",
      "175341/175341 [==============================] - 178s 1ms/step - loss: 0.2643 - accuracy: 0.8778 - val_loss: 0.4757 - val_accuracy: 0.7437\n",
      "Epoch 108/200\n",
      "175341/175341 [==============================] - 179s 1ms/step - loss: 0.2640 - accuracy: 0.8777 - val_loss: 0.4292 - val_accuracy: 0.7585\n",
      "Epoch 109/200\n",
      "175341/175341 [==============================] - 179s 1ms/step - loss: 0.2642 - accuracy: 0.8779 - val_loss: 0.4374 - val_accuracy: 0.7563\n",
      "Epoch 110/200\n",
      "175341/175341 [==============================] - 178s 1ms/step - loss: 0.2647 - accuracy: 0.8771 - val_loss: 0.4324 - val_accuracy: 0.7511\n",
      "Epoch 111/200\n",
      "175341/175341 [==============================] - 180s 1ms/step - loss: 0.2637 - accuracy: 0.8774 - val_loss: 0.4592 - val_accuracy: 0.7362\n",
      "Epoch 112/200\n",
      "175341/175341 [==============================] - 180s 1ms/step - loss: 0.2635 - accuracy: 0.8778 - val_loss: 0.4779 - val_accuracy: 0.7441\n",
      "Epoch 113/200\n",
      "175341/175341 [==============================] - 180s 1ms/step - loss: 0.2641 - accuracy: 0.8780 - val_loss: 0.4747 - val_accuracy: 0.7443\n",
      "Epoch 114/200\n",
      "175341/175341 [==============================] - 181s 1ms/step - loss: 0.2649 - accuracy: 0.8774 - val_loss: 0.4398 - val_accuracy: 0.7433\n",
      "Epoch 115/200\n",
      "175341/175341 [==============================] - 181s 1ms/step - loss: 0.2630 - accuracy: 0.8783 - val_loss: 0.4432 - val_accuracy: 0.7561\n",
      "Epoch 116/200\n",
      "175341/175341 [==============================] - 181s 1ms/step - loss: 0.2638 - accuracy: 0.8776 - val_loss: 0.4295 - val_accuracy: 0.7577\n",
      "Epoch 117/200\n",
      "175341/175341 [==============================] - 182s 1ms/step - loss: 0.2636 - accuracy: 0.8779 - val_loss: 0.4171 - val_accuracy: 0.7534\n",
      "Epoch 118/200\n",
      "175341/175341 [==============================] - 177s 1ms/step - loss: 0.2631 - accuracy: 0.8785 - val_loss: 0.4291 - val_accuracy: 0.7454\n",
      "Epoch 119/200\n",
      "175341/175341 [==============================] - 177s 1ms/step - loss: 0.2630 - accuracy: 0.8780 - val_loss: 0.4178 - val_accuracy: 0.7576\n",
      "Epoch 120/200\n",
      "175341/175341 [==============================] - 177s 1ms/step - loss: 0.2618 - accuracy: 0.8788 - val_loss: 0.4477 - val_accuracy: 0.7472\n",
      "Epoch 121/200\n",
      "175341/175341 [==============================] - 178s 1ms/step - loss: 0.2645 - accuracy: 0.8774 - val_loss: 0.4281 - val_accuracy: 0.7590\n",
      "Epoch 122/200\n",
      "175341/175341 [==============================] - 177s 1ms/step - loss: 0.2637 - accuracy: 0.8775 - val_loss: 0.4241 - val_accuracy: 0.7556\n",
      "Epoch 123/200\n",
      "175341/175341 [==============================] - 178s 1ms/step - loss: 0.2628 - accuracy: 0.8781 - val_loss: 0.4505 - val_accuracy: 0.7443\n",
      "Epoch 124/200\n",
      "175341/175341 [==============================] - 177s 1ms/step - loss: 0.2622 - accuracy: 0.8790 - val_loss: 0.4555 - val_accuracy: 0.7448\n",
      "Epoch 125/200\n",
      "175341/175341 [==============================] - 176s 1ms/step - loss: 0.2606 - accuracy: 0.8794 - val_loss: 0.4542 - val_accuracy: 0.7472\n",
      "Epoch 126/200\n",
      "175341/175341 [==============================] - 177s 1ms/step - loss: 0.2623 - accuracy: 0.8791 - val_loss: 0.4443 - val_accuracy: 0.7376\n",
      "Epoch 127/200\n",
      "175341/175341 [==============================] - 178s 1ms/step - loss: 0.2618 - accuracy: 0.8791 - val_loss: 0.4511 - val_accuracy: 0.7414\n",
      "Epoch 128/200\n",
      "175341/175341 [==============================] - 177s 1ms/step - loss: 0.2621 - accuracy: 0.8789 - val_loss: 0.4355 - val_accuracy: 0.7394\n",
      "Epoch 129/200\n",
      "175341/175341 [==============================] - 178s 1ms/step - loss: 0.2617 - accuracy: 0.8791 - val_loss: 0.4501 - val_accuracy: 0.7483\n",
      "Epoch 130/200\n",
      "175341/175341 [==============================] - 178s 1ms/step - loss: 0.2617 - accuracy: 0.8795 - val_loss: 0.4579 - val_accuracy: 0.7445\n",
      "Epoch 131/200\n",
      "175341/175341 [==============================] - 177s 1ms/step - loss: 0.2618 - accuracy: 0.8790 - val_loss: 0.4357 - val_accuracy: 0.7503\n",
      "Epoch 132/200\n",
      "175341/175341 [==============================] - 178s 1ms/step - loss: 0.2590 - accuracy: 0.8804 - val_loss: 0.4633 - val_accuracy: 0.7397\n",
      "Epoch 133/200\n",
      "175341/175341 [==============================] - 178s 1ms/step - loss: 0.2608 - accuracy: 0.8791 - val_loss: 0.4217 - val_accuracy: 0.7478\n",
      "Epoch 134/200\n",
      "175341/175341 [==============================] - 182s 1ms/step - loss: 0.2617 - accuracy: 0.8790 - val_loss: 0.4103 - val_accuracy: 0.7404\n",
      "Epoch 135/200\n",
      "175341/175341 [==============================] - 186s 1ms/step - loss: 0.2611 - accuracy: 0.8789 - val_loss: 0.4741 - val_accuracy: 0.7458\n",
      "Epoch 136/200\n",
      "175341/175341 [==============================] - 186s 1ms/step - loss: 0.2608 - accuracy: 0.8798 - val_loss: 0.4298 - val_accuracy: 0.7564\n",
      "Epoch 137/200\n",
      "175341/175341 [==============================] - 186s 1ms/step - loss: 0.2596 - accuracy: 0.8803 - val_loss: 0.4708 - val_accuracy: 0.7432\n",
      "Epoch 138/200\n",
      "175341/175341 [==============================] - 186s 1ms/step - loss: 0.2604 - accuracy: 0.8799 - val_loss: 0.4390 - val_accuracy: 0.7369\n",
      "Epoch 139/200\n",
      "175341/175341 [==============================] - 186s 1ms/step - loss: 0.2619 - accuracy: 0.8780 - val_loss: 0.4213 - val_accuracy: 0.7464\n",
      "Epoch 140/200\n",
      "175341/175341 [==============================] - 187s 1ms/step - loss: 0.2610 - accuracy: 0.8799 - val_loss: 0.4431 - val_accuracy: 0.7452\n",
      "Epoch 141/200\n",
      "175341/175341 [==============================] - 168s 958us/step - loss: 0.2616 - accuracy: 0.8793 - val_loss: 0.4365 - val_accuracy: 0.7425\n",
      "Epoch 142/200\n",
      "175341/175341 [==============================] - 169s 961us/step - loss: 0.2621 - accuracy: 0.8778 - val_loss: 0.4427 - val_accuracy: 0.7416\n",
      "Epoch 143/200\n",
      "175341/175341 [==============================] - 169s 964us/step - loss: 0.2598 - accuracy: 0.8802 - val_loss: 0.4077 - val_accuracy: 0.7413\n",
      "Epoch 144/200\n",
      "175341/175341 [==============================] - 169s 962us/step - loss: 0.2597 - accuracy: 0.8800 - val_loss: 0.4434 - val_accuracy: 0.7438\n",
      "Epoch 145/200\n",
      "175341/175341 [==============================] - 170s 971us/step - loss: 0.2593 - accuracy: 0.8808 - val_loss: 0.4381 - val_accuracy: 0.7445\n",
      "Epoch 146/200\n",
      "175341/175341 [==============================] - 170s 967us/step - loss: 0.2596 - accuracy: 0.8798 - val_loss: 0.4481 - val_accuracy: 0.7438\n",
      "Epoch 147/200\n",
      "175341/175341 [==============================] - 170s 972us/step - loss: 0.2605 - accuracy: 0.8799 - val_loss: 0.4387 - val_accuracy: 0.7434\n",
      "Epoch 148/200\n",
      "175341/175341 [==============================] - 171s 973us/step - loss: 0.2598 - accuracy: 0.8804 - val_loss: 0.4286 - val_accuracy: 0.7467\n",
      "Epoch 149/200\n",
      "175341/175341 [==============================] - 172s 979us/step - loss: 0.2608 - accuracy: 0.8795 - val_loss: 0.4749 - val_accuracy: 0.7462\n",
      "Epoch 150/200\n",
      "175341/175341 [==============================] - 171s 976us/step - loss: 0.2588 - accuracy: 0.8799 - val_loss: 0.4362 - val_accuracy: 0.7444\n",
      "Epoch 151/200\n",
      "175341/175341 [==============================] - 172s 984us/step - loss: 0.2590 - accuracy: 0.8805 - val_loss: 0.4168 - val_accuracy: 0.7405\n",
      "Epoch 152/200\n",
      "175341/175341 [==============================] - 173s 987us/step - loss: 0.2598 - accuracy: 0.8799 - val_loss: 0.4383 - val_accuracy: 0.7440\n",
      "Epoch 153/200\n",
      "175341/175341 [==============================] - 172s 983us/step - loss: 0.2588 - accuracy: 0.8812 - val_loss: 0.4437 - val_accuracy: 0.7462\n",
      "Epoch 154/200\n",
      "175341/175341 [==============================] - 175s 997us/step - loss: 0.2585 - accuracy: 0.8806 - val_loss: 0.4515 - val_accuracy: 0.7421\n",
      "Epoch 155/200\n",
      "175341/175341 [==============================] - 170s 969us/step - loss: 0.2583 - accuracy: 0.8805 - val_loss: 0.4022 - val_accuracy: 0.7584\n",
      "Epoch 156/200\n",
      "175341/175341 [==============================] - 170s 972us/step - loss: 0.2584 - accuracy: 0.8810 - val_loss: 0.4439 - val_accuracy: 0.7472\n",
      "Epoch 157/200\n",
      "175341/175341 [==============================] - 170s 968us/step - loss: 0.2602 - accuracy: 0.8786 - val_loss: 0.4330 - val_accuracy: 0.7468\n",
      "Epoch 158/200\n",
      "175341/175341 [==============================] - 171s 975us/step - loss: 0.2604 - accuracy: 0.8796 - val_loss: 0.4675 - val_accuracy: 0.7369\n",
      "Epoch 159/200\n",
      "175341/175341 [==============================] - 170s 969us/step - loss: 0.2597 - accuracy: 0.8790 - val_loss: 0.4553 - val_accuracy: 0.7371\n",
      "Epoch 160/200\n",
      "175341/175341 [==============================] - 171s 973us/step - loss: 0.2595 - accuracy: 0.8804 - val_loss: 0.4463 - val_accuracy: 0.7487\n",
      "Epoch 161/200\n",
      "175341/175341 [==============================] - 170s 972us/step - loss: 0.2576 - accuracy: 0.8814 - val_loss: 0.4757 - val_accuracy: 0.7452\n",
      "Epoch 162/200\n",
      "175341/175341 [==============================] - 170s 969us/step - loss: 0.2584 - accuracy: 0.8801 - val_loss: 0.4243 - val_accuracy: 0.7377\n",
      "Epoch 163/200\n",
      "175341/175341 [==============================] - 171s 976us/step - loss: 0.2600 - accuracy: 0.8798 - val_loss: 0.4580 - val_accuracy: 0.7451\n",
      "Epoch 164/200\n",
      "175341/175341 [==============================] - 170s 972us/step - loss: 0.2565 - accuracy: 0.8821 - val_loss: 0.4409 - val_accuracy: 0.7417\n",
      "Epoch 165/200\n",
      "175341/175341 [==============================] - 171s 978us/step - loss: 0.2579 - accuracy: 0.8810 - val_loss: 0.4437 - val_accuracy: 0.7448\n",
      "Epoch 166/200\n",
      "175341/175341 [==============================] - 170s 972us/step - loss: 0.2587 - accuracy: 0.8806 - val_loss: 0.4313 - val_accuracy: 0.7444\n",
      "Epoch 167/200\n",
      "175341/175341 [==============================] - 171s 976us/step - loss: 0.2572 - accuracy: 0.8816 - val_loss: 0.4331 - val_accuracy: 0.7477\n",
      "Epoch 168/200\n",
      "175341/175341 [==============================] - 171s 974us/step - loss: 0.2577 - accuracy: 0.8810 - val_loss: 0.4369 - val_accuracy: 0.7431\n",
      "Epoch 169/200\n",
      "175341/175341 [==============================] - 171s 975us/step - loss: 0.2586 - accuracy: 0.8810 - val_loss: 0.4609 - val_accuracy: 0.7425\n",
      "Epoch 170/200\n",
      "175341/175341 [==============================] - 171s 977us/step - loss: 0.2579 - accuracy: 0.8810 - val_loss: 0.4142 - val_accuracy: 0.7420\n",
      "Epoch 171/200\n",
      "175341/175341 [==============================] - 171s 974us/step - loss: 0.2582 - accuracy: 0.8813 - val_loss: 0.4269 - val_accuracy: 0.7489\n",
      "Epoch 172/200\n",
      "175341/175341 [==============================] - 172s 981us/step - loss: 0.2585 - accuracy: 0.8808 - val_loss: 0.4386 - val_accuracy: 0.7453\n",
      "Epoch 173/200\n",
      "175341/175341 [==============================] - 171s 978us/step - loss: 0.2572 - accuracy: 0.8820 - val_loss: 0.4120 - val_accuracy: 0.7593\n",
      "Epoch 174/200\n",
      "175341/175341 [==============================] - 172s 983us/step - loss: 0.2589 - accuracy: 0.8800 - val_loss: 0.4192 - val_accuracy: 0.7470\n",
      "Epoch 175/200\n",
      "175341/175341 [==============================] - 171s 977us/step - loss: 0.2558 - accuracy: 0.8829 - val_loss: 0.4326 - val_accuracy: 0.7460\n",
      "Epoch 176/200\n",
      "175341/175341 [==============================] - 173s 985us/step - loss: 0.2582 - accuracy: 0.8799 - val_loss: 0.4376 - val_accuracy: 0.7438\n",
      "Epoch 177/200\n",
      "175341/175341 [==============================] - 172s 980us/step - loss: 0.2574 - accuracy: 0.8815 - val_loss: 0.4778 - val_accuracy: 0.7455\n",
      "Epoch 178/200\n",
      "175341/175341 [==============================] - 171s 977us/step - loss: 0.2576 - accuracy: 0.8809 - val_loss: 0.4400 - val_accuracy: 0.7455\n",
      "Epoch 179/200\n",
      "175341/175341 [==============================] - 172s 982us/step - loss: 0.2572 - accuracy: 0.8819 - val_loss: 0.4144 - val_accuracy: 0.7638\n",
      "Epoch 180/200\n",
      "175341/175341 [==============================] - 172s 980us/step - loss: 0.2558 - accuracy: 0.8817 - val_loss: 0.4385 - val_accuracy: 0.7459\n",
      "Epoch 181/200\n",
      "175341/175341 [==============================] - 172s 984us/step - loss: 0.2574 - accuracy: 0.8814 - val_loss: 0.4406 - val_accuracy: 0.7382\n",
      "Epoch 182/200\n",
      "175341/175341 [==============================] - 172s 982us/step - loss: 0.2558 - accuracy: 0.8820 - val_loss: 0.4275 - val_accuracy: 0.7444\n",
      "Epoch 183/200\n",
      "175341/175341 [==============================] - 172s 981us/step - loss: 0.2579 - accuracy: 0.8806 - val_loss: 0.4639 - val_accuracy: 0.7454\n",
      "Epoch 184/200\n",
      "175341/175341 [==============================] - 173s 986us/step - loss: 0.2571 - accuracy: 0.8810 - val_loss: 0.4550 - val_accuracy: 0.7427\n",
      "Epoch 185/200\n",
      "175341/175341 [==============================] - 172s 980us/step - loss: 0.2601 - accuracy: 0.8793 - val_loss: 0.4866 - val_accuracy: 0.7406\n",
      "Epoch 186/200\n",
      "175341/175341 [==============================] - 173s 984us/step - loss: 0.2564 - accuracy: 0.8826 - val_loss: 0.4704 - val_accuracy: 0.7456\n",
      "Epoch 187/200\n",
      "175341/175341 [==============================] - 172s 982us/step - loss: 0.2568 - accuracy: 0.8815 - val_loss: 0.4412 - val_accuracy: 0.7438\n",
      "Epoch 188/200\n",
      "175341/175341 [==============================] - 173s 984us/step - loss: 0.2563 - accuracy: 0.8822 - val_loss: 0.4429 - val_accuracy: 0.7494\n",
      "Epoch 189/200\n",
      "175341/175341 [==============================] - 173s 988us/step - loss: 0.2571 - accuracy: 0.8808 - val_loss: 0.4320 - val_accuracy: 0.7385\n",
      "Epoch 190/200\n",
      "175341/175341 [==============================] - 173s 986us/step - loss: 0.2553 - accuracy: 0.8823 - val_loss: 0.4293 - val_accuracy: 0.7451\n",
      "Epoch 191/200\n",
      "175341/175341 [==============================] - 174s 990us/step - loss: 0.2554 - accuracy: 0.8826 - val_loss: 0.4321 - val_accuracy: 0.7349\n",
      "Epoch 192/200\n",
      "175341/175341 [==============================] - 172s 983us/step - loss: 0.2562 - accuracy: 0.8820 - val_loss: 0.4702 - val_accuracy: 0.7330\n",
      "Epoch 193/200\n",
      "175341/175341 [==============================] - 174s 992us/step - loss: 0.2564 - accuracy: 0.8817 - val_loss: 0.4439 - val_accuracy: 0.7428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194/200\n",
      "175341/175341 [==============================] - 163s 932us/step - loss: 0.2572 - accuracy: 0.8806 - val_loss: 0.4474 - val_accuracy: 0.7375\n",
      "Epoch 195/200\n",
      "175341/175341 [==============================] - 163s 932us/step - loss: 0.2578 - accuracy: 0.8802 - val_loss: 0.4229 - val_accuracy: 0.7447\n",
      "Epoch 196/200\n",
      "175341/175341 [==============================] - 164s 936us/step - loss: 0.2533 - accuracy: 0.8828 - val_loss: 0.4360 - val_accuracy: 0.7457\n",
      "Epoch 197/200\n",
      "175341/175341 [==============================] - 165s 943us/step - loss: 0.2560 - accuracy: 0.8812 - val_loss: 0.4433 - val_accuracy: 0.7493\n",
      "Epoch 198/200\n",
      "175341/175341 [==============================] - 164s 936us/step - loss: 0.2556 - accuracy: 0.8814 - val_loss: 0.4868 - val_accuracy: 0.7384\n",
      "Epoch 199/200\n",
      "175341/175341 [==============================] - 165s 938us/step - loss: 0.2556 - accuracy: 0.8812 - val_loss: 0.4781 - val_accuracy: 0.7472\n",
      "Epoch 200/200\n",
      "175341/175341 [==============================] - 164s 937us/step - loss: 0.2555 - accuracy: 0.8807 - val_loss: 0.4269 - val_accuracy: 0.7563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f018e43cba8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Convolution1D, Dense, Dropout, Flatten, MaxPooling1D, concatenate, Reshape\n",
    "from keras.models import Model\n",
    "\n",
    "lstm_output_size=70\n",
    "Xshape = Input(shape=(41, 1))\n",
    "Delta1shape = Input(shape=(41, 1))\n",
    "\n",
    "x1 = Convolution1D(64, 3, border_mode=\"same\",activation=\"relu\")(Xshape)\n",
    "x1 = MaxPooling1D(pool_length=(2))(x1)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "   \n",
    "x2 = Convolution1D(64, 3, border_mode=\"same\",activation=\"relu\")(Delta1shape)\n",
    "x2 = MaxPooling1D(pool_length=(2))(x2)\n",
    "x2 = Dropout(0.5)(x2)\n",
    "\n",
    "output = concatenate([x1, x2])\n",
    "\n",
    "x3 = Bidirectional(LSTM(lstm_output_size))(output)\n",
    "x3 = Dropout(0.1)(x3)\n",
    "x3 = Dense(1, activation=\"sigmoid\")(x3)\n",
    "\n",
    "model = Model([Xshape, Delta1shape],x3)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit([X,Delta1], Y, epochs=200, validation_data=([x,delta1], y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 200/200\n",
    "175341/175341 [==============================] - 164s 937us/step - loss: 0.2555 - accuracy: 0.8807 - val_loss: 0.4269 - val_accuracy: 0.7563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
