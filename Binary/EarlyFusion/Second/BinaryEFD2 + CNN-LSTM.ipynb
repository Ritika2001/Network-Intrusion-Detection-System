{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN, Bidirectional\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "traindata = pd.read_csv('/home/ritika/NetworkIntrusionDetection/UNSW-NB15 - CSV Files/a part of training and testing set/UNSW_NB15_training-set.csv')\n",
    "testdata = pd.read_csv('/home/ritika/NetworkIntrusionDetection/UNSW-NB15 - CSV Files/a part of training and testing set/UNSW_NB15_testing-set.csv')\n",
    "\n",
    "traindata=traindata.drop('id',axis=1)\n",
    "testdata=testdata.drop('id',axis=1)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb_make = LabelEncoder()\n",
    "\n",
    "traindata['proto'] = lb_make.fit_transform(traindata['proto'])\n",
    "traindata['service'] = lb_make.fit_transform(traindata['service'])\n",
    "traindata['state'] = lb_make.fit_transform(traindata['state'])\n",
    "\n",
    "testdata['proto'] = lb_make.fit_transform(testdata['proto'])\n",
    "testdata['service'] = lb_make.fit_transform(testdata['service'])\n",
    "testdata['state'] = lb_make.fit_transform(testdata['state'])\n",
    "\n",
    "X = traindata.iloc[:,1:42]\n",
    "Y = traindata.iloc[:,-1]\n",
    "x = testdata.iloc[:,1:42]\n",
    "y = testdata.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", input_shape=(123, 1), padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 175341 samples, validate on 82332 samples\n",
      "Epoch 1/200\n",
      "175341/175341 [==============================] - 970s 6ms/step - loss: 0.3954 - accuracy: 0.7954 - val_loss: 0.5462 - val_accuracy: 0.7052\n",
      "Epoch 2/200\n",
      "175341/175341 [==============================] - 1275s 7ms/step - loss: 0.3647 - accuracy: 0.8231 - val_loss: 0.5331 - val_accuracy: 0.7201\n",
      "Epoch 3/200\n",
      "175341/175341 [==============================] - 1355s 8ms/step - loss: 0.3574 - accuracy: 0.8281 - val_loss: 0.5608 - val_accuracy: 0.6979\n",
      "Epoch 4/200\n",
      "175341/175341 [==============================] - 1496s 9ms/step - loss: 0.4249 - accuracy: 0.7792 - val_loss: 0.5618 - val_accuracy: 0.7049\n",
      "Epoch 5/200\n",
      "175341/175341 [==============================] - 1525s 9ms/step - loss: 0.4386 - accuracy: 0.7596 - val_loss: 0.5641 - val_accuracy: 0.6334\n",
      "Epoch 6/200\n",
      "175341/175341 [==============================] - 1474s 8ms/step - loss: 0.4104 - accuracy: 0.7831 - val_loss: 0.5670 - val_accuracy: 0.6867\n",
      "Epoch 7/200\n",
      "175341/175341 [==============================] - 1469s 8ms/step - loss: 0.3989 - accuracy: 0.7866 - val_loss: 0.5588 - val_accuracy: 0.7086\n",
      "Epoch 8/200\n",
      "175341/175341 [==============================] - 1358s 8ms/step - loss: 0.4326 - accuracy: 0.7578 - val_loss: 0.5996 - val_accuracy: 0.6041\n",
      "Epoch 9/200\n",
      "175341/175341 [==============================] - 1092s 6ms/step - loss: 0.4274 - accuracy: 0.7674 - val_loss: 0.5579 - val_accuracy: 0.6699\n",
      "Epoch 10/200\n",
      "175341/175341 [==============================] - 1087s 6ms/step - loss: 0.3807 - accuracy: 0.8125 - val_loss: 0.5524 - val_accuracy: 0.6896\n",
      "Epoch 11/200\n",
      "175341/175341 [==============================] - 1004s 6ms/step - loss: 0.3896 - accuracy: 0.8004 - val_loss: 0.5555 - val_accuracy: 0.6846\n",
      "Epoch 12/200\n",
      "175341/175341 [==============================] - 990s 6ms/step - loss: 0.3789 - accuracy: 0.8099 - val_loss: 0.5343 - val_accuracy: 0.6962\n",
      "Epoch 13/200\n",
      "175341/175341 [==============================] - 987s 6ms/step - loss: 0.3666 - accuracy: 0.8162 - val_loss: 0.5843 - val_accuracy: 0.6813\n",
      "Epoch 14/200\n",
      "175341/175341 [==============================] - 986s 6ms/step - loss: 0.3790 - accuracy: 0.8078 - val_loss: 0.5530 - val_accuracy: 0.6821\n",
      "Epoch 15/200\n",
      "175341/175341 [==============================] - 975s 6ms/step - loss: 0.3629 - accuracy: 0.8194 - val_loss: 0.5392 - val_accuracy: 0.7236\n",
      "Epoch 16/200\n",
      "175341/175341 [==============================] - 972s 6ms/step - loss: 0.3602 - accuracy: 0.8223 - val_loss: 0.5312 - val_accuracy: 0.7202\n",
      "Epoch 17/200\n",
      "175341/175341 [==============================] - 974s 6ms/step - loss: 0.3677 - accuracy: 0.8100 - val_loss: 0.5619 - val_accuracy: 0.7292\n",
      "Epoch 18/200\n",
      "175341/175341 [==============================] - 973s 6ms/step - loss: 0.3645 - accuracy: 0.8201 - val_loss: 0.5506 - val_accuracy: 0.7175\n",
      "Epoch 19/200\n",
      "175341/175341 [==============================] - 974s 6ms/step - loss: 0.3492 - accuracy: 0.8296 - val_loss: 0.5450 - val_accuracy: 0.7249\n",
      "Epoch 20/200\n",
      "175341/175341 [==============================] - 970s 6ms/step - loss: 0.3474 - accuracy: 0.8293 - val_loss: 0.5361 - val_accuracy: 0.6989\n",
      "Epoch 21/200\n",
      "175341/175341 [==============================] - 969s 6ms/step - loss: 0.3441 - accuracy: 0.8306 - val_loss: 0.5602 - val_accuracy: 0.7110\n",
      "Epoch 22/200\n",
      "175341/175341 [==============================] - 971s 6ms/step - loss: 0.3512 - accuracy: 0.8258 - val_loss: 0.5282 - val_accuracy: 0.7390\n",
      "Epoch 23/200\n",
      "175341/175341 [==============================] - 982s 6ms/step - loss: 0.3425 - accuracy: 0.8331 - val_loss: 0.5592 - val_accuracy: 0.6959\n",
      "Epoch 24/200\n",
      "175341/175341 [==============================] - 1025s 6ms/step - loss: 0.3388 - accuracy: 0.8352 - val_loss: 0.5295 - val_accuracy: 0.7117\n",
      "Epoch 25/200\n",
      "175341/175341 [==============================] - 1025s 6ms/step - loss: 0.3371 - accuracy: 0.8368 - val_loss: 0.5440 - val_accuracy: 0.7164\n",
      "Epoch 26/200\n",
      "175341/175341 [==============================] - 1026s 6ms/step - loss: 0.3348 - accuracy: 0.8388 - val_loss: 0.5555 - val_accuracy: 0.7008\n",
      "Epoch 27/200\n",
      "175341/175341 [==============================] - 1014s 6ms/step - loss: 0.3261 - accuracy: 0.8470 - val_loss: 0.5093 - val_accuracy: 0.7305\n",
      "Epoch 28/200\n",
      "175341/175341 [==============================] - 1007s 6ms/step - loss: 0.3077 - accuracy: 0.8642 - val_loss: 0.5347 - val_accuracy: 0.7279\n",
      "Epoch 29/200\n",
      "175341/175341 [==============================] - 1011s 6ms/step - loss: 0.2986 - accuracy: 0.8715 - val_loss: 0.5347 - val_accuracy: 0.7213\n",
      "Epoch 30/200\n",
      "175341/175341 [==============================] - 1010s 6ms/step - loss: 0.2958 - accuracy: 0.8743 - val_loss: 0.5124 - val_accuracy: 0.7324\n",
      "Epoch 31/200\n",
      "175341/175341 [==============================] - 922s 5ms/step - loss: 0.2935 - accuracy: 0.8762 - val_loss: 0.5120 - val_accuracy: 0.7349\n",
      "Epoch 32/200\n",
      "175341/175341 [==============================] - 663s 4ms/step - loss: 0.2913 - accuracy: 0.8778 - val_loss: 0.5526 - val_accuracy: 0.7311\n",
      "Epoch 33/200\n",
      "175341/175341 [==============================] - 667s 4ms/step - loss: 0.2850 - accuracy: 0.8790 - val_loss: 0.5296 - val_accuracy: 0.7354\n",
      "Epoch 34/200\n",
      "175341/175341 [==============================] - 664s 4ms/step - loss: 0.2677 - accuracy: 0.8784 - val_loss: 0.4454 - val_accuracy: 0.7463\n",
      "Epoch 35/200\n",
      "175341/175341 [==============================] - 663s 4ms/step - loss: 0.2587 - accuracy: 0.8796 - val_loss: 0.4246 - val_accuracy: 0.7423\n",
      "Epoch 36/200\n",
      "175341/175341 [==============================] - 664s 4ms/step - loss: 0.2553 - accuracy: 0.8806 - val_loss: 0.4291 - val_accuracy: 0.7541\n",
      "Epoch 37/200\n",
      "175341/175341 [==============================] - 663s 4ms/step - loss: 0.2541 - accuracy: 0.8808 - val_loss: 0.4082 - val_accuracy: 0.7565\n",
      "Epoch 38/200\n",
      "175341/175341 [==============================] - 665s 4ms/step - loss: 0.2519 - accuracy: 0.8815 - val_loss: 0.4131 - val_accuracy: 0.7555\n",
      "Epoch 39/200\n",
      "175341/175341 [==============================] - 663s 4ms/step - loss: 0.2495 - accuracy: 0.8819 - val_loss: 0.4064 - val_accuracy: 0.7553\n",
      "Epoch 40/200\n",
      "175341/175341 [==============================] - 670s 4ms/step - loss: 0.2484 - accuracy: 0.8826 - val_loss: 0.4620 - val_accuracy: 0.7404\n",
      "Epoch 41/200\n",
      "175341/175341 [==============================] - 666s 4ms/step - loss: 0.2464 - accuracy: 0.8829 - val_loss: 0.4432 - val_accuracy: 0.7548\n",
      "Epoch 42/200\n",
      "175341/175341 [==============================] - 663s 4ms/step - loss: 0.2444 - accuracy: 0.8836 - val_loss: 0.4153 - val_accuracy: 0.7539\n",
      "Epoch 43/200\n",
      "175341/175341 [==============================] - 663s 4ms/step - loss: 0.2438 - accuracy: 0.8848 - val_loss: 0.3972 - val_accuracy: 0.7571\n",
      "Epoch 44/200\n",
      "175341/175341 [==============================] - 661s 4ms/step - loss: 0.2423 - accuracy: 0.8853 - val_loss: 0.4632 - val_accuracy: 0.7556\n",
      "Epoch 45/200\n",
      "175341/175341 [==============================] - 660s 4ms/step - loss: 0.2412 - accuracy: 0.8854 - val_loss: 0.4018 - val_accuracy: 0.7278\n",
      "Epoch 46/200\n",
      "175341/175341 [==============================] - 658s 4ms/step - loss: 0.2397 - accuracy: 0.8865 - val_loss: 0.4059 - val_accuracy: 0.7567\n",
      "Epoch 47/200\n",
      "175341/175341 [==============================] - 660s 4ms/step - loss: 0.2384 - accuracy: 0.8871 - val_loss: 0.4074 - val_accuracy: 0.7582\n",
      "Epoch 48/200\n",
      "175341/175341 [==============================] - 659s 4ms/step - loss: 0.2373 - accuracy: 0.8883 - val_loss: 0.4006 - val_accuracy: 0.7641\n",
      "Epoch 49/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175341/175341 [==============================] - 647s 4ms/step - loss: 0.2368 - accuracy: 0.8883 - val_loss: 0.4077 - val_accuracy: 0.7576\n",
      "Epoch 50/200\n",
      "175341/175341 [==============================] - 647s 4ms/step - loss: 0.2359 - accuracy: 0.8889 - val_loss: 0.4066 - val_accuracy: 0.7615\n",
      "Epoch 51/200\n",
      "175341/175341 [==============================] - 646s 4ms/step - loss: 0.2348 - accuracy: 0.8899 - val_loss: 0.3956 - val_accuracy: 0.7631\n",
      "Epoch 52/200\n",
      "175341/175341 [==============================] - 649s 4ms/step - loss: 0.2341 - accuracy: 0.8902 - val_loss: 0.3977 - val_accuracy: 0.7631\n",
      "Epoch 53/200\n",
      "175341/175341 [==============================] - 647s 4ms/step - loss: 0.2411 - accuracy: 0.8889 - val_loss: 0.4241 - val_accuracy: 0.7639\n",
      "Epoch 54/200\n",
      "175341/175341 [==============================] - 647s 4ms/step - loss: 0.2365 - accuracy: 0.8910 - val_loss: 0.4077 - val_accuracy: 0.7567\n",
      "Epoch 55/200\n",
      "175341/175341 [==============================] - 647s 4ms/step - loss: 0.2351 - accuracy: 0.8913 - val_loss: 0.4067 - val_accuracy: 0.7645\n",
      "Epoch 56/200\n",
      "175341/175341 [==============================] - 646s 4ms/step - loss: 0.2335 - accuracy: 0.8919 - val_loss: 0.4060 - val_accuracy: 0.7588\n",
      "Epoch 57/200\n",
      "175341/175341 [==============================] - 647s 4ms/step - loss: 0.2328 - accuracy: 0.8928 - val_loss: 0.4147 - val_accuracy: 0.7656\n",
      "Epoch 58/200\n",
      "175341/175341 [==============================] - 647s 4ms/step - loss: 0.2319 - accuracy: 0.8932 - val_loss: 0.4172 - val_accuracy: 0.7579\n",
      "Epoch 59/200\n",
      "175341/175341 [==============================] - 647s 4ms/step - loss: 0.2315 - accuracy: 0.8934 - val_loss: 0.3995 - val_accuracy: 0.7574\n",
      "Epoch 60/200\n",
      "175341/175341 [==============================] - 648s 4ms/step - loss: 0.2310 - accuracy: 0.8938 - val_loss: 0.4000 - val_accuracy: 0.7575\n",
      "Epoch 61/200\n",
      "175341/175341 [==============================] - 648s 4ms/step - loss: 0.2301 - accuracy: 0.8941 - val_loss: 0.4156 - val_accuracy: 0.7583\n",
      "Epoch 62/200\n",
      "175341/175341 [==============================] - 647s 4ms/step - loss: 0.2308 - accuracy: 0.8932 - val_loss: 0.3945 - val_accuracy: 0.7578\n",
      "Epoch 63/200\n",
      "175341/175341 [==============================] - 648s 4ms/step - loss: 0.2293 - accuracy: 0.8943 - val_loss: 0.4131 - val_accuracy: 0.7658\n",
      "Epoch 64/200\n",
      "175341/175341 [==============================] - 648s 4ms/step - loss: 0.2289 - accuracy: 0.8944 - val_loss: 0.4034 - val_accuracy: 0.7646\n",
      "Epoch 65/200\n",
      "175341/175341 [==============================] - 649s 4ms/step - loss: 0.2290 - accuracy: 0.8946 - val_loss: 0.4203 - val_accuracy: 0.7586\n",
      "Epoch 66/200\n",
      "175341/175341 [==============================] - 648s 4ms/step - loss: 0.2293 - accuracy: 0.8939 - val_loss: 0.4056 - val_accuracy: 0.7569\n",
      "Epoch 67/200\n",
      "175341/175341 [==============================] - 648s 4ms/step - loss: 0.2283 - accuracy: 0.8942 - val_loss: 0.4058 - val_accuracy: 0.7606\n",
      "Epoch 68/200\n",
      "175341/175341 [==============================] - 648s 4ms/step - loss: 0.2278 - accuracy: 0.8951 - val_loss: 0.4082 - val_accuracy: 0.7534\n",
      "Epoch 69/200\n",
      "175341/175341 [==============================] - 649s 4ms/step - loss: 0.2275 - accuracy: 0.8952 - val_loss: 0.4182 - val_accuracy: 0.7633\n",
      "Epoch 70/200\n",
      "175341/175341 [==============================] - 648s 4ms/step - loss: 0.2270 - accuracy: 0.8957 - val_loss: 0.4062 - val_accuracy: 0.7502\n",
      "Epoch 71/200\n",
      "175341/175341 [==============================] - 649s 4ms/step - loss: 0.2264 - accuracy: 0.8954 - val_loss: 0.3757 - val_accuracy: 0.7604\n",
      "Epoch 72/200\n",
      "175341/175341 [==============================] - 695s 4ms/step - loss: 0.2271 - accuracy: 0.8953 - val_loss: 0.4039 - val_accuracy: 0.7592\n",
      "Epoch 73/200\n",
      "175341/175341 [==============================] - 651s 4ms/step - loss: 0.2260 - accuracy: 0.8957 - val_loss: 0.4032 - val_accuracy: 0.7648\n",
      "Epoch 74/200\n",
      "175341/175341 [==============================] - 649s 4ms/step - loss: 0.2266 - accuracy: 0.8954 - val_loss: 0.3919 - val_accuracy: 0.7635\n",
      "Epoch 75/200\n",
      "175341/175341 [==============================] - 649s 4ms/step - loss: 0.2255 - accuracy: 0.8960 - val_loss: 0.3869 - val_accuracy: 0.7660\n",
      "Epoch 76/200\n",
      "175341/175341 [==============================] - 650s 4ms/step - loss: 0.2252 - accuracy: 0.8964 - val_loss: 0.3952 - val_accuracy: 0.7677\n",
      "Epoch 77/200\n",
      "175341/175341 [==============================] - 650s 4ms/step - loss: 0.2249 - accuracy: 0.8965 - val_loss: 0.3869 - val_accuracy: 0.7676\n",
      "Epoch 78/200\n",
      "175341/175341 [==============================] - 650s 4ms/step - loss: 0.2262 - accuracy: 0.8965 - val_loss: 0.4044 - val_accuracy: 0.7625\n",
      "Epoch 79/200\n",
      "175341/175341 [==============================] - 652s 4ms/step - loss: 0.2253 - accuracy: 0.8967 - val_loss: 0.3951 - val_accuracy: 0.7682\n",
      "Epoch 80/200\n",
      "175341/175341 [==============================] - 650s 4ms/step - loss: 0.2251 - accuracy: 0.8963 - val_loss: 0.4082 - val_accuracy: 0.7606\n",
      "Epoch 81/200\n",
      "175341/175341 [==============================] - 651s 4ms/step - loss: 0.2239 - accuracy: 0.8969 - val_loss: 0.3956 - val_accuracy: 0.7675\n",
      "Epoch 82/200\n",
      "175341/175341 [==============================] - 651s 4ms/step - loss: 0.2240 - accuracy: 0.8974 - val_loss: 0.3867 - val_accuracy: 0.7690\n",
      "Epoch 83/200\n",
      "175341/175341 [==============================] - 652s 4ms/step - loss: 0.2235 - accuracy: 0.8975 - val_loss: 0.3847 - val_accuracy: 0.7666\n",
      "Epoch 84/200\n",
      "175341/175341 [==============================] - 650s 4ms/step - loss: 0.2215 - accuracy: 0.8988 - val_loss: 0.3929 - val_accuracy: 0.7675\n",
      "Epoch 85/200\n",
      "175341/175341 [==============================] - 651s 4ms/step - loss: 0.2223 - accuracy: 0.8982 - val_loss: 0.3965 - val_accuracy: 0.7680\n",
      "Epoch 86/200\n",
      "175341/175341 [==============================] - 652s 4ms/step - loss: 0.2224 - accuracy: 0.8980 - val_loss: 0.3842 - val_accuracy: 0.7705\n",
      "Epoch 87/200\n",
      "175341/175341 [==============================] - 651s 4ms/step - loss: 0.2265 - accuracy: 0.8970 - val_loss: 0.3992 - val_accuracy: 0.7670\n",
      "Epoch 88/200\n",
      "175341/175341 [==============================] - 652s 4ms/step - loss: 0.2237 - accuracy: 0.8975 - val_loss: 0.3884 - val_accuracy: 0.7661\n",
      "Epoch 89/200\n",
      "175341/175341 [==============================] - 651s 4ms/step - loss: 0.2236 - accuracy: 0.8979 - val_loss: 0.3897 - val_accuracy: 0.7598\n",
      "Epoch 90/200\n",
      "175341/175341 [==============================] - 653s 4ms/step - loss: 0.2231 - accuracy: 0.8975 - val_loss: 0.3924 - val_accuracy: 0.7684\n",
      "Epoch 91/200\n",
      "175341/175341 [==============================] - 652s 4ms/step - loss: 0.2225 - accuracy: 0.8982 - val_loss: 0.3802 - val_accuracy: 0.7604\n",
      "Epoch 92/200\n",
      "175341/175341 [==============================] - 652s 4ms/step - loss: 0.2223 - accuracy: 0.8981 - val_loss: 0.3945 - val_accuracy: 0.7650\n",
      "Epoch 93/200\n",
      "175341/175341 [==============================] - 651s 4ms/step - loss: 0.2213 - accuracy: 0.8982 - val_loss: 0.4152 - val_accuracy: 0.7635\n",
      "Epoch 94/200\n",
      "175341/175341 [==============================] - 658s 4ms/step - loss: 0.2211 - accuracy: 0.8987 - val_loss: 0.4155 - val_accuracy: 0.7461\n",
      "Epoch 95/200\n",
      "175341/175341 [==============================] - 644s 4ms/step - loss: 0.2209 - accuracy: 0.8991 - val_loss: 0.3907 - val_accuracy: 0.7706\n",
      "Epoch 96/200\n",
      "175341/175341 [==============================] - 643s 4ms/step - loss: 0.2197 - accuracy: 0.8996 - val_loss: 0.3850 - val_accuracy: 0.7641\n",
      "Epoch 97/200\n",
      "175341/175341 [==============================] - 642s 4ms/step - loss: 0.2206 - accuracy: 0.8989 - val_loss: 0.3868 - val_accuracy: 0.7637\n",
      "Epoch 98/200\n",
      "175341/175341 [==============================] - 644s 4ms/step - loss: 0.2212 - accuracy: 0.8990 - val_loss: 0.3807 - val_accuracy: 0.7643\n",
      "Epoch 99/200\n",
      "175341/175341 [==============================] - 642s 4ms/step - loss: 0.2332 - accuracy: 0.8968 - val_loss: 0.4090 - val_accuracy: 0.7703\n",
      "Epoch 100/200\n",
      "175341/175341 [==============================] - 640s 4ms/step - loss: 0.2217 - accuracy: 0.8998 - val_loss: 0.3980 - val_accuracy: 0.7648\n",
      "Epoch 101/200\n",
      "175341/175341 [==============================] - 634s 4ms/step - loss: 0.2235 - accuracy: 0.8994 - val_loss: 0.3911 - val_accuracy: 0.7654\n",
      "Epoch 102/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175341/175341 [==============================] - 634s 4ms/step - loss: 0.2209 - accuracy: 0.8999 - val_loss: 0.3820 - val_accuracy: 0.7647\n",
      "Epoch 103/200\n",
      "175341/175341 [==============================] - 633s 4ms/step - loss: 0.2414 - accuracy: 0.8910 - val_loss: 0.4038 - val_accuracy: 0.7619\n",
      "Epoch 104/200\n",
      "175341/175341 [==============================] - 633s 4ms/step - loss: 0.2258 - accuracy: 0.8978 - val_loss: 0.3985 - val_accuracy: 0.7646\n",
      "Epoch 105/200\n",
      "175341/175341 [==============================] - 633s 4ms/step - loss: 0.2203 - accuracy: 0.9000 - val_loss: 0.3958 - val_accuracy: 0.7651\n",
      "Epoch 106/200\n",
      "175341/175341 [==============================] - 637s 4ms/step - loss: 0.2193 - accuracy: 0.9001 - val_loss: 0.3794 - val_accuracy: 0.7712\n",
      "Epoch 107/200\n",
      "175341/175341 [==============================] - 635s 4ms/step - loss: 0.2188 - accuracy: 0.9001 - val_loss: 0.3910 - val_accuracy: 0.7650\n",
      "Epoch 108/200\n",
      "175341/175341 [==============================] - 634s 4ms/step - loss: 0.2189 - accuracy: 0.9002 - val_loss: 0.3780 - val_accuracy: 0.7720\n",
      "Epoch 109/200\n",
      "175341/175341 [==============================] - 635s 4ms/step - loss: 0.2183 - accuracy: 0.9005 - val_loss: 0.3948 - val_accuracy: 0.7715\n",
      "Epoch 110/200\n",
      "175341/175341 [==============================] - 635s 4ms/step - loss: 0.2204 - accuracy: 0.8996 - val_loss: 0.4018 - val_accuracy: 0.7604\n",
      "Epoch 111/200\n",
      "175341/175341 [==============================] - 635s 4ms/step - loss: 0.2257 - accuracy: 0.8975 - val_loss: 0.3853 - val_accuracy: 0.7714\n",
      "Epoch 112/200\n",
      "175341/175341 [==============================] - 634s 4ms/step - loss: 0.2191 - accuracy: 0.9007 - val_loss: 0.3983 - val_accuracy: 0.7717\n",
      "Epoch 113/200\n",
      "175341/175341 [==============================] - 634s 4ms/step - loss: 0.2186 - accuracy: 0.9009 - val_loss: 0.4033 - val_accuracy: 0.7652\n",
      "Epoch 114/200\n",
      "175341/175341 [==============================] - 634s 4ms/step - loss: 0.2179 - accuracy: 0.9006 - val_loss: 0.3829 - val_accuracy: 0.7716\n",
      "Epoch 115/200\n",
      "175341/175341 [==============================] - 635s 4ms/step - loss: 0.2180 - accuracy: 0.9006 - val_loss: 0.3965 - val_accuracy: 0.7716\n",
      "Epoch 116/200\n",
      "175341/175341 [==============================] - 634s 4ms/step - loss: 0.2182 - accuracy: 0.9005 - val_loss: 0.3963 - val_accuracy: 0.7713\n",
      "Epoch 117/200\n",
      "175341/175341 [==============================] - 635s 4ms/step - loss: 0.2171 - accuracy: 0.9015 - val_loss: 0.3836 - val_accuracy: 0.7657\n",
      "Epoch 118/200\n",
      "175341/175341 [==============================] - 635s 4ms/step - loss: 0.2163 - accuracy: 0.9015 - val_loss: 0.4029 - val_accuracy: 0.7687\n",
      "Epoch 119/200\n",
      "175341/175341 [==============================] - 635s 4ms/step - loss: 0.2163 - accuracy: 0.9018 - val_loss: 0.3902 - val_accuracy: 0.7722\n",
      "Epoch 120/200\n",
      "175341/175341 [==============================] - 634s 4ms/step - loss: 0.2160 - accuracy: 0.9017 - val_loss: 0.3878 - val_accuracy: 0.7722\n",
      "Epoch 121/200\n",
      "175341/175341 [==============================] - 635s 4ms/step - loss: 0.2152 - accuracy: 0.9020 - val_loss: 0.3841 - val_accuracy: 0.7719\n",
      "Epoch 122/200\n",
      "175341/175341 [==============================] - 691s 4ms/step - loss: 0.2153 - accuracy: 0.9020 - val_loss: 0.3794 - val_accuracy: 0.7662\n",
      "Epoch 123/200\n",
      "175341/175341 [==============================] - 713s 4ms/step - loss: 0.2150 - accuracy: 0.9021 - val_loss: 0.3923 - val_accuracy: 0.7666\n",
      "Epoch 124/200\n",
      "175341/175341 [==============================] - 640s 4ms/step - loss: 0.2144 - accuracy: 0.9025 - val_loss: 0.5451 - val_accuracy: 0.7512\n",
      "Epoch 125/200\n",
      "175341/175341 [==============================] - 666s 4ms/step - loss: 0.2143 - accuracy: 0.9027 - val_loss: 0.3873 - val_accuracy: 0.7729\n",
      "Epoch 126/200\n",
      "175341/175341 [==============================] - 679s 4ms/step - loss: 0.2137 - accuracy: 0.9024 - val_loss: 0.3937 - val_accuracy: 0.7736\n",
      "Epoch 127/200\n",
      "175341/175341 [==============================] - 668s 4ms/step - loss: 0.2219 - accuracy: 0.8996 - val_loss: 0.3838 - val_accuracy: 0.7656\n",
      "Epoch 128/200\n",
      "175341/175341 [==============================] - 686s 4ms/step - loss: 0.2275 - accuracy: 0.8965 - val_loss: 0.4063 - val_accuracy: 0.7590\n",
      "Epoch 129/200\n",
      "175341/175341 [==============================] - 681s 4ms/step - loss: 0.2237 - accuracy: 0.8980 - val_loss: 0.3974 - val_accuracy: 0.7625\n",
      "Epoch 130/200\n",
      "175341/175341 [==============================] - 666s 4ms/step - loss: 0.2228 - accuracy: 0.8983 - val_loss: 0.3806 - val_accuracy: 0.7640\n",
      "Epoch 131/200\n",
      "175341/175341 [==============================] - 667s 4ms/step - loss: 0.2221 - accuracy: 0.8984 - val_loss: 0.3923 - val_accuracy: 0.7700\n",
      "Epoch 132/200\n",
      "175341/175341 [==============================] - 669s 4ms/step - loss: 0.2215 - accuracy: 0.8991 - val_loss: 0.3890 - val_accuracy: 0.7503\n",
      "Epoch 133/200\n",
      "175341/175341 [==============================] - 667s 4ms/step - loss: 0.2199 - accuracy: 0.8998 - val_loss: 0.3967 - val_accuracy: 0.7631\n",
      "Epoch 134/200\n",
      "175341/175341 [==============================] - 665s 4ms/step - loss: 0.2203 - accuracy: 0.8996 - val_loss: 0.3909 - val_accuracy: 0.7640\n",
      "Epoch 135/200\n",
      "175341/175341 [==============================] - 662s 4ms/step - loss: 0.2190 - accuracy: 0.9000 - val_loss: 0.3790 - val_accuracy: 0.7704\n",
      "Epoch 136/200\n",
      "175341/175341 [==============================] - 663s 4ms/step - loss: 0.2190 - accuracy: 0.9005 - val_loss: 0.4026 - val_accuracy: 0.7710\n",
      "Epoch 137/200\n",
      "175341/175341 [==============================] - 664s 4ms/step - loss: 0.2185 - accuracy: 0.9008 - val_loss: 0.3927 - val_accuracy: 0.7653\n",
      "Epoch 138/200\n",
      "175341/175341 [==============================] - 663s 4ms/step - loss: 0.2177 - accuracy: 0.9009 - val_loss: 0.3960 - val_accuracy: 0.7704\n",
      "Epoch 139/200\n",
      "175341/175341 [==============================] - 664s 4ms/step - loss: 0.2185 - accuracy: 0.9005 - val_loss: 0.3900 - val_accuracy: 0.7681\n",
      "Epoch 140/200\n",
      "175341/175341 [==============================] - 664s 4ms/step - loss: 0.2182 - accuracy: 0.9007 - val_loss: 0.3962 - val_accuracy: 0.7631\n",
      "Epoch 141/200\n",
      "175341/175341 [==============================] - 663s 4ms/step - loss: 0.2173 - accuracy: 0.9011 - val_loss: 0.4115 - val_accuracy: 0.7519\n",
      "Epoch 142/200\n",
      "175341/175341 [==============================] - 665s 4ms/step - loss: 0.2171 - accuracy: 0.9016 - val_loss: 0.3844 - val_accuracy: 0.7720\n",
      "Epoch 143/200\n",
      "175341/175341 [==============================] - 666s 4ms/step - loss: 0.2164 - accuracy: 0.9017 - val_loss: 0.3905 - val_accuracy: 0.7695\n",
      "Epoch 144/200\n",
      "175341/175341 [==============================] - 665s 4ms/step - loss: 0.2149 - accuracy: 0.9022 - val_loss: 0.3896 - val_accuracy: 0.7720\n",
      "Epoch 145/200\n",
      "175341/175341 [==============================] - 667s 4ms/step - loss: 0.2157 - accuracy: 0.9016 - val_loss: 0.3737 - val_accuracy: 0.7659\n",
      "Epoch 146/200\n",
      "175341/175341 [==============================] - 668s 4ms/step - loss: 0.2339 - accuracy: 0.8944 - val_loss: 0.4022 - val_accuracy: 0.7696\n",
      "Epoch 147/200\n",
      "175341/175341 [==============================] - 666s 4ms/step - loss: 0.2236 - accuracy: 0.8995 - val_loss: 0.3917 - val_accuracy: 0.7692\n",
      "Epoch 148/200\n",
      "175341/175341 [==============================] - 665s 4ms/step - loss: 0.2169 - accuracy: 0.9012 - val_loss: 0.3891 - val_accuracy: 0.7726\n",
      "Epoch 149/200\n",
      "175341/175341 [==============================] - 666s 4ms/step - loss: 0.2167 - accuracy: 0.9016 - val_loss: 0.3819 - val_accuracy: 0.7719\n",
      "Epoch 150/200\n",
      "175341/175341 [==============================] - 666s 4ms/step - loss: 0.2165 - accuracy: 0.9013 - val_loss: 0.3795 - val_accuracy: 0.7650\n",
      "Epoch 151/200\n",
      "175341/175341 [==============================] - 667s 4ms/step - loss: 0.2165 - accuracy: 0.9015 - val_loss: 0.3918 - val_accuracy: 0.7695\n",
      "Epoch 152/200\n",
      "175341/175341 [==============================] - 665s 4ms/step - loss: 0.2163 - accuracy: 0.9013 - val_loss: 0.3761 - val_accuracy: 0.7646\n",
      "Epoch 153/200\n",
      "175341/175341 [==============================] - 667s 4ms/step - loss: 0.2169 - accuracy: 0.9014 - val_loss: 0.3842 - val_accuracy: 0.7717\n",
      "Epoch 154/200\n",
      "175341/175341 [==============================] - 666s 4ms/step - loss: 0.2160 - accuracy: 0.9019 - val_loss: 0.3849 - val_accuracy: 0.7656\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175341/175341 [==============================] - 649s 4ms/step - loss: 0.2158 - accuracy: 0.9019 - val_loss: 0.3984 - val_accuracy: 0.7663\n",
      "Epoch 156/200\n",
      "175341/175341 [==============================] - 651s 4ms/step - loss: 0.2149 - accuracy: 0.9025 - val_loss: 0.3902 - val_accuracy: 0.7731\n",
      "Epoch 157/200\n",
      "175341/175341 [==============================] - 650s 4ms/step - loss: 0.2156 - accuracy: 0.9016 - val_loss: 0.3890 - val_accuracy: 0.7646\n",
      "Epoch 158/200\n",
      "175341/175341 [==============================] - 650s 4ms/step - loss: 0.2147 - accuracy: 0.9024 - val_loss: 0.3909 - val_accuracy: 0.7662\n",
      "Epoch 159/200\n",
      "175341/175341 [==============================] - 651s 4ms/step - loss: 0.2151 - accuracy: 0.9019 - val_loss: 0.3772 - val_accuracy: 0.7725\n",
      "Epoch 160/200\n",
      "175341/175341 [==============================] - 651s 4ms/step - loss: 0.2152 - accuracy: 0.9021 - val_loss: 0.3945 - val_accuracy: 0.7720\n",
      "Epoch 161/200\n",
      "175341/175341 [==============================] - 650s 4ms/step - loss: 0.2149 - accuracy: 0.9021 - val_loss: 0.3751 - val_accuracy: 0.7731\n",
      "Epoch 162/200\n",
      "175341/175341 [==============================] - 651s 4ms/step - loss: 0.2143 - accuracy: 0.9024 - val_loss: 0.3960 - val_accuracy: 0.7663\n",
      "Epoch 163/200\n",
      "175341/175341 [==============================] - 651s 4ms/step - loss: 0.2146 - accuracy: 0.9021 - val_loss: 0.3944 - val_accuracy: 0.7658\n",
      "Epoch 164/200\n",
      "175341/175341 [==============================] - 650s 4ms/step - loss: 0.2147 - accuracy: 0.9024 - val_loss: 0.3741 - val_accuracy: 0.7712\n",
      "Epoch 165/200\n",
      "175341/175341 [==============================] - 652s 4ms/step - loss: 0.2147 - accuracy: 0.9023 - val_loss: 0.3857 - val_accuracy: 0.7717\n",
      "Epoch 166/200\n",
      "175341/175341 [==============================] - 651s 4ms/step - loss: 0.2147 - accuracy: 0.9023 - val_loss: 0.3794 - val_accuracy: 0.7653\n",
      "Epoch 167/200\n",
      "175341/175341 [==============================] - 650s 4ms/step - loss: 0.2140 - accuracy: 0.9025 - val_loss: 0.3728 - val_accuracy: 0.7663\n",
      "Epoch 168/200\n",
      "175341/175341 [==============================] - 652s 4ms/step - loss: 0.2142 - accuracy: 0.9026 - val_loss: 0.3931 - val_accuracy: 0.7729\n",
      "Epoch 169/200\n",
      "175341/175341 [==============================] - 651s 4ms/step - loss: 0.2141 - accuracy: 0.9030 - val_loss: 0.3893 - val_accuracy: 0.7729\n",
      "Epoch 170/200\n",
      "175341/175341 [==============================] - 652s 4ms/step - loss: 0.2140 - accuracy: 0.9028 - val_loss: 0.3912 - val_accuracy: 0.7665\n",
      "Epoch 171/200\n",
      "175341/175341 [==============================] - 650s 4ms/step - loss: 0.2134 - accuracy: 0.9029 - val_loss: 0.3905 - val_accuracy: 0.7730\n",
      "Epoch 172/200\n",
      "175341/175341 [==============================] - 652s 4ms/step - loss: 0.2133 - accuracy: 0.9028 - val_loss: 0.3835 - val_accuracy: 0.7731\n",
      "Epoch 173/200\n",
      "175341/175341 [==============================] - 651s 4ms/step - loss: 0.2130 - accuracy: 0.9032 - val_loss: 0.3937 - val_accuracy: 0.7728\n",
      "Epoch 174/200\n",
      "175341/175341 [==============================] - 650s 4ms/step - loss: 0.2130 - accuracy: 0.9033 - val_loss: 0.3939 - val_accuracy: 0.7740\n",
      "Epoch 175/200\n",
      "175341/175341 [==============================] - 651s 4ms/step - loss: 0.2129 - accuracy: 0.9037 - val_loss: 0.3740 - val_accuracy: 0.7736\n",
      "Epoch 176/200\n",
      "175341/175341 [==============================] - 652s 4ms/step - loss: 0.2127 - accuracy: 0.9036 - val_loss: 0.4083 - val_accuracy: 0.7708\n",
      "Epoch 177/200\n",
      "175341/175341 [==============================] - 651s 4ms/step - loss: 0.2126 - accuracy: 0.9030 - val_loss: 0.3826 - val_accuracy: 0.7666\n",
      "Epoch 178/200\n",
      "175341/175341 [==============================] - 651s 4ms/step - loss: 0.2126 - accuracy: 0.9031 - val_loss: 0.3812 - val_accuracy: 0.7701\n",
      "Epoch 179/200\n",
      "175341/175341 [==============================] - 652s 4ms/step - loss: 0.2129 - accuracy: 0.9030 - val_loss: 0.3945 - val_accuracy: 0.7726\n",
      "Epoch 180/200\n",
      "175341/175341 [==============================] - 652s 4ms/step - loss: 0.2135 - accuracy: 0.9028 - val_loss: 0.3821 - val_accuracy: 0.7742\n",
      "Epoch 181/200\n",
      "175341/175341 [==============================] - 653s 4ms/step - loss: 0.2126 - accuracy: 0.9029 - val_loss: 0.3936 - val_accuracy: 0.7727\n",
      "Epoch 182/200\n",
      "175341/175341 [==============================] - 652s 4ms/step - loss: 0.2129 - accuracy: 0.9031 - val_loss: 0.3877 - val_accuracy: 0.7710\n",
      "Epoch 183/200\n",
      "175341/175341 [==============================] - 653s 4ms/step - loss: 0.2126 - accuracy: 0.9031 - val_loss: 0.3752 - val_accuracy: 0.7740\n",
      "Epoch 184/200\n",
      "175341/175341 [==============================] - 653s 4ms/step - loss: 0.2126 - accuracy: 0.9030 - val_loss: 0.3944 - val_accuracy: 0.7722\n",
      "Epoch 185/200\n",
      "175341/175341 [==============================] - 653s 4ms/step - loss: 0.2139 - accuracy: 0.9023 - val_loss: 0.3867 - val_accuracy: 0.7737\n",
      "Epoch 186/200\n",
      "175341/175341 [==============================] - 653s 4ms/step - loss: 0.2124 - accuracy: 0.9033 - val_loss: 0.3782 - val_accuracy: 0.7729\n",
      "Epoch 187/200\n",
      "175341/175341 [==============================] - 654s 4ms/step - loss: 0.2124 - accuracy: 0.9032 - val_loss: 0.3737 - val_accuracy: 0.7730\n",
      "Epoch 188/200\n",
      "175341/175341 [==============================] - 654s 4ms/step - loss: 0.2120 - accuracy: 0.9035 - val_loss: 0.3831 - val_accuracy: 0.7673\n",
      "Epoch 189/200\n",
      "175341/175341 [==============================] - 653s 4ms/step - loss: 0.2133 - accuracy: 0.9025 - val_loss: 0.3837 - val_accuracy: 0.7725\n",
      "Epoch 190/200\n",
      "175341/175341 [==============================] - 655s 4ms/step - loss: 0.2120 - accuracy: 0.9038 - val_loss: 0.4038 - val_accuracy: 0.7665\n",
      "Epoch 191/200\n",
      "175341/175341 [==============================] - 654s 4ms/step - loss: 0.2126 - accuracy: 0.9035 - val_loss: 0.3817 - val_accuracy: 0.7672\n",
      "Epoch 192/200\n",
      "175341/175341 [==============================] - 654s 4ms/step - loss: 0.2124 - accuracy: 0.9037 - val_loss: 0.3933 - val_accuracy: 0.7661\n",
      "Epoch 193/200\n",
      "175341/175341 [==============================] - 654s 4ms/step - loss: 0.2118 - accuracy: 0.9035 - val_loss: 0.3871 - val_accuracy: 0.7728\n",
      "Epoch 194/200\n",
      "175341/175341 [==============================] - 655s 4ms/step - loss: 0.2121 - accuracy: 0.9033 - val_loss: 0.3732 - val_accuracy: 0.7721\n",
      "Epoch 195/200\n",
      "175341/175341 [==============================] - 655s 4ms/step - loss: 0.2114 - accuracy: 0.9035 - val_loss: 0.3802 - val_accuracy: 0.7736\n",
      "Epoch 196/200\n",
      "175341/175341 [==============================] - 656s 4ms/step - loss: 0.2113 - accuracy: 0.9038 - val_loss: 0.3802 - val_accuracy: 0.7661\n",
      "Epoch 197/200\n",
      "175341/175341 [==============================] - 655s 4ms/step - loss: 0.2108 - accuracy: 0.9041 - val_loss: 0.3796 - val_accuracy: 0.7680\n",
      "Epoch 198/200\n",
      "175341/175341 [==============================] - 656s 4ms/step - loss: 0.2133 - accuracy: 0.9025 - val_loss: 0.3893 - val_accuracy: 0.7659\n",
      "Epoch 199/200\n",
      "175341/175341 [==============================] - 656s 4ms/step - loss: 0.2117 - accuracy: 0.9036 - val_loss: 0.3831 - val_accuracy: 0.7739\n",
      "Epoch 200/200\n",
      "175341/175341 [==============================] - 655s 4ms/step - loss: 0.2113 - accuracy: 0.9036 - val_loss: 0.3811 - val_accuracy: 0.7741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f93a6f7c6d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Arr = X.values\n",
    "arr = x.values\n",
    "\n",
    "import librosa\n",
    "Arr_Delta = librosa.feature.delta(Arr)\n",
    "arr_delta = librosa.feature.delta(arr)\n",
    "\n",
    "Delta = librosa.feature.delta(Arr_Delta)\n",
    "delta = librosa.feature.delta(arr_delta)\n",
    "\n",
    "Delta1 = pd.DataFrame(Arr_Delta)\n",
    "delta1 = pd.DataFrame(arr_delta)\n",
    "Delta2 = pd.DataFrame(Delta)\n",
    "delta2 = pd.DataFrame(delta)\n",
    "\n",
    "XDelta1 = pd.concat([X, Delta1, Delta2], axis=1)\n",
    "xdelta1 = pd.concat([x, delta1, delta2], axis=1)\n",
    "\n",
    "scaler = Normalizer().fit(XDelta1)\n",
    "XDelta1 = scaler.transform(XDelta1)\n",
    "scaler = Normalizer().fit(xdelta1)\n",
    "xdelta1 = scaler.transform(xdelta1)\n",
    "\n",
    "Y = np.array(Y)\n",
    "y = np.array(y)\n",
    "\n",
    "XDelta1 = np.reshape(XDelta1, (XDelta1.shape[0],XDelta1.shape[1],1))\n",
    "xdelta1 = np.reshape(xdelta1, (xdelta1.shape[0],xdelta1.shape[1],1))\n",
    "\n",
    "lstm_output_size = 70\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(Convolution1D(64, 3, border_mode=\"same\",activation=\"relu\",input_shape=(123, 1)))\n",
    "cnn.add(MaxPooling1D(pool_size=2))\n",
    "cnn.add(Bidirectional(LSTM(lstm_output_size)))\n",
    "cnn.add(Dropout(0.1))\n",
    "cnn.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "cnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "folds=10\n",
    "   \n",
    "cnn.fit(XDelta1, Y, epochs=200, validation_data=(xdelta1, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7740975562357285\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.52      0.68     37000\n",
      "           1       0.72      0.98      0.83     45332\n",
      "\n",
      "    accuracy                           0.77     82332\n",
      "   macro avg       0.83      0.75      0.75     82332\n",
      "weighted avg       0.82      0.77      0.76     82332\n",
      "\n",
      "          predicted_negative  predicted_positive\n",
      "negative               19422               17578\n",
      "positive                1021               44311\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "predictions = cnn.predict([xdelta1])\n",
    "predictions = [0 if i<0.5 else 1 for i in predictions]\n",
    "print(\"Accuracy: \",accuracy_score(y,predictions))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Classification Report: \\n\",classification_report(y,predictions))\n",
    "conmat = np.array(confusion_matrix(y, predictions, labels=[0,1]))\n",
    "confusion = pd.DataFrame(conmat, index=['negative', 'positive'],\n",
    "                         columns=['predicted_negative','predicted_positive'])\n",
    "print (confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
