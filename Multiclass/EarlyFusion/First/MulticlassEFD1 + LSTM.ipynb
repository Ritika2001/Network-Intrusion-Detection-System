{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN, Bidirectional\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = pd.read_csv('/home/ritika/NetworkIntrusionDetection/UNSW-NB15 - CSV Files/a part of training and testing set/UNSW_NB15_training-set.csv')\n",
    "testdata = pd.read_csv('/home/ritika/NetworkIntrusionDetection/UNSW-NB15 - CSV Files/a part of training and testing set/UNSW_NB15_testing-set.csv')\n",
    "\n",
    "#traindata=traindata.drop('id',axis=1)\n",
    "#testdata=testdata.drop('id',axis=1)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb_make = LabelEncoder()\n",
    "\n",
    "traindata['proto'] = lb_make.fit_transform(traindata['proto'])\n",
    "traindata['service'] = lb_make.fit_transform(traindata['service'])\n",
    "traindata['state'] = lb_make.fit_transform(traindata['state'])\n",
    "traindata['attack_cat'] = lb_make.fit_transform(traindata['attack_cat'])\n",
    "\n",
    "testdata['proto'] = lb_make.fit_transform(testdata['proto'])\n",
    "testdata['service'] = lb_make.fit_transform(testdata['service'])\n",
    "testdata['state'] = lb_make.fit_transform(testdata['state'])\n",
    "testdata['attack_cat'] = lb_make.fit_transform(testdata['attack_cat'])\n",
    "\n",
    "X = traindata.iloc[:,1:42]\n",
    "Y = traindata.iloc[:,-2]\n",
    "x = testdata.iloc[:,1:42]\n",
    "y = testdata.iloc[:,-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arr = X.values\n",
    "arr = x.values\n",
    "\n",
    "import librosa\n",
    "Arr_Delta = librosa.feature.delta(Arr)\n",
    "arr_delta = librosa.feature.delta(arr)\n",
    "\n",
    "Delta1 = pd.DataFrame(Arr_Delta)\n",
    "delta1 = pd.DataFrame(arr_delta)\n",
    "\n",
    "XDelta1 = pd.concat([X, Delta1], axis=1)\n",
    "xdelta1 = pd.concat([x, delta1], axis=1)\n",
    "\n",
    "scaler = Normalizer().fit(XDelta1)\n",
    "XDelta1 = scaler.transform(XDelta1)\n",
    "scaler = Normalizer().fit(xdelta1)\n",
    "xdelta1 = scaler.transform(xdelta1)\n",
    "\n",
    "Y = np.array(Y)\n",
    "y = np.array(y)\n",
    "\n",
    "XDelta1 = np.reshape(XDelta1, (XDelta1.shape[0],1,XDelta1.shape[1]))\n",
    "xdelta1 = np.reshape(xdelta1, (xdelta1.shape[0],1,xdelta1.shape[1]))\n",
    "\n",
    "lstm_output_size = 70\n",
    "\n",
    "Y = to_categorical(Y)\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82332, 10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \n",
      "/home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(4, input_shape=(None, 82))`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 4)                 1392      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,442\n",
      "Trainable params: 1,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(4,input_dim=82)) \n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 175341 samples, validate on 82332 samples\n",
      "Epoch 1/200\n",
      "175341/175341 [==============================] - 50s 287us/step - loss: 1.5099 - accuracy: 0.4880 - val_loss: 1.2968 - val_accuracy: 0.6170\n",
      "Epoch 2/200\n",
      "175341/175341 [==============================] - 46s 262us/step - loss: 1.4053 - accuracy: 0.5192 - val_loss: 1.2972 - val_accuracy: 0.6176\n",
      "Epoch 3/200\n",
      "175341/175341 [==============================] - 48s 272us/step - loss: 1.3916 - accuracy: 0.5270 - val_loss: 1.2912 - val_accuracy: 0.6180\n",
      "Epoch 4/200\n",
      "175341/175341 [==============================] - 49s 282us/step - loss: 1.3803 - accuracy: 0.5312 - val_loss: 1.2811 - val_accuracy: 0.6177\n",
      "Epoch 5/200\n",
      "175341/175341 [==============================] - 51s 294us/step - loss: 1.3701 - accuracy: 0.5323 - val_loss: 1.2821 - val_accuracy: 0.6188\n",
      "Epoch 6/200\n",
      "175341/175341 [==============================] - 51s 288us/step - loss: 1.3600 - accuracy: 0.5335 - val_loss: 1.2676 - val_accuracy: 0.6190\n",
      "Epoch 7/200\n",
      "175341/175341 [==============================] - 50s 286us/step - loss: 1.3486 - accuracy: 0.5337 - val_loss: 1.2587 - val_accuracy: 0.6193\n",
      "Epoch 8/200\n",
      "175341/175341 [==============================] - 53s 302us/step - loss: 1.3337 - accuracy: 0.5339 - val_loss: 1.2520 - val_accuracy: 0.6193\n",
      "Epoch 9/200\n",
      "175341/175341 [==============================] - 50s 284us/step - loss: 1.3166 - accuracy: 0.5341 - val_loss: 1.2308 - val_accuracy: 0.6194\n",
      "Epoch 10/200\n",
      "175341/175341 [==============================] - 50s 285us/step - loss: 1.2961 - accuracy: 0.5362 - val_loss: 1.2398 - val_accuracy: 0.6182\n",
      "Epoch 11/200\n",
      "175341/175341 [==============================] - 52s 294us/step - loss: 1.2790 - accuracy: 0.5374 - val_loss: 1.2144 - val_accuracy: 0.6201\n",
      "Epoch 12/200\n",
      "175341/175341 [==============================] - 49s 282us/step - loss: 1.2623 - accuracy: 0.5407 - val_loss: 1.2127 - val_accuracy: 0.6169\n",
      "Epoch 13/200\n",
      "175341/175341 [==============================] - 49s 282us/step - loss: 1.2463 - accuracy: 0.5459 - val_loss: 1.2135 - val_accuracy: 0.6173\n",
      "Epoch 14/200\n",
      "175341/175341 [==============================] - 51s 289us/step - loss: 1.2338 - accuracy: 0.5542 - val_loss: 1.1922 - val_accuracy: 0.6205\n",
      "Epoch 15/200\n",
      "175341/175341 [==============================] - 49s 278us/step - loss: 1.2233 - accuracy: 0.5644 - val_loss: 1.1848 - val_accuracy: 0.6502\n",
      "Epoch 16/200\n",
      "175341/175341 [==============================] - 48s 274us/step - loss: 1.2113 - accuracy: 0.5764 - val_loss: 1.1889 - val_accuracy: 0.6510\n",
      "Epoch 17/200\n",
      "175341/175341 [==============================] - 50s 283us/step - loss: 1.2024 - accuracy: 0.5831 - val_loss: 1.1895 - val_accuracy: 0.6445\n",
      "Epoch 18/200\n",
      "175341/175341 [==============================] - 49s 277us/step - loss: 1.1941 - accuracy: 0.5861 - val_loss: 1.1838 - val_accuracy: 0.6456\n",
      "Epoch 19/200\n",
      "175341/175341 [==============================] - 49s 278us/step - loss: 1.1861 - accuracy: 0.5883 - val_loss: 1.1641 - val_accuracy: 0.6509\n",
      "Epoch 20/200\n",
      "175341/175341 [==============================] - 51s 291us/step - loss: 1.1812 - accuracy: 0.5886 - val_loss: 1.1693 - val_accuracy: 0.6472\n",
      "Epoch 21/200\n",
      "175341/175341 [==============================] - 50s 284us/step - loss: 1.1775 - accuracy: 0.5881 - val_loss: 1.1790 - val_accuracy: 0.6456\n",
      "Epoch 22/200\n",
      "175341/175341 [==============================] - 53s 304us/step - loss: 1.1736 - accuracy: 0.5899 - val_loss: 1.1858 - val_accuracy: 0.6372\n",
      "Epoch 23/200\n",
      "175341/175341 [==============================] - 58s 331us/step - loss: 1.1711 - accuracy: 0.5902 - val_loss: 1.1757 - val_accuracy: 0.6453\n",
      "Epoch 24/200\n",
      "175341/175341 [==============================] - 52s 295us/step - loss: 1.1683 - accuracy: 0.5896 - val_loss: 1.1975 - val_accuracy: 0.6292\n",
      "Epoch 25/200\n",
      "175341/175341 [==============================] - 54s 309us/step - loss: 1.1683 - accuracy: 0.5893 - val_loss: 1.1912 - val_accuracy: 0.6304\n",
      "Epoch 26/200\n",
      "175341/175341 [==============================] - 55s 312us/step - loss: 1.1658 - accuracy: 0.5904 - val_loss: 1.2183 - val_accuracy: 0.5725\n",
      "Epoch 27/200\n",
      "175341/175341 [==============================] - 53s 301us/step - loss: 1.1650 - accuracy: 0.5903 - val_loss: 1.1946 - val_accuracy: 0.6307\n",
      "Epoch 28/200\n",
      "175341/175341 [==============================] - 55s 316us/step - loss: 1.1626 - accuracy: 0.5903 - val_loss: 1.1981 - val_accuracy: 0.6299\n",
      "Epoch 29/200\n",
      "175341/175341 [==============================] - 56s 321us/step - loss: 1.1628 - accuracy: 0.5904 - val_loss: 1.1678 - val_accuracy: 0.6472\n",
      "Epoch 30/200\n",
      "175341/175341 [==============================] - 53s 305us/step - loss: 1.1621 - accuracy: 0.5908 - val_loss: 1.1641 - val_accuracy: 0.6523\n",
      "Epoch 31/200\n",
      "175341/175341 [==============================] - 54s 309us/step - loss: 1.1614 - accuracy: 0.5910 - val_loss: 1.1922 - val_accuracy: 0.6342\n",
      "Epoch 32/200\n",
      "175341/175341 [==============================] - 57s 325us/step - loss: 1.1604 - accuracy: 0.5898 - val_loss: 1.2017 - val_accuracy: 0.6084\n",
      "Epoch 33/200\n",
      "175341/175341 [==============================] - 53s 304us/step - loss: 1.1601 - accuracy: 0.5903 - val_loss: 1.1907 - val_accuracy: 0.6320\n",
      "Epoch 34/200\n",
      "175341/175341 [==============================] - 56s 318us/step - loss: 1.1611 - accuracy: 0.5905 - val_loss: 1.2096 - val_accuracy: 0.5544\n",
      "Epoch 35/200\n",
      "175341/175341 [==============================] - 53s 302us/step - loss: 1.1598 - accuracy: 0.5920 - val_loss: 1.1707 - val_accuracy: 0.6450\n",
      "Epoch 36/200\n",
      "175341/175341 [==============================] - 54s 308us/step - loss: 1.1583 - accuracy: 0.5916 - val_loss: 1.1694 - val_accuracy: 0.6444\n",
      "Epoch 37/200\n",
      "175341/175341 [==============================] - 55s 314us/step - loss: 1.1580 - accuracy: 0.5902 - val_loss: 1.2021 - val_accuracy: 0.6250\n",
      "Epoch 38/200\n",
      "175341/175341 [==============================] - 55s 313us/step - loss: 1.1573 - accuracy: 0.5905 - val_loss: 1.1839 - val_accuracy: 0.6389\n",
      "Epoch 39/200\n",
      "175341/175341 [==============================] - 52s 299us/step - loss: 1.1571 - accuracy: 0.5911 - val_loss: 1.1995 - val_accuracy: 0.6218\n",
      "Epoch 40/200\n",
      "175341/175341 [==============================] - 54s 309us/step - loss: 1.1565 - accuracy: 0.5921 - val_loss: 1.1961 - val_accuracy: 0.6132\n",
      "Epoch 41/200\n",
      "175341/175341 [==============================] - 52s 297us/step - loss: 1.1559 - accuracy: 0.5915 - val_loss: 1.1884 - val_accuracy: 0.6439\n",
      "Epoch 42/200\n",
      "175341/175341 [==============================] - 52s 295us/step - loss: 1.1565 - accuracy: 0.5920 - val_loss: 1.1906 - val_accuracy: 0.6296\n",
      "Epoch 43/200\n",
      "175341/175341 [==============================] - 54s 309us/step - loss: 1.1557 - accuracy: 0.5918 - val_loss: 1.1885 - val_accuracy: 0.6386\n",
      "Epoch 44/200\n",
      "175341/175341 [==============================] - 53s 303us/step - loss: 1.1552 - accuracy: 0.5914 - val_loss: 1.1857 - val_accuracy: 0.6368\n",
      "Epoch 45/200\n",
      "175341/175341 [==============================] - 50s 288us/step - loss: 1.1558 - accuracy: 0.5907 - val_loss: 1.1741 - val_accuracy: 0.6479\n",
      "Epoch 46/200\n",
      "175341/175341 [==============================] - 54s 310us/step - loss: 1.1548 - accuracy: 0.5922 - val_loss: 1.1736 - val_accuracy: 0.6505\n",
      "Epoch 47/200\n",
      "175341/175341 [==============================] - 52s 296us/step - loss: 1.1553 - accuracy: 0.5913 - val_loss: 1.1825 - val_accuracy: 0.6422\n",
      "Epoch 48/200\n",
      "175341/175341 [==============================] - 50s 285us/step - loss: 1.1537 - accuracy: 0.5918 - val_loss: 1.1897 - val_accuracy: 0.6339\n",
      "Epoch 49/200\n",
      "175341/175341 [==============================] - 54s 306us/step - loss: 1.1530 - accuracy: 0.5915 - val_loss: 1.1911 - val_accuracy: 0.6291\n",
      "Epoch 50/200\n",
      "175341/175341 [==============================] - 53s 302us/step - loss: 1.1531 - accuracy: 0.5922 - val_loss: 1.2088 - val_accuracy: 0.6050\n",
      "Epoch 51/200\n",
      "175341/175341 [==============================] - 52s 298us/step - loss: 1.1521 - accuracy: 0.5923 - val_loss: 1.1724 - val_accuracy: 0.6458\n",
      "Epoch 52/200\n",
      "175341/175341 [==============================] - 54s 311us/step - loss: 1.1530 - accuracy: 0.5914 - val_loss: 1.2179 - val_accuracy: 0.5317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/200\n",
      "175341/175341 [==============================] - 51s 291us/step - loss: 1.1537 - accuracy: 0.5916 - val_loss: 1.1650 - val_accuracy: 0.6481\n",
      "Epoch 54/200\n",
      "175341/175341 [==============================] - 51s 290us/step - loss: 1.1508 - accuracy: 0.5919 - val_loss: 1.2078 - val_accuracy: 0.6155\n",
      "Epoch 55/200\n",
      "175341/175341 [==============================] - 51s 289us/step - loss: 1.1518 - accuracy: 0.5919 - val_loss: 1.1790 - val_accuracy: 0.6404\n",
      "Epoch 56/200\n",
      "175341/175341 [==============================] - 50s 287us/step - loss: 1.1514 - accuracy: 0.5923 - val_loss: 1.1728 - val_accuracy: 0.6455\n",
      "Epoch 57/200\n",
      "175341/175341 [==============================] - 51s 292us/step - loss: 1.1514 - accuracy: 0.5921 - val_loss: 1.1798 - val_accuracy: 0.6443\n",
      "Epoch 58/200\n",
      "175341/175341 [==============================] - 52s 298us/step - loss: 1.1520 - accuracy: 0.5914 - val_loss: 1.1905 - val_accuracy: 0.6210\n",
      "Epoch 59/200\n",
      "175341/175341 [==============================] - 52s 299us/step - loss: 1.1508 - accuracy: 0.5913 - val_loss: 1.1878 - val_accuracy: 0.6334\n",
      "Epoch 60/200\n",
      "175341/175341 [==============================] - 54s 309us/step - loss: 1.1497 - accuracy: 0.5914 - val_loss: 1.1877 - val_accuracy: 0.6354\n",
      "Epoch 61/200\n",
      "175341/175341 [==============================] - 55s 311us/step - loss: 1.1499 - accuracy: 0.5924 - val_loss: 1.1980 - val_accuracy: 0.6162\n",
      "Epoch 62/200\n",
      "175341/175341 [==============================] - 51s 293us/step - loss: 1.1495 - accuracy: 0.5917 - val_loss: 1.1904 - val_accuracy: 0.6304\n",
      "Epoch 63/200\n",
      "175341/175341 [==============================] - 55s 315us/step - loss: 1.1486 - accuracy: 0.5923 - val_loss: 1.2076 - val_accuracy: 0.5943\n",
      "Epoch 64/200\n",
      "175341/175341 [==============================] - 54s 305us/step - loss: 1.1499 - accuracy: 0.5919 - val_loss: 1.2006 - val_accuracy: 0.6060\n",
      "Epoch 65/200\n",
      "175341/175341 [==============================] - 52s 297us/step - loss: 1.1496 - accuracy: 0.5917 - val_loss: 1.1828 - val_accuracy: 0.6408\n",
      "Epoch 66/200\n",
      "175341/175341 [==============================] - 55s 312us/step - loss: 1.1499 - accuracy: 0.5920 - val_loss: 1.1802 - val_accuracy: 0.6415\n",
      "Epoch 67/200\n",
      "175341/175341 [==============================] - 54s 308us/step - loss: 1.1487 - accuracy: 0.5912 - val_loss: 1.1897 - val_accuracy: 0.6353\n",
      "Epoch 68/200\n",
      "175341/175341 [==============================] - 53s 301us/step - loss: 1.1498 - accuracy: 0.5918 - val_loss: 1.1651 - val_accuracy: 0.6451\n",
      "Epoch 69/200\n",
      "175341/175341 [==============================] - 55s 315us/step - loss: 1.1473 - accuracy: 0.5914 - val_loss: 1.1806 - val_accuracy: 0.6445\n",
      "Epoch 70/200\n",
      "175341/175341 [==============================] - 53s 303us/step - loss: 1.1474 - accuracy: 0.5919 - val_loss: 1.1813 - val_accuracy: 0.6442\n",
      "Epoch 71/200\n",
      "175341/175341 [==============================] - 55s 312us/step - loss: 1.1461 - accuracy: 0.5916 - val_loss: 1.1770 - val_accuracy: 0.6447\n",
      "Epoch 72/200\n",
      "175341/175341 [==============================] - 54s 306us/step - loss: 1.1466 - accuracy: 0.5916 - val_loss: 1.2172 - val_accuracy: 0.5999\n",
      "Epoch 73/200\n",
      "175341/175341 [==============================] - 54s 308us/step - loss: 1.1470 - accuracy: 0.5918 - val_loss: 1.2031 - val_accuracy: 0.6153\n",
      "Epoch 74/200\n",
      "175341/175341 [==============================] - 56s 318us/step - loss: 1.1460 - accuracy: 0.5915 - val_loss: 1.1714 - val_accuracy: 0.6491\n",
      "Epoch 75/200\n",
      "175341/175341 [==============================] - 54s 306us/step - loss: 1.1463 - accuracy: 0.5921 - val_loss: 1.1953 - val_accuracy: 0.6333\n",
      "Epoch 76/200\n",
      "175341/175341 [==============================] - 55s 312us/step - loss: 1.1460 - accuracy: 0.5919 - val_loss: 1.1963 - val_accuracy: 0.6305\n",
      "Epoch 77/200\n",
      "175341/175341 [==============================] - 57s 324us/step - loss: 1.1468 - accuracy: 0.5924 - val_loss: 1.1670 - val_accuracy: 0.6495\n",
      "Epoch 78/200\n",
      "175341/175341 [==============================] - 55s 311us/step - loss: 1.1463 - accuracy: 0.5921 - val_loss: 1.1805 - val_accuracy: 0.6472\n",
      "Epoch 79/200\n",
      "175341/175341 [==============================] - 54s 308us/step - loss: 1.1475 - accuracy: 0.5923 - val_loss: 1.1718 - val_accuracy: 0.6432\n",
      "Epoch 80/200\n",
      "175341/175341 [==============================] - 56s 317us/step - loss: 1.1451 - accuracy: 0.5919 - val_loss: 1.1871 - val_accuracy: 0.6381\n",
      "Epoch 81/200\n",
      "175341/175341 [==============================] - 53s 300us/step - loss: 1.1452 - accuracy: 0.5921 - val_loss: 1.1749 - val_accuracy: 0.6370\n",
      "Epoch 82/200\n",
      "175341/175341 [==============================] - 55s 312us/step - loss: 1.1443 - accuracy: 0.5923 - val_loss: 1.1912 - val_accuracy: 0.6311\n",
      "Epoch 83/200\n",
      "175341/175341 [==============================] - 55s 316us/step - loss: 1.1446 - accuracy: 0.5922 - val_loss: 1.1866 - val_accuracy: 0.6256\n",
      "Epoch 84/200\n",
      "175341/175341 [==============================] - 53s 300us/step - loss: 1.1437 - accuracy: 0.5929 - val_loss: 1.1772 - val_accuracy: 0.6373\n",
      "Epoch 85/200\n",
      "175341/175341 [==============================] - 53s 304us/step - loss: 1.1436 - accuracy: 0.5925 - val_loss: 1.1861 - val_accuracy: 0.6433\n",
      "Epoch 86/200\n",
      "175341/175341 [==============================] - 54s 311us/step - loss: 1.1431 - accuracy: 0.5924 - val_loss: 1.1908 - val_accuracy: 0.6327\n",
      "Epoch 87/200\n",
      "175341/175341 [==============================] - 49s 282us/step - loss: 1.1460 - accuracy: 0.5915 - val_loss: 1.1845 - val_accuracy: 0.6436\n",
      "Epoch 88/200\n",
      "175341/175341 [==============================] - 52s 294us/step - loss: 1.1447 - accuracy: 0.5921 - val_loss: 1.1838 - val_accuracy: 0.6415\n",
      "Epoch 89/200\n",
      "175341/175341 [==============================] - 53s 304us/step - loss: 1.1433 - accuracy: 0.5911 - val_loss: 1.1861 - val_accuracy: 0.6374\n",
      "Epoch 90/200\n",
      "175341/175341 [==============================] - 49s 282us/step - loss: 1.1425 - accuracy: 0.5923 - val_loss: 1.2211 - val_accuracy: 0.5296\n",
      "Epoch 91/200\n",
      "175341/175341 [==============================] - 50s 286us/step - loss: 1.1441 - accuracy: 0.5924 - val_loss: 1.1597 - val_accuracy: 0.6477\n",
      "Epoch 92/200\n",
      "175341/175341 [==============================] - 51s 292us/step - loss: 1.1436 - accuracy: 0.5925 - val_loss: 1.2020 - val_accuracy: 0.6302\n",
      "Epoch 93/200\n",
      "175341/175341 [==============================] - 49s 281us/step - loss: 1.1431 - accuracy: 0.5916 - val_loss: 1.1841 - val_accuracy: 0.6346\n",
      "Epoch 94/200\n",
      "175341/175341 [==============================] - 49s 282us/step - loss: 1.1433 - accuracy: 0.5923 - val_loss: 1.1870 - val_accuracy: 0.6425\n",
      "Epoch 95/200\n",
      "175341/175341 [==============================] - 53s 300us/step - loss: 1.1435 - accuracy: 0.5922 - val_loss: 1.1705 - val_accuracy: 0.6478\n",
      "Epoch 96/200\n",
      "175341/175341 [==============================] - 49s 281us/step - loss: 1.1415 - accuracy: 0.5922 - val_loss: 1.2108 - val_accuracy: 0.6108\n",
      "Epoch 97/200\n",
      "175341/175341 [==============================] - 50s 288us/step - loss: 1.1419 - accuracy: 0.5924 - val_loss: 1.2122 - val_accuracy: 0.5856\n",
      "Epoch 98/200\n",
      "175341/175341 [==============================] - 52s 299us/step - loss: 1.1429 - accuracy: 0.5926 - val_loss: 1.1708 - val_accuracy: 0.6453\n",
      "Epoch 99/200\n",
      "175341/175341 [==============================] - 49s 279us/step - loss: 1.1423 - accuracy: 0.5918 - val_loss: 1.1826 - val_accuracy: 0.6468\n",
      "Epoch 100/200\n",
      "175341/175341 [==============================] - 52s 295us/step - loss: 1.1416 - accuracy: 0.5922 - val_loss: 1.2188 - val_accuracy: 0.5843\n",
      "Epoch 101/200\n",
      "175341/175341 [==============================] - 53s 304us/step - loss: 1.1415 - accuracy: 0.5925 - val_loss: 1.2074 - val_accuracy: 0.6347\n",
      "Epoch 102/200\n",
      "175341/175341 [==============================] - 49s 279us/step - loss: 1.1411 - accuracy: 0.5923 - val_loss: 1.1846 - val_accuracy: 0.6414\n",
      "Epoch 103/200\n",
      "175341/175341 [==============================] - 51s 292us/step - loss: 1.1409 - accuracy: 0.5925 - val_loss: 1.2017 - val_accuracy: 0.6130\n",
      "Epoch 104/200\n",
      "175341/175341 [==============================] - 53s 301us/step - loss: 1.1413 - accuracy: 0.5931 - val_loss: 1.1841 - val_accuracy: 0.6285\n",
      "Epoch 105/200\n",
      "175341/175341 [==============================] - 50s 283us/step - loss: 1.1410 - accuracy: 0.5921 - val_loss: 1.1874 - val_accuracy: 0.6338\n",
      "Epoch 106/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175341/175341 [==============================] - 50s 286us/step - loss: 1.1417 - accuracy: 0.5921 - val_loss: 1.1963 - val_accuracy: 0.6235\n",
      "Epoch 107/200\n",
      "175341/175341 [==============================] - 52s 296us/step - loss: 1.1406 - accuracy: 0.5916 - val_loss: 1.1691 - val_accuracy: 0.6449\n",
      "Epoch 108/200\n",
      "175341/175341 [==============================] - 50s 284us/step - loss: 1.1408 - accuracy: 0.5930 - val_loss: 1.1835 - val_accuracy: 0.6409\n",
      "Epoch 109/200\n",
      "175341/175341 [==============================] - 51s 292us/step - loss: 1.1395 - accuracy: 0.5928 - val_loss: 1.1793 - val_accuracy: 0.6430\n",
      "Epoch 110/200\n",
      "175341/175341 [==============================] - 52s 299us/step - loss: 1.1400 - accuracy: 0.5931 - val_loss: 1.1826 - val_accuracy: 0.6421\n",
      "Epoch 111/200\n",
      "175341/175341 [==============================] - 51s 290us/step - loss: 1.1404 - accuracy: 0.5929 - val_loss: 1.2182 - val_accuracy: 0.6310\n",
      "Epoch 112/200\n",
      "175341/175341 [==============================] - 52s 295us/step - loss: 1.1407 - accuracy: 0.5923 - val_loss: 1.1787 - val_accuracy: 0.6410\n",
      "Epoch 113/200\n",
      "175341/175341 [==============================] - 53s 305us/step - loss: 1.1395 - accuracy: 0.5931 - val_loss: 1.1899 - val_accuracy: 0.6385\n",
      "Epoch 114/200\n",
      "175341/175341 [==============================] - 50s 283us/step - loss: 1.1389 - accuracy: 0.5938 - val_loss: 1.2063 - val_accuracy: 0.6259\n",
      "Epoch 115/200\n",
      "175341/175341 [==============================] - 51s 289us/step - loss: 1.1394 - accuracy: 0.5923 - val_loss: 1.1670 - val_accuracy: 0.6471\n",
      "Epoch 116/200\n",
      "175341/175341 [==============================] - 52s 298us/step - loss: 1.1409 - accuracy: 0.5930 - val_loss: 1.1902 - val_accuracy: 0.6349\n",
      "Epoch 117/200\n",
      "175341/175341 [==============================] - 51s 293us/step - loss: 1.1399 - accuracy: 0.5928 - val_loss: 1.1945 - val_accuracy: 0.6291\n",
      "Epoch 118/200\n",
      "175341/175341 [==============================] - 50s 287us/step - loss: 1.1394 - accuracy: 0.5930 - val_loss: 1.1887 - val_accuracy: 0.6330\n",
      "Epoch 119/200\n",
      "175341/175341 [==============================] - 53s 302us/step - loss: 1.1395 - accuracy: 0.5929 - val_loss: 1.1904 - val_accuracy: 0.6375\n",
      "Epoch 120/200\n",
      "175341/175341 [==============================] - 51s 290us/step - loss: 1.1386 - accuracy: 0.5936 - val_loss: 1.1676 - val_accuracy: 0.6454\n",
      "Epoch 121/200\n",
      "175341/175341 [==============================] - 50s 285us/step - loss: 1.1379 - accuracy: 0.5934 - val_loss: 1.2101 - val_accuracy: 0.6002\n",
      "Epoch 122/200\n",
      "175341/175341 [==============================] - 51s 291us/step - loss: 1.1387 - accuracy: 0.5931 - val_loss: 1.2308 - val_accuracy: 0.5347\n",
      "Epoch 123/200\n",
      "175341/175341 [==============================] - 52s 298us/step - loss: 1.1386 - accuracy: 0.5932 - val_loss: 1.2016 - val_accuracy: 0.6343\n",
      "Epoch 124/200\n",
      "175341/175341 [==============================] - 49s 280us/step - loss: 1.1395 - accuracy: 0.5941 - val_loss: 1.2097 - val_accuracy: 0.6267\n",
      "Epoch 125/200\n",
      "175341/175341 [==============================] - 52s 297us/step - loss: 1.1389 - accuracy: 0.5931 - val_loss: 1.1790 - val_accuracy: 0.6319\n",
      "Epoch 126/200\n",
      "175341/175341 [==============================] - 53s 300us/step - loss: 1.1376 - accuracy: 0.5926 - val_loss: 1.1878 - val_accuracy: 0.6308\n",
      "Epoch 127/200\n",
      "175341/175341 [==============================] - 49s 278us/step - loss: 1.1382 - accuracy: 0.5925 - val_loss: 1.2025 - val_accuracy: 0.6287\n",
      "Epoch 128/200\n",
      "175341/175341 [==============================] - 50s 284us/step - loss: 1.1385 - accuracy: 0.5927 - val_loss: 1.2112 - val_accuracy: 0.5994\n",
      "Epoch 129/200\n",
      "175341/175341 [==============================] - 51s 289us/step - loss: 1.1396 - accuracy: 0.5928 - val_loss: 1.2271 - val_accuracy: 0.5302\n",
      "Epoch 130/200\n",
      "175341/175341 [==============================] - 48s 272us/step - loss: 1.1384 - accuracy: 0.5934 - val_loss: 1.1794 - val_accuracy: 0.6425\n",
      "Epoch 131/200\n",
      "175341/175341 [==============================] - 49s 279us/step - loss: 1.1378 - accuracy: 0.5926 - val_loss: 1.2112 - val_accuracy: 0.6076\n",
      "Epoch 132/200\n",
      "175341/175341 [==============================] - 51s 290us/step - loss: 1.1384 - accuracy: 0.5932 - val_loss: 1.2049 - val_accuracy: 0.6225\n",
      "Epoch 133/200\n",
      "175341/175341 [==============================] - 47s 268us/step - loss: 1.1372 - accuracy: 0.5931 - val_loss: 1.1823 - val_accuracy: 0.6280\n",
      "Epoch 134/200\n",
      "175341/175341 [==============================] - 49s 279us/step - loss: 1.1366 - accuracy: 0.5940 - val_loss: 1.1891 - val_accuracy: 0.6378\n",
      "Epoch 135/200\n",
      "175341/175341 [==============================] - 50s 284us/step - loss: 1.1371 - accuracy: 0.5939 - val_loss: 1.1850 - val_accuracy: 0.6345\n",
      "Epoch 136/200\n",
      "175341/175341 [==============================] - 48s 274us/step - loss: 1.1368 - accuracy: 0.5931 - val_loss: 1.1756 - val_accuracy: 0.6438\n",
      "Epoch 137/200\n",
      "175341/175341 [==============================] - 48s 274us/step - loss: 1.1384 - accuracy: 0.5930 - val_loss: 1.1675 - val_accuracy: 0.6477\n",
      "Epoch 138/200\n",
      "175341/175341 [==============================] - 51s 290us/step - loss: 1.1366 - accuracy: 0.5928 - val_loss: 1.1894 - val_accuracy: 0.6441\n",
      "Epoch 139/200\n",
      "175341/175341 [==============================] - 47s 271us/step - loss: 1.1372 - accuracy: 0.5932 - val_loss: 1.1879 - val_accuracy: 0.6289\n",
      "Epoch 140/200\n",
      "175341/175341 [==============================] - 49s 281us/step - loss: 1.1363 - accuracy: 0.5941 - val_loss: 1.1697 - val_accuracy: 0.6417\n",
      "Epoch 141/200\n",
      "175341/175341 [==============================] - 50s 284us/step - loss: 1.1373 - accuracy: 0.5936 - val_loss: 1.1849 - val_accuracy: 0.6414\n",
      "Epoch 142/200\n",
      "175341/175341 [==============================] - 47s 269us/step - loss: 1.1371 - accuracy: 0.5938 - val_loss: 1.2134 - val_accuracy: 0.6012\n",
      "Epoch 143/200\n",
      "175341/175341 [==============================] - 47s 266us/step - loss: 1.1365 - accuracy: 0.5943 - val_loss: 1.1868 - val_accuracy: 0.6260\n",
      "Epoch 144/200\n",
      "175341/175341 [==============================] - 49s 278us/step - loss: 1.1365 - accuracy: 0.5941 - val_loss: 1.1710 - val_accuracy: 0.6458\n",
      "Epoch 145/200\n",
      "175341/175341 [==============================] - 47s 265us/step - loss: 1.1366 - accuracy: 0.5941 - val_loss: 1.1895 - val_accuracy: 0.6361\n",
      "Epoch 146/200\n",
      "175341/175341 [==============================] - 46s 264us/step - loss: 1.1372 - accuracy: 0.5943 - val_loss: 1.2126 - val_accuracy: 0.6085\n",
      "Epoch 147/200\n",
      "175341/175341 [==============================] - 48s 275us/step - loss: 1.1360 - accuracy: 0.5935 - val_loss: 1.1639 - val_accuracy: 0.6466\n",
      "Epoch 148/200\n",
      "175341/175341 [==============================] - 47s 269us/step - loss: 1.1359 - accuracy: 0.5938 - val_loss: 1.1823 - val_accuracy: 0.6335\n",
      "Epoch 149/200\n",
      "175341/175341 [==============================] - 47s 270us/step - loss: 1.1351 - accuracy: 0.5936 - val_loss: 1.2041 - val_accuracy: 0.6307\n",
      "Epoch 150/200\n",
      "175341/175341 [==============================] - 48s 276us/step - loss: 1.1348 - accuracy: 0.5944 - val_loss: 1.1762 - val_accuracy: 0.6328\n",
      "Epoch 151/200\n",
      "175341/175341 [==============================] - 46s 263us/step - loss: 1.1361 - accuracy: 0.5940 - val_loss: 1.1750 - val_accuracy: 0.6316\n",
      "Epoch 152/200\n",
      "175341/175341 [==============================] - 46s 263us/step - loss: 1.1360 - accuracy: 0.5945 - val_loss: 1.2139 - val_accuracy: 0.6108\n",
      "Epoch 153/200\n",
      "175341/175341 [==============================] - 50s 283us/step - loss: 1.1356 - accuracy: 0.5945 - val_loss: 1.1728 - val_accuracy: 0.6452\n",
      "Epoch 154/200\n",
      "175341/175341 [==============================] - 47s 266us/step - loss: 1.1357 - accuracy: 0.5930 - val_loss: 1.1697 - val_accuracy: 0.6459\n",
      "Epoch 155/200\n",
      "175341/175341 [==============================] - 49s 279us/step - loss: 1.1345 - accuracy: 0.5935 - val_loss: 1.1950 - val_accuracy: 0.6331\n",
      "Epoch 156/200\n",
      "175341/175341 [==============================] - 49s 280us/step - loss: 1.1351 - accuracy: 0.5941 - val_loss: 1.1618 - val_accuracy: 0.6474\n",
      "Epoch 157/200\n",
      "175341/175341 [==============================] - 46s 265us/step - loss: 1.1368 - accuracy: 0.5940 - val_loss: 1.1884 - val_accuracy: 0.6014\n",
      "Epoch 158/200\n",
      "175341/175341 [==============================] - 47s 267us/step - loss: 1.1350 - accuracy: 0.5940 - val_loss: 1.1891 - val_accuracy: 0.6356\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175341/175341 [==============================] - 49s 280us/step - loss: 1.1352 - accuracy: 0.5934 - val_loss: 1.2022 - val_accuracy: 0.6222\n",
      "Epoch 160/200\n",
      "175341/175341 [==============================] - 47s 267us/step - loss: 1.1358 - accuracy: 0.5933 - val_loss: 1.2136 - val_accuracy: 0.6162\n",
      "Epoch 161/200\n",
      "175341/175341 [==============================] - 47s 269us/step - loss: 1.1331 - accuracy: 0.5943 - val_loss: 1.1876 - val_accuracy: 0.6314\n",
      "Epoch 162/200\n",
      "175341/175341 [==============================] - 49s 281us/step - loss: 1.1357 - accuracy: 0.5946 - val_loss: 1.1901 - val_accuracy: 0.6246\n",
      "Epoch 163/200\n",
      "175341/175341 [==============================] - 46s 265us/step - loss: 1.1350 - accuracy: 0.5942 - val_loss: 1.2107 - val_accuracy: 0.6193\n",
      "Epoch 164/200\n",
      "175341/175341 [==============================] - 46s 264us/step - loss: 1.1335 - accuracy: 0.5946 - val_loss: 1.1868 - val_accuracy: 0.6290\n",
      "Epoch 165/200\n",
      "175341/175341 [==============================] - 48s 275us/step - loss: 1.1343 - accuracy: 0.5947 - val_loss: 1.1997 - val_accuracy: 0.6176\n",
      "Epoch 166/200\n",
      "175341/175341 [==============================] - 47s 268us/step - loss: 1.1349 - accuracy: 0.5949 - val_loss: 1.1827 - val_accuracy: 0.6339\n",
      "Epoch 167/200\n",
      "175341/175341 [==============================] - 47s 267us/step - loss: 1.1336 - accuracy: 0.5942 - val_loss: 1.1944 - val_accuracy: 0.6371\n",
      "Epoch 168/200\n",
      "175341/175341 [==============================] - 49s 280us/step - loss: 1.1344 - accuracy: 0.5948 - val_loss: 1.1987 - val_accuracy: 0.6315\n",
      "Epoch 169/200\n",
      "175341/175341 [==============================] - 47s 271us/step - loss: 1.1333 - accuracy: 0.5944 - val_loss: 1.1881 - val_accuracy: 0.6355\n",
      "Epoch 170/200\n",
      "175341/175341 [==============================] - 47s 268us/step - loss: 1.1336 - accuracy: 0.5956 - val_loss: 1.1821 - val_accuracy: 0.6339\n",
      "Epoch 171/200\n",
      "175341/175341 [==============================] - 50s 287us/step - loss: 1.1342 - accuracy: 0.5943 - val_loss: 1.2080 - val_accuracy: 0.6195\n",
      "Epoch 172/200\n",
      "175341/175341 [==============================] - 48s 273us/step - loss: 1.1332 - accuracy: 0.5953 - val_loss: 1.1875 - val_accuracy: 0.6287\n",
      "Epoch 173/200\n",
      "175341/175341 [==============================] - 47s 268us/step - loss: 1.1325 - accuracy: 0.5942 - val_loss: 1.1973 - val_accuracy: 0.6402\n",
      "Epoch 174/200\n",
      "175341/175341 [==============================] - 49s 279us/step - loss: 1.1342 - accuracy: 0.5960 - val_loss: 1.1837 - val_accuracy: 0.6333\n",
      "Epoch 175/200\n",
      "175341/175341 [==============================] - 49s 277us/step - loss: 1.1329 - accuracy: 0.5942 - val_loss: 1.1864 - val_accuracy: 0.6132\n",
      "Epoch 176/200\n",
      "175341/175341 [==============================] - 47s 268us/step - loss: 1.1326 - accuracy: 0.5952 - val_loss: 1.1844 - val_accuracy: 0.6362\n",
      "Epoch 177/200\n",
      "175341/175341 [==============================] - 49s 282us/step - loss: 1.1351 - accuracy: 0.5945 - val_loss: 1.1843 - val_accuracy: 0.6437\n",
      "Epoch 178/200\n",
      "175341/175341 [==============================] - 48s 273us/step - loss: 1.1326 - accuracy: 0.5949 - val_loss: 1.1796 - val_accuracy: 0.6356\n",
      "Epoch 179/200\n",
      "175341/175341 [==============================] - 46s 262us/step - loss: 1.1326 - accuracy: 0.5957 - val_loss: 1.1744 - val_accuracy: 0.6283\n",
      "Epoch 180/200\n",
      "175341/175341 [==============================] - 43s 244us/step - loss: 1.1333 - accuracy: 0.5943 - val_loss: 1.1746 - val_accuracy: 0.6445\n",
      "Epoch 181/200\n",
      "175341/175341 [==============================] - 38s 216us/step - loss: 1.1329 - accuracy: 0.5948 - val_loss: 1.1773 - val_accuracy: 0.6361\n",
      "Epoch 182/200\n",
      "175341/175341 [==============================] - 42s 238us/step - loss: 1.1334 - accuracy: 0.5945 - val_loss: 1.1868 - val_accuracy: 0.6320\n",
      "Epoch 183/200\n",
      "175341/175341 [==============================] - 42s 242us/step - loss: 1.1336 - accuracy: 0.5946 - val_loss: 1.1946 - val_accuracy: 0.6116\n",
      "Epoch 184/200\n",
      "175341/175341 [==============================] - 37s 210us/step - loss: 1.1320 - accuracy: 0.5950 - val_loss: 1.1804 - val_accuracy: 0.6394\n",
      "Epoch 185/200\n",
      "175341/175341 [==============================] - 42s 242us/step - loss: 1.1319 - accuracy: 0.5949 - val_loss: 1.2045 - val_accuracy: 0.6298\n",
      "Epoch 186/200\n",
      "175341/175341 [==============================] - 41s 233us/step - loss: 1.1340 - accuracy: 0.5945 - val_loss: 1.1819 - val_accuracy: 0.6415\n",
      "Epoch 187/200\n",
      "175341/175341 [==============================] - 39s 223us/step - loss: 1.1322 - accuracy: 0.5963 - val_loss: 1.1828 - val_accuracy: 0.6373\n",
      "Epoch 188/200\n",
      "175341/175341 [==============================] - 42s 242us/step - loss: 1.1321 - accuracy: 0.5948 - val_loss: 1.1827 - val_accuracy: 0.6251\n",
      "Epoch 189/200\n",
      "175341/175341 [==============================] - 38s 215us/step - loss: 1.1324 - accuracy: 0.5964 - val_loss: 1.1890 - val_accuracy: 0.6323\n",
      "Epoch 190/200\n",
      "175341/175341 [==============================] - 42s 241us/step - loss: 1.1332 - accuracy: 0.5946 - val_loss: 1.1921 - val_accuracy: 0.6397\n",
      "Epoch 191/200\n",
      "175341/175341 [==============================] - 42s 242us/step - loss: 1.1313 - accuracy: 0.5955 - val_loss: 1.1947 - val_accuracy: 0.6367\n",
      "Epoch 192/200\n",
      "175341/175341 [==============================] - 37s 210us/step - loss: 1.1317 - accuracy: 0.5959 - val_loss: 1.2276 - val_accuracy: 0.5311\n",
      "Epoch 193/200\n",
      "175341/175341 [==============================] - 42s 242us/step - loss: 1.1320 - accuracy: 0.5952 - val_loss: 1.1786 - val_accuracy: 0.6407\n",
      "Epoch 194/200\n",
      "175341/175341 [==============================] - 39s 223us/step - loss: 1.1311 - accuracy: 0.5962 - val_loss: 1.1812 - val_accuracy: 0.6396\n",
      "Epoch 195/200\n",
      "175341/175341 [==============================] - 41s 236us/step - loss: 1.1308 - accuracy: 0.5955 - val_loss: 1.2012 - val_accuracy: 0.6135\n",
      "Epoch 196/200\n",
      "175341/175341 [==============================] - 43s 245us/step - loss: 1.1307 - accuracy: 0.5956 - val_loss: 1.1900 - val_accuracy: 0.6366\n",
      "Epoch 197/200\n",
      "175341/175341 [==============================] - 37s 209us/step - loss: 1.1310 - accuracy: 0.5955 - val_loss: 1.1877 - val_accuracy: 0.6235\n",
      "Epoch 198/200\n",
      "175341/175341 [==============================] - 42s 242us/step - loss: 1.1309 - accuracy: 0.5968 - val_loss: 1.1903 - val_accuracy: 0.6342\n",
      "Epoch 199/200\n",
      "175341/175341 [==============================] - 41s 236us/step - loss: 1.1305 - accuracy: 0.5960 - val_loss: 1.1735 - val_accuracy: 0.6372\n",
      "Epoch 200/200\n",
      "175341/175341 [==============================] - 38s 217us/step - loss: 1.1311 - accuracy: 0.5957 - val_loss: 1.1866 - val_accuracy: 0.6312\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(XDelta1, Y, epochs=200, validation_data=(xdelta1, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03368428 0.03462839 0.24563344 0.36159346 0.16308738 0.00300522\n",
      "  0.05135835 0.08847929 0.01806662 0.00046367]\n",
      " [0.03367553 0.03532544 0.2504449  0.37808135 0.16750422 0.00061064\n",
      "  0.03841875 0.07542875 0.02007264 0.00043773]\n",
      " [0.03370727 0.03518355 0.24947913 0.374226   0.16648316 0.00091572\n",
      "  0.0414     0.0786015  0.01955903 0.00044463]\n",
      " [0.03371622 0.03510872 0.2489662  0.37232676 0.16597869 0.0011098\n",
      "  0.04288625 0.08014288 0.01931667 0.00044785]\n",
      " [0.03366456 0.03535938 0.25067428 0.37906432 0.16776522 0.00054887\n",
      "  0.03766744 0.07461137 0.02020862 0.0004359 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predictions = model.predict([xdelta1])\n",
    "print(predictions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "#pred = np.zeros((82332,1), dtype=int)\n",
    "for i in range(0, predictions.shape[0]):\n",
    "    pred = np.where(predictions[i] == np.amax(predictions[i]))\n",
    "    for j in range(1,10):\n",
    "        predictions[i,j] = 0\n",
    "    predictions[i,pred]=1\n",
    "    \n",
    "print(predictions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6311640674342904\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       677\n",
      "           1       0.00      0.00      0.00       583\n",
      "           2       0.36      0.00      0.00      4089\n",
      "           3       0.23      0.37      0.29     11132\n",
      "           4       0.42      0.00      0.01      6062\n",
      "           5       0.84      0.96      0.89     18871\n",
      "           6       0.69      0.80      0.74     37000\n",
      "           7       0.00      0.00      0.00      3496\n",
      "           8       0.00      0.00      0.00       378\n",
      "           9       0.00      0.00      0.00        44\n",
      "\n",
      "   micro avg       0.63      0.63      0.63     82332\n",
      "   macro avg       0.25      0.21      0.19     82332\n",
      "weighted avg       0.58      0.63      0.58     82332\n",
      " samples avg       0.63      0.63      0.63     82332\n",
      "\n",
      "[[    0     0     2   622     0     2    51     0     0     0]\n",
      " [    0     0     2   528     0     6    47     0     0     0]\n",
      " [    0     0    10  2986     6   139   948     0     0     0]\n",
      " [    0     0    10  4085     8   127  6902     0     0     0]\n",
      " [    0     0     4  2461    25   397  3175     0     0     0]\n",
      " [    0     0     0   223     0 18168   480     0     0     0]\n",
      " [    0     0     0  4483    20  2820 29677     0     0     0]\n",
      " [    0     0     0  1923     0    45  1528     0     0     0]\n",
      " [    0     0     0   184     0    33   161     0     0     0]\n",
      " [    0     0     0    10     0     2    32     0     0     0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Accuracy: \",accuracy_score(y,predictions))\n",
    "from sklearn.metrics import multilabel_confusion_matrix,classification_report,confusion_matrix\n",
    "print(\"Classification Report: \\n\",classification_report(y,predictions))\n",
    "conmat = np.array(confusion_matrix(y.argmax(axis=1), predictions.argmax(axis=1)))\n",
    "confusion = pd.DataFrame(conmat, index=['0','1','2','3','4','5','6','7','8','9'],\n",
    "                        columns=[['0','1','2','3','4','5','6','7','8','9']])\n",
    "print (conmat)\n",
    "conmat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
