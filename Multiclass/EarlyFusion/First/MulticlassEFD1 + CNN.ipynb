{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN, Bidirectional\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = pd.read_csv('/home/ritika/NetworkIntrusionDetection/UNSW-NB15 - CSV Files/a part of training and testing set/UNSW_NB15_training-set.csv')\n",
    "testdata = pd.read_csv('/home/ritika/NetworkIntrusionDetection/UNSW-NB15 - CSV Files/a part of training and testing set/UNSW_NB15_testing-set.csv')\n",
    "\n",
    "#traindata=traindata.drop('id',axis=1)\n",
    "#testdata=testdata.drop('id',axis=1)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb_make = LabelEncoder()\n",
    "\n",
    "traindata['proto'] = lb_make.fit_transform(traindata['proto'])\n",
    "traindata['service'] = lb_make.fit_transform(traindata['service'])\n",
    "traindata['state'] = lb_make.fit_transform(traindata['state'])\n",
    "traindata['attack_cat'] = lb_make.fit_transform(traindata['attack_cat'])\n",
    "\n",
    "testdata['proto'] = lb_make.fit_transform(testdata['proto'])\n",
    "testdata['service'] = lb_make.fit_transform(testdata['service'])\n",
    "testdata['state'] = lb_make.fit_transform(testdata['state'])\n",
    "testdata['attack_cat'] = lb_make.fit_transform(testdata['attack_cat'])\n",
    "\n",
    "X = traindata.iloc[:,1:42]\n",
    "Y = traindata.iloc[:,-2]\n",
    "x = testdata.iloc[:,1:42]\n",
    "y = testdata.iloc[:,-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arr = X.values\n",
    "arr = x.values\n",
    "\n",
    "import librosa\n",
    "Arr_Delta = librosa.feature.delta(Arr)\n",
    "arr_delta = librosa.feature.delta(arr)\n",
    "\n",
    "Delta1 = pd.DataFrame(Arr_Delta)\n",
    "delta1 = pd.DataFrame(arr_delta)\n",
    "\n",
    "XDelta1 = pd.concat([X, Delta1], axis=1)\n",
    "xdelta1 = pd.concat([x, delta1], axis=1)\n",
    "\n",
    "scaler = Normalizer().fit(XDelta1)\n",
    "XDelta1 = scaler.transform(XDelta1)\n",
    "scaler = Normalizer().fit(xdelta1)\n",
    "xdelta1 = scaler.transform(xdelta1)\n",
    "\n",
    "Y = np.array(Y)\n",
    "y = np.array(y)\n",
    "\n",
    "XDelta1 = np.reshape(XDelta1, (XDelta1.shape[0],XDelta1.shape[1],1))\n",
    "xdelta1 = np.reshape(xdelta1, (xdelta1.shape[0],xdelta1.shape[1],1))\n",
    "\n",
    "lstm_output_size = 70\n",
    "\n",
    "Y = to_categorical(Y)\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", input_shape=(82, 1), padding=\"same\")`\n",
      "  \n",
      "/home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 82, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 41, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2624)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               336000    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 337,546\n",
      "Trainable params: 337,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Convolution1D(64, 3, border_mode=\"same\",activation=\"relu\",input_shape=(82, 1)))\n",
    "cnn.add(MaxPooling1D(pool_length=(2)))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(128, activation=\"relu\"))\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(10, activation=\"softmax\"))\n",
    "cnn.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 175341 samples, validate on 82332 samples\n",
      "Epoch 1/200\n",
      "175341/175341 [==============================] - 119s 677us/step - loss: 1.3927 - accuracy: 0.5303 - val_loss: 1.2809 - val_accuracy: 0.6178\n",
      "Epoch 2/200\n",
      "175341/175341 [==============================] - 114s 648us/step - loss: 1.3471 - accuracy: 0.5354 - val_loss: 1.2767 - val_accuracy: 0.6186\n",
      "Epoch 3/200\n",
      "175341/175341 [==============================] - 114s 649us/step - loss: 1.3350 - accuracy: 0.5405 - val_loss: 1.2874 - val_accuracy: 0.5959\n",
      "Epoch 4/200\n",
      "175341/175341 [==============================] - 121s 690us/step - loss: 1.3296 - accuracy: 0.5432 - val_loss: 1.2720 - val_accuracy: 0.5994\n",
      "Epoch 5/200\n",
      "175341/175341 [==============================] - 133s 759us/step - loss: 1.3251 - accuracy: 0.5436 - val_loss: 1.2829 - val_accuracy: 0.5924\n",
      "Epoch 6/200\n",
      "175341/175341 [==============================] - 141s 806us/step - loss: 1.3230 - accuracy: 0.5453 - val_loss: 1.2720 - val_accuracy: 0.5979\n",
      "Epoch 7/200\n",
      "175341/175341 [==============================] - 149s 851us/step - loss: 1.3219 - accuracy: 0.5458 - val_loss: 1.2724 - val_accuracy: 0.5868\n",
      "Epoch 8/200\n",
      "175341/175341 [==============================] - 151s 863us/step - loss: 1.3199 - accuracy: 0.5461 - val_loss: 1.2684 - val_accuracy: 0.5937\n",
      "Epoch 9/200\n",
      "175341/175341 [==============================] - 153s 873us/step - loss: 1.3188 - accuracy: 0.5472 - val_loss: 1.2734 - val_accuracy: 0.5825\n",
      "Epoch 10/200\n",
      "175341/175341 [==============================] - 148s 846us/step - loss: 1.3172 - accuracy: 0.5471 - val_loss: 1.2649 - val_accuracy: 0.5862\n",
      "Epoch 11/200\n",
      "175341/175341 [==============================] - 149s 848us/step - loss: 1.3169 - accuracy: 0.5472 - val_loss: 1.2626 - val_accuracy: 0.5994\n",
      "Epoch 12/200\n",
      "175341/175341 [==============================] - 156s 888us/step - loss: 1.3158 - accuracy: 0.5478 - val_loss: 1.2636 - val_accuracy: 0.5951\n",
      "Epoch 13/200\n",
      "175341/175341 [==============================] - 168s 958us/step - loss: 1.3153 - accuracy: 0.5483 - val_loss: 1.2682 - val_accuracy: 0.5953\n",
      "Epoch 14/200\n",
      "175341/175341 [==============================] - 170s 968us/step - loss: 1.3148 - accuracy: 0.5481 - val_loss: 1.2667 - val_accuracy: 0.5872\n",
      "Epoch 15/200\n",
      "175341/175341 [==============================] - 170s 970us/step - loss: 1.3153 - accuracy: 0.5482 - val_loss: 1.2586 - val_accuracy: 0.5945\n",
      "Epoch 16/200\n",
      "175341/175341 [==============================] - 171s 976us/step - loss: 1.3144 - accuracy: 0.5483 - val_loss: 1.2906 - val_accuracy: 0.5793\n",
      "Epoch 17/200\n",
      "175341/175341 [==============================] - 168s 960us/step - loss: 1.3144 - accuracy: 0.5488 - val_loss: 1.2823 - val_accuracy: 0.5858\n",
      "Epoch 18/200\n",
      "175341/175341 [==============================] - 167s 951us/step - loss: 1.3129 - accuracy: 0.5490 - val_loss: 1.2746 - val_accuracy: 0.5869\n",
      "Epoch 19/200\n",
      "175341/175341 [==============================] - 161s 919us/step - loss: 1.3123 - accuracy: 0.5488 - val_loss: 1.2828 - val_accuracy: 0.5839\n",
      "Epoch 20/200\n",
      "175341/175341 [==============================] - 162s 924us/step - loss: 1.3119 - accuracy: 0.5491 - val_loss: 1.2617 - val_accuracy: 0.5949\n",
      "Epoch 21/200\n",
      "175341/175341 [==============================] - 162s 926us/step - loss: 1.3120 - accuracy: 0.5499 - val_loss: 1.2806 - val_accuracy: 0.5879\n",
      "Epoch 22/200\n",
      "175341/175341 [==============================] - 160s 913us/step - loss: 1.3116 - accuracy: 0.5498 - val_loss: 1.2661 - val_accuracy: 0.5930\n",
      "Epoch 23/200\n",
      "175341/175341 [==============================] - 155s 883us/step - loss: 1.3117 - accuracy: 0.5494 - val_loss: 1.2774 - val_accuracy: 0.5889\n",
      "Epoch 24/200\n",
      "175341/175341 [==============================] - 160s 911us/step - loss: 1.3107 - accuracy: 0.5495 - val_loss: 1.2845 - val_accuracy: 0.5854\n",
      "Epoch 25/200\n",
      "175341/175341 [==============================] - 166s 949us/step - loss: 1.3101 - accuracy: 0.5492 - val_loss: 1.2741 - val_accuracy: 0.5912\n",
      "Epoch 26/200\n",
      "175341/175341 [==============================] - 165s 939us/step - loss: 1.3103 - accuracy: 0.5492 - val_loss: 1.2735 - val_accuracy: 0.5864\n",
      "Epoch 27/200\n",
      "175341/175341 [==============================] - 166s 947us/step - loss: 1.3101 - accuracy: 0.5498 - val_loss: 1.2861 - val_accuracy: 0.5872\n",
      "Epoch 28/200\n",
      "175341/175341 [==============================] - 167s 951us/step - loss: 1.3096 - accuracy: 0.5500 - val_loss: 1.2755 - val_accuracy: 0.5893\n",
      "Epoch 29/200\n",
      "175341/175341 [==============================] - 169s 961us/step - loss: 1.3091 - accuracy: 0.5502 - val_loss: 1.2694 - val_accuracy: 0.5855\n",
      "Epoch 30/200\n",
      "175341/175341 [==============================] - 165s 940us/step - loss: 1.3091 - accuracy: 0.5499 - val_loss: 1.2817 - val_accuracy: 0.5841\n",
      "Epoch 31/200\n",
      "175341/175341 [==============================] - 167s 952us/step - loss: 1.3086 - accuracy: 0.5507 - val_loss: 1.2629 - val_accuracy: 0.5896\n",
      "Epoch 32/200\n",
      "175341/175341 [==============================] - 168s 960us/step - loss: 1.3087 - accuracy: 0.5503 - val_loss: 1.2757 - val_accuracy: 0.5828\n",
      "Epoch 33/200\n",
      "175341/175341 [==============================] - 162s 923us/step - loss: 1.3087 - accuracy: 0.5502 - val_loss: 1.2773 - val_accuracy: 0.5890\n",
      "Epoch 34/200\n",
      "175341/175341 [==============================] - 160s 910us/step - loss: 1.3086 - accuracy: 0.5498 - val_loss: 1.2693 - val_accuracy: 0.5903\n",
      "Epoch 35/200\n",
      "175341/175341 [==============================] - 157s 896us/step - loss: 1.3083 - accuracy: 0.5512 - val_loss: 1.2766 - val_accuracy: 0.5862\n",
      "Epoch 36/200\n",
      "175341/175341 [==============================] - 157s 893us/step - loss: 1.3078 - accuracy: 0.5511 - val_loss: 1.2752 - val_accuracy: 0.5828\n",
      "Epoch 37/200\n",
      "175341/175341 [==============================] - 157s 894us/step - loss: 1.3082 - accuracy: 0.5502 - val_loss: 1.2820 - val_accuracy: 0.5862\n",
      "Epoch 38/200\n",
      "175341/175341 [==============================] - 156s 889us/step - loss: 1.3079 - accuracy: 0.5507 - val_loss: 1.2858 - val_accuracy: 0.5850\n",
      "Epoch 39/200\n",
      "175341/175341 [==============================] - 154s 880us/step - loss: 1.3077 - accuracy: 0.5501 - val_loss: 1.2649 - val_accuracy: 0.5909\n",
      "Epoch 40/200\n",
      "175341/175341 [==============================] - 154s 881us/step - loss: 1.3081 - accuracy: 0.5507 - val_loss: 1.2791 - val_accuracy: 0.5881\n",
      "Epoch 41/200\n",
      "175341/175341 [==============================] - 156s 888us/step - loss: 1.3073 - accuracy: 0.5511 - val_loss: 1.2586 - val_accuracy: 0.5948\n",
      "Epoch 42/200\n",
      "175341/175341 [==============================] - 156s 892us/step - loss: 1.3075 - accuracy: 0.5502 - val_loss: 1.2826 - val_accuracy: 0.5841\n",
      "Epoch 43/200\n",
      "175341/175341 [==============================] - 155s 883us/step - loss: 1.3076 - accuracy: 0.5505 - val_loss: 1.2888 - val_accuracy: 0.5868\n",
      "Epoch 44/200\n",
      "175341/175341 [==============================] - 157s 896us/step - loss: 1.3073 - accuracy: 0.5512 - val_loss: 1.2774 - val_accuracy: 0.5884\n",
      "Epoch 45/200\n",
      "175341/175341 [==============================] - 157s 896us/step - loss: 1.3071 - accuracy: 0.5508 - val_loss: 1.2672 - val_accuracy: 0.5875\n",
      "Epoch 46/200\n",
      "175341/175341 [==============================] - 158s 900us/step - loss: 1.3065 - accuracy: 0.5512 - val_loss: 1.2806 - val_accuracy: 0.5835\n",
      "Epoch 47/200\n",
      "175341/175341 [==============================] - 154s 880us/step - loss: 1.3068 - accuracy: 0.5510 - val_loss: 1.2580 - val_accuracy: 0.5925\n",
      "Epoch 48/200\n",
      "175341/175341 [==============================] - 152s 869us/step - loss: 1.3075 - accuracy: 0.5508 - val_loss: 1.2671 - val_accuracy: 0.5868\n",
      "Epoch 49/200\n",
      "175341/175341 [==============================] - 150s 857us/step - loss: 1.3070 - accuracy: 0.5513 - val_loss: 1.2646 - val_accuracy: 0.5878\n",
      "Epoch 50/200\n",
      "175341/175341 [==============================] - 151s 862us/step - loss: 1.3063 - accuracy: 0.5512 - val_loss: 1.2754 - val_accuracy: 0.5865\n",
      "Epoch 51/200\n",
      "175341/175341 [==============================] - 150s 855us/step - loss: 1.3061 - accuracy: 0.5510 - val_loss: 1.2780 - val_accuracy: 0.5833\n",
      "Epoch 52/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175341/175341 [==============================] - 143s 815us/step - loss: 1.3066 - accuracy: 0.5510 - val_loss: 1.2684 - val_accuracy: 0.5851\n",
      "Epoch 53/200\n",
      "175341/175341 [==============================] - 145s 825us/step - loss: 1.3066 - accuracy: 0.5509 - val_loss: 1.2737 - val_accuracy: 0.5829\n",
      "Epoch 54/200\n",
      "175341/175341 [==============================] - 144s 820us/step - loss: 1.3064 - accuracy: 0.5520 - val_loss: 1.2768 - val_accuracy: 0.5852\n",
      "Epoch 55/200\n",
      "175341/175341 [==============================] - 145s 825us/step - loss: 1.3061 - accuracy: 0.5512 - val_loss: 1.2815 - val_accuracy: 0.5836\n",
      "Epoch 56/200\n",
      "175341/175341 [==============================] - 145s 826us/step - loss: 1.3058 - accuracy: 0.5517 - val_loss: 1.2824 - val_accuracy: 0.5859\n",
      "Epoch 57/200\n",
      "175341/175341 [==============================] - 144s 821us/step - loss: 1.3060 - accuracy: 0.5515 - val_loss: 1.2664 - val_accuracy: 0.5902\n",
      "Epoch 58/200\n",
      "175341/175341 [==============================] - 145s 826us/step - loss: 1.3056 - accuracy: 0.5513 - val_loss: 1.2744 - val_accuracy: 0.5834\n",
      "Epoch 59/200\n",
      "175341/175341 [==============================] - 144s 822us/step - loss: 1.3051 - accuracy: 0.5514 - val_loss: 1.2845 - val_accuracy: 0.5832\n",
      "Epoch 60/200\n",
      "175341/175341 [==============================] - 144s 821us/step - loss: 1.3049 - accuracy: 0.5517 - val_loss: 1.2809 - val_accuracy: 0.5901\n",
      "Epoch 61/200\n",
      "175341/175341 [==============================] - 144s 821us/step - loss: 1.3048 - accuracy: 0.5514 - val_loss: 1.2898 - val_accuracy: 0.5803\n",
      "Epoch 62/200\n",
      "175341/175341 [==============================] - 146s 834us/step - loss: 1.3050 - accuracy: 0.5515 - val_loss: 1.2775 - val_accuracy: 0.5884\n",
      "Epoch 63/200\n",
      "175341/175341 [==============================] - 146s 832us/step - loss: 1.3055 - accuracy: 0.5502 - val_loss: 1.2822 - val_accuracy: 0.5855\n",
      "Epoch 64/200\n",
      "175341/175341 [==============================] - 118s 671us/step - loss: 1.3050 - accuracy: 0.5519 - val_loss: 1.2802 - val_accuracy: 0.5822\n",
      "Epoch 65/200\n",
      "175341/175341 [==============================] - 106s 603us/step - loss: 1.3046 - accuracy: 0.5508 - val_loss: 1.2744 - val_accuracy: 0.5917\n",
      "Epoch 66/200\n",
      "175341/175341 [==============================] - 105s 598us/step - loss: 1.3041 - accuracy: 0.5511 - val_loss: 1.2709 - val_accuracy: 0.5863\n",
      "Epoch 67/200\n",
      "175341/175341 [==============================] - 106s 605us/step - loss: 1.3039 - accuracy: 0.5512 - val_loss: 1.2953 - val_accuracy: 0.5703\n",
      "Epoch 68/200\n",
      "175341/175341 [==============================] - 106s 603us/step - loss: 1.3032 - accuracy: 0.5514 - val_loss: 1.2580 - val_accuracy: 0.5870\n",
      "Epoch 69/200\n",
      "175341/175341 [==============================] - 106s 605us/step - loss: 1.3032 - accuracy: 0.5518 - val_loss: 1.2881 - val_accuracy: 0.5868\n",
      "Epoch 70/200\n",
      "175341/175341 [==============================] - 106s 607us/step - loss: 1.3028 - accuracy: 0.5512 - val_loss: 1.2721 - val_accuracy: 0.5885\n",
      "Epoch 71/200\n",
      "175341/175341 [==============================] - 106s 602us/step - loss: 1.3035 - accuracy: 0.5509 - val_loss: 1.2814 - val_accuracy: 0.5880\n",
      "Epoch 72/200\n",
      "175341/175341 [==============================] - 90s 511us/step - loss: 1.3016 - accuracy: 0.5517 - val_loss: 1.2658 - val_accuracy: 0.5879\n",
      "Epoch 73/200\n",
      "175341/175341 [==============================] - 83s 473us/step - loss: 1.3027 - accuracy: 0.5507 - val_loss: 1.2912 - val_accuracy: 0.5765\n",
      "Epoch 74/200\n",
      "175341/175341 [==============================] - 83s 472us/step - loss: 1.3019 - accuracy: 0.5506 - val_loss: 1.2597 - val_accuracy: 0.5873\n",
      "Epoch 75/200\n",
      "175341/175341 [==============================] - 83s 474us/step - loss: 1.3012 - accuracy: 0.5505 - val_loss: 1.2708 - val_accuracy: 0.5855\n",
      "Epoch 76/200\n",
      "175341/175341 [==============================] - 83s 473us/step - loss: 1.3007 - accuracy: 0.5513 - val_loss: 1.2838 - val_accuracy: 0.5795\n",
      "Epoch 77/200\n",
      "175341/175341 [==============================] - 83s 476us/step - loss: 1.3009 - accuracy: 0.5501 - val_loss: 1.2656 - val_accuracy: 0.5867\n",
      "Epoch 78/200\n",
      "175341/175341 [==============================] - 83s 474us/step - loss: 1.3003 - accuracy: 0.5508 - val_loss: 1.2737 - val_accuracy: 0.5845\n",
      "Epoch 79/200\n",
      "175341/175341 [==============================] - 83s 476us/step - loss: 1.2985 - accuracy: 0.5511 - val_loss: 1.2801 - val_accuracy: 0.5787\n",
      "Epoch 80/200\n",
      "175341/175341 [==============================] - 83s 476us/step - loss: 1.2988 - accuracy: 0.5514 - val_loss: 1.2869 - val_accuracy: 0.5780\n",
      "Epoch 81/200\n",
      "175341/175341 [==============================] - 83s 476us/step - loss: 1.2986 - accuracy: 0.5509 - val_loss: 1.2671 - val_accuracy: 0.5862\n",
      "Epoch 82/200\n",
      "175341/175341 [==============================] - 83s 476us/step - loss: 1.2978 - accuracy: 0.5510 - val_loss: 1.2747 - val_accuracy: 0.5855\n",
      "Epoch 83/200\n",
      "175341/175341 [==============================] - 83s 476us/step - loss: 1.2972 - accuracy: 0.5508 - val_loss: 1.2700 - val_accuracy: 0.5826\n",
      "Epoch 84/200\n",
      "175341/175341 [==============================] - 84s 478us/step - loss: 1.2975 - accuracy: 0.5506 - val_loss: 1.2809 - val_accuracy: 0.5833\n",
      "Epoch 85/200\n",
      "175341/175341 [==============================] - 83s 476us/step - loss: 1.2970 - accuracy: 0.5505 - val_loss: 1.2717 - val_accuracy: 0.5733\n",
      "Epoch 86/200\n",
      "175341/175341 [==============================] - 83s 476us/step - loss: 1.2958 - accuracy: 0.5510 - val_loss: 1.2585 - val_accuracy: 0.5880\n",
      "Epoch 87/200\n",
      "175341/175341 [==============================] - 83s 476us/step - loss: 1.2963 - accuracy: 0.5501 - val_loss: 1.2851 - val_accuracy: 0.5583\n",
      "Epoch 88/200\n",
      "175341/175341 [==============================] - 84s 478us/step - loss: 1.2958 - accuracy: 0.5502 - val_loss: 1.2712 - val_accuracy: 0.5812\n",
      "Epoch 89/200\n",
      "175341/175341 [==============================] - 83s 474us/step - loss: 1.2952 - accuracy: 0.5510 - val_loss: 1.2878 - val_accuracy: 0.5707\n",
      "Epoch 90/200\n",
      "175341/175341 [==============================] - 84s 477us/step - loss: 1.2949 - accuracy: 0.5511 - val_loss: 1.2707 - val_accuracy: 0.5841\n",
      "Epoch 91/200\n",
      "175341/175341 [==============================] - 83s 476us/step - loss: 1.2937 - accuracy: 0.5509 - val_loss: 1.2765 - val_accuracy: 0.5805\n",
      "Epoch 92/200\n",
      "175341/175341 [==============================] - 83s 474us/step - loss: 1.2939 - accuracy: 0.5504 - val_loss: 1.2804 - val_accuracy: 0.5673\n",
      "Epoch 93/200\n",
      "175341/175341 [==============================] - 83s 473us/step - loss: 1.2934 - accuracy: 0.5506 - val_loss: 1.2621 - val_accuracy: 0.5907\n",
      "Epoch 94/200\n",
      "175341/175341 [==============================] - 84s 476us/step - loss: 1.2938 - accuracy: 0.5509 - val_loss: 1.2790 - val_accuracy: 0.5883\n",
      "Epoch 95/200\n",
      "175341/175341 [==============================] - 83s 474us/step - loss: 1.2939 - accuracy: 0.5505 - val_loss: 1.2662 - val_accuracy: 0.5828\n",
      "Epoch 96/200\n",
      "175341/175341 [==============================] - 83s 473us/step - loss: 1.2931 - accuracy: 0.5513 - val_loss: 1.2730 - val_accuracy: 0.5688\n",
      "Epoch 97/200\n",
      "175341/175341 [==============================] - 83s 475us/step - loss: 1.2934 - accuracy: 0.5517 - val_loss: 1.2489 - val_accuracy: 0.5895\n",
      "Epoch 98/200\n",
      "175341/175341 [==============================] - 84s 477us/step - loss: 1.2931 - accuracy: 0.5508 - val_loss: 1.2724 - val_accuracy: 0.5733\n",
      "Epoch 99/200\n",
      "175341/175341 [==============================] - 83s 476us/step - loss: 1.2917 - accuracy: 0.5506 - val_loss: 1.2738 - val_accuracy: 0.5804\n",
      "Epoch 100/200\n",
      "175341/175341 [==============================] - 83s 475us/step - loss: 1.2921 - accuracy: 0.5508 - val_loss: 1.2643 - val_accuracy: 0.5825\n",
      "Epoch 101/200\n",
      "175341/175341 [==============================] - 83s 476us/step - loss: 1.2912 - accuracy: 0.5510 - val_loss: 1.2473 - val_accuracy: 0.5913\n",
      "Epoch 102/200\n",
      "175341/175341 [==============================] - 84s 478us/step - loss: 1.2909 - accuracy: 0.5510 - val_loss: 1.2602 - val_accuracy: 0.5923\n",
      "Epoch 103/200\n",
      "175341/175341 [==============================] - 84s 478us/step - loss: 1.2927 - accuracy: 0.5513 - val_loss: 1.2960 - val_accuracy: 0.5473\n",
      "Epoch 104/200\n",
      "175341/175341 [==============================] - 84s 477us/step - loss: 1.2917 - accuracy: 0.5502 - val_loss: 1.2561 - val_accuracy: 0.5896\n",
      "Epoch 105/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175341/175341 [==============================] - 82s 468us/step - loss: 1.2918 - accuracy: 0.5512 - val_loss: 1.2766 - val_accuracy: 0.5852\n",
      "Epoch 106/200\n",
      "175341/175341 [==============================] - 82s 470us/step - loss: 1.2916 - accuracy: 0.5503 - val_loss: 1.2617 - val_accuracy: 0.5856\n",
      "Epoch 107/200\n",
      "175341/175341 [==============================] - 82s 469us/step - loss: 1.2921 - accuracy: 0.5499 - val_loss: 1.2817 - val_accuracy: 0.5789\n",
      "Epoch 108/200\n",
      "175341/175341 [==============================] - 82s 468us/step - loss: 1.2915 - accuracy: 0.5512 - val_loss: 1.2549 - val_accuracy: 0.5884\n",
      "Epoch 109/200\n",
      "175341/175341 [==============================] - 82s 470us/step - loss: 1.2921 - accuracy: 0.5501 - val_loss: 1.2894 - val_accuracy: 0.5541\n",
      "Epoch 110/200\n",
      "175341/175341 [==============================] - 83s 471us/step - loss: 1.2920 - accuracy: 0.5498 - val_loss: 1.2698 - val_accuracy: 0.5645\n",
      "Epoch 111/200\n",
      "175341/175341 [==============================] - 82s 470us/step - loss: 1.2921 - accuracy: 0.5506 - val_loss: 1.2671 - val_accuracy: 0.5842\n",
      "Epoch 112/200\n",
      "175341/175341 [==============================] - 83s 471us/step - loss: 1.2924 - accuracy: 0.5502 - val_loss: 1.2610 - val_accuracy: 0.5854\n",
      "Epoch 113/200\n",
      "175341/175341 [==============================] - 83s 471us/step - loss: 1.2917 - accuracy: 0.5499 - val_loss: 1.2728 - val_accuracy: 0.5846\n",
      "Epoch 114/200\n",
      "175341/175341 [==============================] - 83s 471us/step - loss: 1.2915 - accuracy: 0.5496 - val_loss: 1.2617 - val_accuracy: 0.5832\n",
      "Epoch 115/200\n",
      "175341/175341 [==============================] - 82s 470us/step - loss: 1.2902 - accuracy: 0.5497 - val_loss: 1.2915 - val_accuracy: 0.5807\n",
      "Epoch 116/200\n",
      "175341/175341 [==============================] - 82s 470us/step - loss: 1.2903 - accuracy: 0.5497 - val_loss: 1.2691 - val_accuracy: 0.5856\n",
      "Epoch 117/200\n",
      "175341/175341 [==============================] - 83s 472us/step - loss: 1.2906 - accuracy: 0.5492 - val_loss: 1.2617 - val_accuracy: 0.5789\n",
      "Epoch 118/200\n",
      "175341/175341 [==============================] - 83s 472us/step - loss: 1.2908 - accuracy: 0.5501 - val_loss: 1.2795 - val_accuracy: 0.5561\n",
      "Epoch 119/200\n",
      "175341/175341 [==============================] - 83s 472us/step - loss: 1.2906 - accuracy: 0.5501 - val_loss: 1.2793 - val_accuracy: 0.5708\n",
      "Epoch 120/200\n",
      "175341/175341 [==============================] - 83s 474us/step - loss: 1.2912 - accuracy: 0.5502 - val_loss: 1.2794 - val_accuracy: 0.5665\n",
      "Epoch 121/200\n",
      "175341/175341 [==============================] - 83s 473us/step - loss: 1.2906 - accuracy: 0.5498 - val_loss: 1.2478 - val_accuracy: 0.5929\n",
      "Epoch 122/200\n",
      "175341/175341 [==============================] - 83s 471us/step - loss: 1.2903 - accuracy: 0.5503 - val_loss: 1.2597 - val_accuracy: 0.5842\n",
      "Epoch 123/200\n",
      "175341/175341 [==============================] - 83s 471us/step - loss: 1.2905 - accuracy: 0.5498 - val_loss: 1.2596 - val_accuracy: 0.5821\n",
      "Epoch 124/200\n",
      "175341/175341 [==============================] - 83s 474us/step - loss: 1.2906 - accuracy: 0.5495 - val_loss: 1.2612 - val_accuracy: 0.5864\n",
      "Epoch 125/200\n",
      "175341/175341 [==============================] - 83s 473us/step - loss: 1.2895 - accuracy: 0.5509 - val_loss: 1.2686 - val_accuracy: 0.5753\n",
      "Epoch 126/200\n",
      "175341/175341 [==============================] - 83s 473us/step - loss: 1.2915 - accuracy: 0.5498 - val_loss: 1.2609 - val_accuracy: 0.5858\n",
      "Epoch 127/200\n",
      "175341/175341 [==============================] - 83s 475us/step - loss: 1.2898 - accuracy: 0.5501 - val_loss: 1.2847 - val_accuracy: 0.5593\n",
      "Epoch 128/200\n",
      "175341/175341 [==============================] - 83s 473us/step - loss: 1.2909 - accuracy: 0.5500 - val_loss: 1.2722 - val_accuracy: 0.5630\n",
      "Epoch 129/200\n",
      "175341/175341 [==============================] - 83s 474us/step - loss: 1.2902 - accuracy: 0.5497 - val_loss: 1.2680 - val_accuracy: 0.5784\n",
      "Epoch 130/200\n",
      "175341/175341 [==============================] - 83s 474us/step - loss: 1.2905 - accuracy: 0.5497 - val_loss: 1.2668 - val_accuracy: 0.5762\n",
      "Epoch 131/200\n",
      "175341/175341 [==============================] - 84s 477us/step - loss: 1.2897 - accuracy: 0.5502 - val_loss: 1.2674 - val_accuracy: 0.5829\n",
      "Epoch 132/200\n",
      "175341/175341 [==============================] - 84s 477us/step - loss: 1.2902 - accuracy: 0.5504 - val_loss: 1.2806 - val_accuracy: 0.5818\n",
      "Epoch 133/200\n",
      "175341/175341 [==============================] - 83s 476us/step - loss: 1.2895 - accuracy: 0.5503 - val_loss: 1.2772 - val_accuracy: 0.5838\n",
      "Epoch 134/200\n",
      "175341/175341 [==============================] - 83s 476us/step - loss: 1.2884 - accuracy: 0.5504 - val_loss: 1.3016 - val_accuracy: 0.5438\n",
      "Epoch 135/200\n",
      "175341/175341 [==============================] - 84s 477us/step - loss: 1.2895 - accuracy: 0.5503 - val_loss: 1.2687 - val_accuracy: 0.5715\n",
      "Epoch 136/200\n",
      "175341/175341 [==============================] - 83s 475us/step - loss: 1.2890 - accuracy: 0.5507 - val_loss: 1.2630 - val_accuracy: 0.5861\n",
      "Epoch 137/200\n",
      "175341/175341 [==============================] - 84s 477us/step - loss: 1.2908 - accuracy: 0.5504 - val_loss: 1.2736 - val_accuracy: 0.5748\n",
      "Epoch 138/200\n",
      "175341/175341 [==============================] - 84s 477us/step - loss: 1.2887 - accuracy: 0.5505 - val_loss: 1.2977 - val_accuracy: 0.5480\n",
      "Epoch 139/200\n",
      "175341/175341 [==============================] - 83s 476us/step - loss: 1.2894 - accuracy: 0.5498 - val_loss: 1.2649 - val_accuracy: 0.5834\n",
      "Epoch 140/200\n",
      "175341/175341 [==============================] - 84s 477us/step - loss: 1.2891 - accuracy: 0.5505 - val_loss: 1.2645 - val_accuracy: 0.5844\n",
      "Epoch 141/200\n",
      "175341/175341 [==============================] - 83s 476us/step - loss: 1.2904 - accuracy: 0.5506 - val_loss: 1.2871 - val_accuracy: 0.5846\n",
      "Epoch 142/200\n",
      "175341/175341 [==============================] - 83s 475us/step - loss: 1.2902 - accuracy: 0.5509 - val_loss: 1.2620 - val_accuracy: 0.5848\n",
      "Epoch 143/200\n",
      "175341/175341 [==============================] - 84s 476us/step - loss: 1.2894 - accuracy: 0.5503 - val_loss: 1.2932 - val_accuracy: 0.5670\n",
      "Epoch 144/200\n",
      "175341/175341 [==============================] - 83s 474us/step - loss: 1.2898 - accuracy: 0.5502 - val_loss: 1.2551 - val_accuracy: 0.5916\n",
      "Epoch 145/200\n",
      "175341/175341 [==============================] - 83s 473us/step - loss: 1.2889 - accuracy: 0.5507 - val_loss: 1.2757 - val_accuracy: 0.5829\n",
      "Epoch 146/200\n",
      "175341/175341 [==============================] - 83s 475us/step - loss: 1.2886 - accuracy: 0.5500 - val_loss: 1.2807 - val_accuracy: 0.5821\n",
      "Epoch 147/200\n",
      "175341/175341 [==============================] - 83s 476us/step - loss: 1.2897 - accuracy: 0.5502 - val_loss: 1.2680 - val_accuracy: 0.5851\n",
      "Epoch 148/200\n",
      "175341/175341 [==============================] - 83s 475us/step - loss: 1.2883 - accuracy: 0.5502 - val_loss: 1.2723 - val_accuracy: 0.5851\n",
      "Epoch 149/200\n",
      "175341/175341 [==============================] - 84s 477us/step - loss: 1.2891 - accuracy: 0.5498 - val_loss: 1.2724 - val_accuracy: 0.5688\n",
      "Epoch 150/200\n",
      "175341/175341 [==============================] - 84s 477us/step - loss: 1.2881 - accuracy: 0.5509 - val_loss: 1.2809 - val_accuracy: 0.5624\n",
      "Epoch 151/200\n",
      "175341/175341 [==============================] - 83s 474us/step - loss: 1.2887 - accuracy: 0.5502 - val_loss: 1.2797 - val_accuracy: 0.5720\n",
      "Epoch 152/200\n",
      "175341/175341 [==============================] - 84s 477us/step - loss: 1.2893 - accuracy: 0.5499 - val_loss: 1.2512 - val_accuracy: 0.5865\n",
      "Epoch 153/200\n",
      "175341/175341 [==============================] - 84s 479us/step - loss: 1.2895 - accuracy: 0.5510 - val_loss: 1.2623 - val_accuracy: 0.5853\n",
      "Epoch 154/200\n",
      "175341/175341 [==============================] - 84s 477us/step - loss: 1.2902 - accuracy: 0.5501 - val_loss: 1.2850 - val_accuracy: 0.5833\n",
      "Epoch 155/200\n",
      "175341/175341 [==============================] - 83s 475us/step - loss: 1.2886 - accuracy: 0.5503 - val_loss: 1.2691 - val_accuracy: 0.5833\n",
      "Epoch 156/200\n",
      "175341/175341 [==============================] - 84s 478us/step - loss: 1.2886 - accuracy: 0.5500 - val_loss: 1.2621 - val_accuracy: 0.5869\n",
      "Epoch 157/200\n",
      "175341/175341 [==============================] - 84s 477us/step - loss: 1.2897 - accuracy: 0.5499 - val_loss: 1.2912 - val_accuracy: 0.5789\n",
      "Epoch 158/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175341/175341 [==============================] - 82s 469us/step - loss: 1.2881 - accuracy: 0.5503 - val_loss: 1.2707 - val_accuracy: 0.5833\n",
      "Epoch 159/200\n",
      "175341/175341 [==============================] - 82s 470us/step - loss: 1.2897 - accuracy: 0.5504 - val_loss: 1.2829 - val_accuracy: 0.5708\n",
      "Epoch 160/200\n",
      "175341/175341 [==============================] - 82s 469us/step - loss: 1.2902 - accuracy: 0.5496 - val_loss: 1.3319 - val_accuracy: 0.5457\n",
      "Epoch 161/200\n",
      "175341/175341 [==============================] - 82s 469us/step - loss: 1.2921 - accuracy: 0.5491 - val_loss: 1.2923 - val_accuracy: 0.5414\n",
      "Epoch 162/200\n",
      "175341/175341 [==============================] - 82s 470us/step - loss: 1.2902 - accuracy: 0.5504 - val_loss: 1.2656 - val_accuracy: 0.5845\n",
      "Epoch 163/200\n",
      "175341/175341 [==============================] - 83s 471us/step - loss: 1.2889 - accuracy: 0.5502 - val_loss: 1.2750 - val_accuracy: 0.5866\n",
      "Epoch 164/200\n",
      "175341/175341 [==============================] - 82s 469us/step - loss: 1.2885 - accuracy: 0.5502 - val_loss: 1.2715 - val_accuracy: 0.5857\n",
      "Epoch 165/200\n",
      "175341/175341 [==============================] - 83s 472us/step - loss: 1.2895 - accuracy: 0.5501 - val_loss: 1.2788 - val_accuracy: 0.5835\n",
      "Epoch 166/200\n",
      "175341/175341 [==============================] - 82s 469us/step - loss: 1.2889 - accuracy: 0.5499 - val_loss: 1.2654 - val_accuracy: 0.5871\n",
      "Epoch 167/200\n",
      "175341/175341 [==============================] - 83s 471us/step - loss: 1.2876 - accuracy: 0.5506 - val_loss: 1.2700 - val_accuracy: 0.5866\n",
      "Epoch 168/200\n",
      "175341/175341 [==============================] - 83s 473us/step - loss: 1.2898 - accuracy: 0.5497 - val_loss: 1.2791 - val_accuracy: 0.5633\n",
      "Epoch 169/200\n",
      "175341/175341 [==============================] - 83s 472us/step - loss: 1.2871 - accuracy: 0.5507 - val_loss: 1.2555 - val_accuracy: 0.5877\n",
      "Epoch 170/200\n",
      "175341/175341 [==============================] - 83s 475us/step - loss: 1.2881 - accuracy: 0.5503 - val_loss: 1.2882 - val_accuracy: 0.5733\n",
      "Epoch 171/200\n",
      "175341/175341 [==============================] - 83s 473us/step - loss: 1.2883 - accuracy: 0.5499 - val_loss: 1.2707 - val_accuracy: 0.5847\n",
      "Epoch 172/200\n",
      "175341/175341 [==============================] - 83s 475us/step - loss: 1.2883 - accuracy: 0.5505 - val_loss: 1.2715 - val_accuracy: 0.5839\n",
      "Epoch 173/200\n",
      "175341/175341 [==============================] - 83s 472us/step - loss: 1.2878 - accuracy: 0.5489 - val_loss: 1.2756 - val_accuracy: 0.5619\n",
      "Epoch 174/200\n",
      "175341/175341 [==============================] - 83s 473us/step - loss: 1.2880 - accuracy: 0.5498 - val_loss: 1.2811 - val_accuracy: 0.5855\n",
      "Epoch 175/200\n",
      "175341/175341 [==============================] - 83s 473us/step - loss: 1.2891 - accuracy: 0.5507 - val_loss: 1.2786 - val_accuracy: 0.5589\n",
      "Epoch 176/200\n",
      "175341/175341 [==============================] - 83s 472us/step - loss: 1.2868 - accuracy: 0.5501 - val_loss: 1.2543 - val_accuracy: 0.6006\n",
      "Epoch 177/200\n",
      "175341/175341 [==============================] - 83s 474us/step - loss: 1.2874 - accuracy: 0.5498 - val_loss: 1.2738 - val_accuracy: 0.5697\n",
      "Epoch 178/200\n",
      "175341/175341 [==============================] - 84s 476us/step - loss: 1.2875 - accuracy: 0.5491 - val_loss: 1.2559 - val_accuracy: 0.5869\n",
      "Epoch 179/200\n",
      "175341/175341 [==============================] - 84s 477us/step - loss: 1.2883 - accuracy: 0.5491 - val_loss: 1.2889 - val_accuracy: 0.5664\n",
      "Epoch 180/200\n",
      "175341/175341 [==============================] - 83s 473us/step - loss: 1.2876 - accuracy: 0.5502 - val_loss: 1.2875 - val_accuracy: 0.5501\n",
      "Epoch 181/200\n",
      "175341/175341 [==============================] - 83s 473us/step - loss: 1.2884 - accuracy: 0.5497 - val_loss: 1.2716 - val_accuracy: 0.5866\n",
      "Epoch 182/200\n",
      "175341/175341 [==============================] - 83s 475us/step - loss: 1.2885 - accuracy: 0.5500 - val_loss: 1.2974 - val_accuracy: 0.5169\n",
      "Epoch 183/200\n",
      "175341/175341 [==============================] - 83s 475us/step - loss: 1.2879 - accuracy: 0.5492 - val_loss: 1.2782 - val_accuracy: 0.5692\n",
      "Epoch 184/200\n",
      "175341/175341 [==============================] - 83s 475us/step - loss: 1.2862 - accuracy: 0.5493 - val_loss: 1.2718 - val_accuracy: 0.5802\n",
      "Epoch 185/200\n",
      "175341/175341 [==============================] - 47s 265us/step - loss: 1.2879 - accuracy: 0.5501 - val_loss: 1.2654 - val_accuracy: 0.5777\n",
      "Epoch 186/200\n",
      "175341/175341 [==============================] - 46s 265us/step - loss: 1.2877 - accuracy: 0.5501 - val_loss: 1.2445 - val_accuracy: 0.5908\n",
      "Epoch 187/200\n",
      "175341/175341 [==============================] - 47s 266us/step - loss: 1.2866 - accuracy: 0.5503 - val_loss: 1.2649 - val_accuracy: 0.5857\n",
      "Epoch 188/200\n",
      "175341/175341 [==============================] - 46s 265us/step - loss: 1.2860 - accuracy: 0.5502 - val_loss: 1.2573 - val_accuracy: 0.5863\n",
      "Epoch 189/200\n",
      "175341/175341 [==============================] - 47s 266us/step - loss: 1.2849 - accuracy: 0.5496 - val_loss: 1.2619 - val_accuracy: 0.5836\n",
      "Epoch 190/200\n",
      "175341/175341 [==============================] - 47s 266us/step - loss: 1.2819 - accuracy: 0.5501 - val_loss: 1.2666 - val_accuracy: 0.5830\n",
      "Epoch 191/200\n",
      "175341/175341 [==============================] - 47s 266us/step - loss: 1.2824 - accuracy: 0.5503 - val_loss: 1.2524 - val_accuracy: 0.5845\n",
      "Epoch 192/200\n",
      "175341/175341 [==============================] - 47s 267us/step - loss: 1.2753 - accuracy: 0.5498 - val_loss: 1.2668 - val_accuracy: 0.5804\n",
      "Epoch 193/200\n",
      "175341/175341 [==============================] - 47s 266us/step - loss: 1.2704 - accuracy: 0.5502 - val_loss: 1.2371 - val_accuracy: 0.5885\n",
      "Epoch 194/200\n",
      "175341/175341 [==============================] - 47s 266us/step - loss: 1.2659 - accuracy: 0.5503 - val_loss: 1.2641 - val_accuracy: 0.5445\n",
      "Epoch 195/200\n",
      "175341/175341 [==============================] - 47s 267us/step - loss: 1.2595 - accuracy: 0.5498 - val_loss: 1.2508 - val_accuracy: 0.5799\n",
      "Epoch 196/200\n",
      "175341/175341 [==============================] - 47s 269us/step - loss: 1.2536 - accuracy: 0.5510 - val_loss: 1.2401 - val_accuracy: 0.5704\n",
      "Epoch 197/200\n",
      "175341/175341 [==============================] - 47s 267us/step - loss: 1.2469 - accuracy: 0.5536 - val_loss: 1.2203 - val_accuracy: 0.5846\n",
      "Epoch 198/200\n",
      "175341/175341 [==============================] - 47s 266us/step - loss: 1.2414 - accuracy: 0.5549 - val_loss: 1.2311 - val_accuracy: 0.5640\n",
      "Epoch 199/200\n",
      "175341/175341 [==============================] - 47s 267us/step - loss: 1.2359 - accuracy: 0.5635 - val_loss: 1.1996 - val_accuracy: 0.5865\n",
      "Epoch 200/200\n",
      "175341/175341 [==============================] - 47s 267us/step - loss: 1.2344 - accuracy: 0.5707 - val_loss: 1.2016 - val_accuracy: 0.5863\n"
     ]
    }
   ],
   "source": [
    "history = cnn.fit(XDelta1, Y, epochs=200, validation_data=(xdelta1, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predictions = cnn.predict([xdelta1])\n",
    "\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "#pred = np.zeros((82332,1), dtype=int)\n",
    "for i in range(0, predictions.shape[0]):\n",
    "    pred = np.where(predictions[i] == np.amax(predictions[i]))\n",
    "    for j in range(0,10):\n",
    "        predictions[i,j] = 0\n",
    "    predictions[i,pred]=1\n",
    "    \n",
    "print(predictions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5863455278627994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       677\n",
      "           1       1.00      0.00      0.01       583\n",
      "           2       0.95      0.00      0.01      4089\n",
      "           3       0.24      0.35      0.29     11132\n",
      "           4       0.34      0.01      0.01      6062\n",
      "           5       0.58      0.96      0.73     18871\n",
      "           6       0.75      0.70      0.73     37000\n",
      "           7       0.00      0.00      0.00      3496\n",
      "           8       0.00      0.00      0.00       378\n",
      "           9       0.00      0.00      0.00        44\n",
      "\n",
      "   micro avg       0.59      0.59      0.59     82332\n",
      "   macro avg       0.39      0.20      0.18     82332\n",
      "weighted avg       0.58      0.59      0.53     82332\n",
      " samples avg       0.59      0.59      0.59     82332\n",
      "\n",
      "[[    0     0     0    45     0   602    30     0     0     0]\n",
      " [    0     2     0    64     3   485    29     0     0     0]\n",
      " [    0     0    19   819    29  2583   639     0     0     0]\n",
      " [    0     0     0  3943    15  2784  4390     0     0     0]\n",
      " [    0     0     0  2167    42  1670  2183     0     0     0]\n",
      " [    0     0     0   369    17 18192   293     0     0     0]\n",
      " [    0     0     0  7739    16  3168 26077     0     0     0]\n",
      " [    0     0     1   885     2  1564  1044     0     0     0]\n",
      " [    0     0     0   115     0   152   111     0     0     0]\n",
      " [    0     0     0    15     0     2    27     0     0     0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Accuracy: \",accuracy_score(y,predictions))\n",
    "from sklearn.metrics import multilabel_confusion_matrix,classification_report,confusion_matrix\n",
    "print(\"Classification Report: \\n\",classification_report(y,predictions))\n",
    "conmat = np.array(confusion_matrix(y.argmax(axis=1), predictions.argmax(axis=1)))\n",
    "confusion = pd.DataFrame(conmat, index=['0','1','2','3','4','5','6','7','8','9'],\n",
    "                        columns=[['0','1','2','3','4','5','6','7','8','9']])\n",
    "print (conmat)\n",
    "conmat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
