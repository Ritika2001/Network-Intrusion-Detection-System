{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN, Bidirectional\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = pd.read_csv('/home/ritika/NetworkIntrusionDetection/UNSW-NB15 - CSV Files/a part of training and testing set/UNSW_NB15_training-set.csv')\n",
    "testdata = pd.read_csv('/home/ritika/NetworkIntrusionDetection/UNSW-NB15 - CSV Files/a part of training and testing set/UNSW_NB15_testing-set.csv')\n",
    "\n",
    "#traindata=traindata.drop('id',axis=1)\n",
    "#testdata=testdata.drop('id',axis=1)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb_make = LabelEncoder()\n",
    "\n",
    "traindata['proto'] = lb_make.fit_transform(traindata['proto'])\n",
    "traindata['service'] = lb_make.fit_transform(traindata['service'])\n",
    "traindata['state'] = lb_make.fit_transform(traindata['state'])\n",
    "traindata['attack_cat'] = lb_make.fit_transform(traindata['attack_cat'])\n",
    "\n",
    "testdata['proto'] = lb_make.fit_transform(testdata['proto'])\n",
    "testdata['service'] = lb_make.fit_transform(testdata['service'])\n",
    "testdata['state'] = lb_make.fit_transform(testdata['state'])\n",
    "testdata['attack_cat'] = lb_make.fit_transform(testdata['attack_cat'])\n",
    "\n",
    "X = traindata.iloc[:,1:42]\n",
    "Y = traindata.iloc[:,-2]\n",
    "x = testdata.iloc[:,1:42]\n",
    "y = testdata.iloc[:,-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arr = X.values\n",
    "arr = x.values\n",
    "\n",
    "import librosa\n",
    "Arr_Delta = librosa.feature.delta(Arr)\n",
    "arr_delta = librosa.feature.delta(arr)\n",
    "\n",
    "Delta = librosa.feature.delta(Arr_Delta)\n",
    "delta = librosa.feature.delta(arr_delta)\n",
    "\n",
    "Delta1 = pd.DataFrame(Arr_Delta)\n",
    "delta1 = pd.DataFrame(arr_delta)\n",
    "Delta2 = pd.DataFrame(Delta)\n",
    "delta2 = pd.DataFrame(delta)\n",
    "\n",
    "XDelta1 = pd.concat([X, Delta1, Delta2], axis=1)\n",
    "xdelta1 = pd.concat([x, delta1, delta2], axis=1)\n",
    "\n",
    "scaler = Normalizer().fit(XDelta1)\n",
    "XDelta1 = scaler.transform(XDelta1)\n",
    "scaler = Normalizer().fit(xdelta1)\n",
    "xdelta1 = scaler.transform(xdelta1)\n",
    "\n",
    "Y = np.array(Y)\n",
    "y = np.array(y)\n",
    "\n",
    "XDelta1 = np.reshape(XDelta1, (XDelta1.shape[0],XDelta1.shape[1],1))\n",
    "xdelta1 = np.reshape(xdelta1, (xdelta1.shape[0],xdelta1.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = to_categorical(Y)\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", input_shape=(123, 1), padding=\"same\")`\n",
      "  \n",
      "/home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 123, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3904)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               499840    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 501,386\n",
      "Trainable params: 501,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Convolution1D(64, 3, border_mode=\"same\",activation=\"relu\",input_shape=(123, 1)))\n",
    "cnn.add(MaxPooling1D(pool_length=(2)))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(128, activation=\"relu\"))\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(10, activation=\"softmax\"))\n",
    "cnn.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "cnn.summary()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 175341 samples, validate on 82332 samples\n",
      "Epoch 1/200\n",
      "175341/175341 [==============================] - 181s 1ms/step - loss: 1.3918 - accuracy: 0.5302 - val_loss: 1.2832 - val_accuracy: 0.6179\n",
      "Epoch 2/200\n",
      "175341/175341 [==============================] - 188s 1ms/step - loss: 1.3455 - accuracy: 0.5353 - val_loss: 1.2780 - val_accuracy: 0.6185\n",
      "Epoch 3/200\n",
      "175341/175341 [==============================] - 185s 1ms/step - loss: 1.3357 - accuracy: 0.5404 - val_loss: 1.2825 - val_accuracy: 0.5978\n",
      "Epoch 4/200\n",
      "175341/175341 [==============================] - 203s 1ms/step - loss: 1.3298 - accuracy: 0.5430 - val_loss: 1.2731 - val_accuracy: 0.6005\n",
      "Epoch 5/200\n",
      "175341/175341 [==============================] - 215s 1ms/step - loss: 1.3263 - accuracy: 0.5431 - val_loss: 1.2823 - val_accuracy: 0.5919\n",
      "Epoch 6/200\n",
      "175341/175341 [==============================] - 189s 1ms/step - loss: 1.3242 - accuracy: 0.5452 - val_loss: 1.2729 - val_accuracy: 0.5977\n",
      "Epoch 7/200\n",
      "175341/175341 [==============================] - 183s 1ms/step - loss: 1.3222 - accuracy: 0.5452 - val_loss: 1.2735 - val_accuracy: 0.5882\n",
      "Epoch 8/200\n",
      "175341/175341 [==============================] - 182s 1ms/step - loss: 1.3203 - accuracy: 0.5465 - val_loss: 1.2611 - val_accuracy: 0.6031\n",
      "Epoch 9/200\n",
      "175341/175341 [==============================] - 183s 1ms/step - loss: 1.3190 - accuracy: 0.5469 - val_loss: 1.2741 - val_accuracy: 0.5876\n",
      "Epoch 10/200\n",
      "175341/175341 [==============================] - 186s 1ms/step - loss: 1.3179 - accuracy: 0.5474 - val_loss: 1.2663 - val_accuracy: 0.5887\n",
      "Epoch 11/200\n",
      "175341/175341 [==============================] - 180s 1ms/step - loss: 1.3186 - accuracy: 0.5469 - val_loss: 1.2674 - val_accuracy: 0.5939\n",
      "Epoch 12/200\n",
      "175341/175341 [==============================] - 184s 1ms/step - loss: 1.3169 - accuracy: 0.5478 - val_loss: 1.2665 - val_accuracy: 0.5918\n",
      "Epoch 13/200\n",
      "175341/175341 [==============================] - 185s 1ms/step - loss: 1.3161 - accuracy: 0.5479 - val_loss: 1.2722 - val_accuracy: 0.5928\n",
      "Epoch 14/200\n",
      "175341/175341 [==============================] - 195s 1ms/step - loss: 1.3163 - accuracy: 0.5475 - val_loss: 1.2687 - val_accuracy: 0.5911\n",
      "Epoch 15/200\n",
      "175341/175341 [==============================] - 205s 1ms/step - loss: 1.3152 - accuracy: 0.5480 - val_loss: 1.2609 - val_accuracy: 0.5967\n",
      "Epoch 16/200\n",
      "175341/175341 [==============================] - 198s 1ms/step - loss: 1.3148 - accuracy: 0.5480 - val_loss: 1.2841 - val_accuracy: 0.5844\n",
      "Epoch 17/200\n",
      "175341/175341 [==============================] - 202s 1ms/step - loss: 1.3148 - accuracy: 0.5481 - val_loss: 1.2833 - val_accuracy: 0.5859\n",
      "Epoch 18/200\n",
      "175341/175341 [==============================] - 212s 1ms/step - loss: 1.3142 - accuracy: 0.5482 - val_loss: 1.2746 - val_accuracy: 0.5930\n",
      "Epoch 19/200\n",
      "175341/175341 [==============================] - 215s 1ms/step - loss: 1.3142 - accuracy: 0.5485 - val_loss: 1.2916 - val_accuracy: 0.5842\n",
      "Epoch 20/200\n",
      "175341/175341 [==============================] - 222s 1ms/step - loss: 1.3133 - accuracy: 0.5492 - val_loss: 1.2663 - val_accuracy: 0.5969\n",
      "Epoch 21/200\n",
      "175341/175341 [==============================] - 216s 1ms/step - loss: 1.3139 - accuracy: 0.5487 - val_loss: 1.2763 - val_accuracy: 0.5889\n",
      "Epoch 22/200\n",
      "175341/175341 [==============================] - 191s 1ms/step - loss: 1.3135 - accuracy: 0.5493 - val_loss: 1.2689 - val_accuracy: 0.5993\n",
      "Epoch 23/200\n",
      "175341/175341 [==============================] - 212s 1ms/step - loss: 1.3130 - accuracy: 0.5494 - val_loss: 1.2891 - val_accuracy: 0.5850\n",
      "Epoch 24/200\n",
      "175341/175341 [==============================] - 213s 1ms/step - loss: 1.3140 - accuracy: 0.5495 - val_loss: 1.2886 - val_accuracy: 0.5852\n",
      "Epoch 25/200\n",
      "175341/175341 [==============================] - 216s 1ms/step - loss: 1.3128 - accuracy: 0.5491 - val_loss: 1.2791 - val_accuracy: 0.5938\n",
      "Epoch 26/200\n",
      "175341/175341 [==============================] - 217s 1ms/step - loss: 1.3125 - accuracy: 0.5491 - val_loss: 1.2842 - val_accuracy: 0.5870\n",
      "Epoch 27/200\n",
      "175341/175341 [==============================] - 219s 1ms/step - loss: 1.3124 - accuracy: 0.5495 - val_loss: 1.2852 - val_accuracy: 0.5878\n",
      "Epoch 28/200\n",
      "175341/175341 [==============================] - 217s 1ms/step - loss: 1.3113 - accuracy: 0.5496 - val_loss: 1.2827 - val_accuracy: 0.5847\n",
      "Epoch 29/200\n",
      "175341/175341 [==============================] - 216s 1ms/step - loss: 1.3106 - accuracy: 0.5499 - val_loss: 1.2731 - val_accuracy: 0.5872\n",
      "Epoch 30/200\n",
      "175341/175341 [==============================] - 205s 1ms/step - loss: 1.3109 - accuracy: 0.5498 - val_loss: 1.2761 - val_accuracy: 0.5862\n",
      "Epoch 31/200\n",
      "175341/175341 [==============================] - 215s 1ms/step - loss: 1.3103 - accuracy: 0.5501 - val_loss: 1.2670 - val_accuracy: 0.5906\n",
      "Epoch 32/200\n",
      "175341/175341 [==============================] - 209s 1ms/step - loss: 1.3105 - accuracy: 0.5498 - val_loss: 1.2785 - val_accuracy: 0.5846\n",
      "Epoch 33/200\n",
      "175341/175341 [==============================] - 207s 1ms/step - loss: 1.3103 - accuracy: 0.5500 - val_loss: 1.2734 - val_accuracy: 0.5865\n",
      "Epoch 34/200\n",
      "175341/175341 [==============================] - 209s 1ms/step - loss: 1.3102 - accuracy: 0.5501 - val_loss: 1.2782 - val_accuracy: 0.5866\n",
      "Epoch 35/200\n",
      "175341/175341 [==============================] - 207s 1ms/step - loss: 1.3100 - accuracy: 0.5504 - val_loss: 1.2755 - val_accuracy: 0.5865\n",
      "Epoch 36/200\n",
      "175341/175341 [==============================] - 205s 1ms/step - loss: 1.3094 - accuracy: 0.5497 - val_loss: 1.2673 - val_accuracy: 0.5874\n",
      "Epoch 37/200\n",
      "175341/175341 [==============================] - 216s 1ms/step - loss: 1.3100 - accuracy: 0.5500 - val_loss: 1.2865 - val_accuracy: 0.5839\n",
      "Epoch 38/200\n",
      "175341/175341 [==============================] - 213s 1ms/step - loss: 1.3094 - accuracy: 0.5496 - val_loss: 1.2814 - val_accuracy: 0.5875\n",
      "Epoch 39/200\n",
      "175341/175341 [==============================] - 210s 1ms/step - loss: 1.3094 - accuracy: 0.5503 - val_loss: 1.2575 - val_accuracy: 0.5984\n",
      "Epoch 40/200\n",
      "175341/175341 [==============================] - 214s 1ms/step - loss: 1.3089 - accuracy: 0.5499 - val_loss: 1.2762 - val_accuracy: 0.5875\n",
      "Epoch 41/200\n",
      "175341/175341 [==============================] - 204s 1ms/step - loss: 1.3085 - accuracy: 0.5508 - val_loss: 1.2580 - val_accuracy: 0.6028\n",
      "Epoch 42/200\n",
      "175341/175341 [==============================] - 208s 1ms/step - loss: 1.3094 - accuracy: 0.5502 - val_loss: 1.2827 - val_accuracy: 0.5873\n",
      "Epoch 43/200\n",
      "175341/175341 [==============================] - 205s 1ms/step - loss: 1.3087 - accuracy: 0.5499 - val_loss: 1.2891 - val_accuracy: 0.5881\n",
      "Epoch 44/200\n",
      "175341/175341 [==============================] - 210s 1ms/step - loss: 1.3089 - accuracy: 0.5500 - val_loss: 1.2909 - val_accuracy: 0.5909\n",
      "Epoch 45/200\n",
      "175341/175341 [==============================] - 211s 1ms/step - loss: 1.3088 - accuracy: 0.5504 - val_loss: 1.2671 - val_accuracy: 0.5909\n",
      "Epoch 46/200\n",
      "175341/175341 [==============================] - 207s 1ms/step - loss: 1.3084 - accuracy: 0.5507 - val_loss: 1.2816 - val_accuracy: 0.5853\n",
      "Epoch 47/200\n",
      "175341/175341 [==============================] - 208s 1ms/step - loss: 1.3086 - accuracy: 0.5510 - val_loss: 1.2633 - val_accuracy: 0.5891\n",
      "Epoch 48/200\n",
      "175341/175341 [==============================] - 211s 1ms/step - loss: 1.3080 - accuracy: 0.5499 - val_loss: 1.2669 - val_accuracy: 0.5885\n",
      "Epoch 49/200\n",
      "175341/175341 [==============================] - 210s 1ms/step - loss: 1.3081 - accuracy: 0.5504 - val_loss: 1.2723 - val_accuracy: 0.5855\n",
      "Epoch 50/200\n",
      "175341/175341 [==============================] - 210s 1ms/step - loss: 1.3076 - accuracy: 0.5509 - val_loss: 1.2721 - val_accuracy: 0.5878\n",
      "Epoch 51/200\n",
      "175341/175341 [==============================] - 193s 1ms/step - loss: 1.3073 - accuracy: 0.5507 - val_loss: 1.2819 - val_accuracy: 0.5849\n",
      "Epoch 52/200\n",
      "175341/175341 [==============================] - 173s 988us/step - loss: 1.3077 - accuracy: 0.5507 - val_loss: 1.2652 - val_accuracy: 0.5859\n",
      "Epoch 53/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175341/175341 [==============================] - 166s 947us/step - loss: 1.3075 - accuracy: 0.5510 - val_loss: 1.2784 - val_accuracy: 0.5843\n",
      "Epoch 54/200\n",
      "175341/175341 [==============================] - 160s 913us/step - loss: 1.3078 - accuracy: 0.5508 - val_loss: 1.2821 - val_accuracy: 0.5899\n",
      "Epoch 55/200\n",
      "175341/175341 [==============================] - 162s 924us/step - loss: 1.3078 - accuracy: 0.5511 - val_loss: 1.2807 - val_accuracy: 0.5898\n",
      "Epoch 56/200\n",
      "175341/175341 [==============================] - 164s 936us/step - loss: 1.3082 - accuracy: 0.5511 - val_loss: 1.2772 - val_accuracy: 0.5884\n",
      "Epoch 57/200\n",
      "175341/175341 [==============================] - 167s 955us/step - loss: 1.3073 - accuracy: 0.5509 - val_loss: 1.2663 - val_accuracy: 0.5918\n",
      "Epoch 58/200\n",
      "175341/175341 [==============================] - 178s 1ms/step - loss: 1.3069 - accuracy: 0.5503 - val_loss: 1.2738 - val_accuracy: 0.5858\n",
      "Epoch 59/200\n",
      "175341/175341 [==============================] - 172s 981us/step - loss: 1.3060 - accuracy: 0.5509 - val_loss: 1.2864 - val_accuracy: 0.5844\n",
      "Epoch 60/200\n",
      "175341/175341 [==============================] - 155s 883us/step - loss: 1.3054 - accuracy: 0.5511 - val_loss: 1.2821 - val_accuracy: 0.5882\n",
      "Epoch 61/200\n",
      "175341/175341 [==============================] - 166s 944us/step - loss: 1.3054 - accuracy: 0.5513 - val_loss: 1.2870 - val_accuracy: 0.5862\n",
      "Epoch 62/200\n",
      "175341/175341 [==============================] - 165s 943us/step - loss: 1.3060 - accuracy: 0.5512 - val_loss: 1.2852 - val_accuracy: 0.5869\n",
      "Epoch 63/200\n",
      "175341/175341 [==============================] - 163s 929us/step - loss: 1.3061 - accuracy: 0.5510 - val_loss: 1.2844 - val_accuracy: 0.5854\n",
      "Epoch 64/200\n",
      "175341/175341 [==============================] - 166s 947us/step - loss: 1.3048 - accuracy: 0.5516 - val_loss: 1.2830 - val_accuracy: 0.5841\n",
      "Epoch 65/200\n",
      "175341/175341 [==============================] - 173s 986us/step - loss: 1.3060 - accuracy: 0.5504 - val_loss: 1.2761 - val_accuracy: 0.5925\n",
      "Epoch 66/200\n",
      "175341/175341 [==============================] - 164s 934us/step - loss: 1.3050 - accuracy: 0.5508 - val_loss: 1.2982 - val_accuracy: 0.5856\n",
      "Epoch 67/200\n",
      "175341/175341 [==============================] - 163s 932us/step - loss: 1.3054 - accuracy: 0.5515 - val_loss: 1.2928 - val_accuracy: 0.5681\n",
      "Epoch 68/200\n",
      "175341/175341 [==============================] - 154s 876us/step - loss: 1.3048 - accuracy: 0.5510 - val_loss: 1.2598 - val_accuracy: 0.5873\n",
      "Epoch 69/200\n",
      "175341/175341 [==============================] - 153s 871us/step - loss: 1.3047 - accuracy: 0.5513 - val_loss: 1.2703 - val_accuracy: 0.5908\n",
      "Epoch 70/200\n",
      "175341/175341 [==============================] - 153s 871us/step - loss: 1.3041 - accuracy: 0.5505 - val_loss: 1.2708 - val_accuracy: 0.5878\n",
      "Epoch 71/200\n",
      "175341/175341 [==============================] - 155s 884us/step - loss: 1.3051 - accuracy: 0.5503 - val_loss: 1.2786 - val_accuracy: 0.5899\n",
      "Epoch 72/200\n",
      "175341/175341 [==============================] - 156s 890us/step - loss: 1.3040 - accuracy: 0.5506 - val_loss: 1.2605 - val_accuracy: 0.5897\n",
      "Epoch 73/200\n",
      "175341/175341 [==============================] - 153s 872us/step - loss: 1.3038 - accuracy: 0.5510 - val_loss: 1.2991 - val_accuracy: 0.5729\n",
      "Epoch 74/200\n",
      "175341/175341 [==============================] - 154s 879us/step - loss: 1.3029 - accuracy: 0.5515 - val_loss: 1.2621 - val_accuracy: 0.5862\n",
      "Epoch 75/200\n",
      "175341/175341 [==============================] - 154s 876us/step - loss: 1.3021 - accuracy: 0.5510 - val_loss: 1.2691 - val_accuracy: 0.5839\n",
      "Epoch 76/200\n",
      "175341/175341 [==============================] - 153s 875us/step - loss: 1.3026 - accuracy: 0.5515 - val_loss: 1.2852 - val_accuracy: 0.5826\n",
      "Epoch 77/200\n",
      "175341/175341 [==============================] - 153s 873us/step - loss: 1.3020 - accuracy: 0.5509 - val_loss: 1.2709 - val_accuracy: 0.5866\n",
      "Epoch 78/200\n",
      "175341/175341 [==============================] - 154s 879us/step - loss: 1.3020 - accuracy: 0.5505 - val_loss: 1.2821 - val_accuracy: 0.5783\n",
      "Epoch 79/200\n",
      "175341/175341 [==============================] - 151s 859us/step - loss: 1.3011 - accuracy: 0.5504 - val_loss: 1.2896 - val_accuracy: 0.5823\n",
      "Epoch 80/200\n",
      "175341/175341 [==============================] - 154s 877us/step - loss: 1.3011 - accuracy: 0.5512 - val_loss: 1.2845 - val_accuracy: 0.5843\n",
      "Epoch 81/200\n",
      "175341/175341 [==============================] - 154s 880us/step - loss: 1.3013 - accuracy: 0.5511 - val_loss: 1.2777 - val_accuracy: 0.5879\n",
      "Epoch 82/200\n",
      "175341/175341 [==============================] - 155s 881us/step - loss: 1.2999 - accuracy: 0.5503 - val_loss: 1.2725 - val_accuracy: 0.5853\n",
      "Epoch 83/200\n",
      "175341/175341 [==============================] - 151s 860us/step - loss: 1.2994 - accuracy: 0.5507 - val_loss: 1.2738 - val_accuracy: 0.5832\n",
      "Epoch 84/200\n",
      "175341/175341 [==============================] - 155s 882us/step - loss: 1.2990 - accuracy: 0.5512 - val_loss: 1.2829 - val_accuracy: 0.5814\n",
      "Epoch 85/200\n",
      "175341/175341 [==============================] - 154s 876us/step - loss: 1.2986 - accuracy: 0.5514 - val_loss: 1.2867 - val_accuracy: 0.5631\n",
      "Epoch 86/200\n",
      "175341/175341 [==============================] - 150s 854us/step - loss: 1.2980 - accuracy: 0.5511 - val_loss: 1.2699 - val_accuracy: 0.5866\n",
      "Epoch 87/200\n",
      "175341/175341 [==============================] - 153s 874us/step - loss: 1.2979 - accuracy: 0.5513 - val_loss: 1.2872 - val_accuracy: 0.5566\n",
      "Epoch 88/200\n",
      "175341/175341 [==============================] - 153s 873us/step - loss: 1.2971 - accuracy: 0.5512 - val_loss: 1.2678 - val_accuracy: 0.5870\n",
      "Epoch 89/200\n",
      "175341/175341 [==============================] - 154s 877us/step - loss: 1.2974 - accuracy: 0.5507 - val_loss: 1.2805 - val_accuracy: 0.5857\n",
      "Epoch 90/200\n",
      "175341/175341 [==============================] - 152s 868us/step - loss: 1.2967 - accuracy: 0.5514 - val_loss: 1.2640 - val_accuracy: 0.5848\n",
      "Epoch 91/200\n",
      "175341/175341 [==============================] - 152s 869us/step - loss: 1.2960 - accuracy: 0.5513 - val_loss: 1.2826 - val_accuracy: 0.5799\n",
      "Epoch 92/200\n",
      "175341/175341 [==============================] - 152s 869us/step - loss: 1.2949 - accuracy: 0.5512 - val_loss: 1.2800 - val_accuracy: 0.5831\n",
      "Epoch 93/200\n",
      "175341/175341 [==============================] - 152s 864us/step - loss: 1.2952 - accuracy: 0.5514 - val_loss: 1.2549 - val_accuracy: 0.5941\n",
      "Epoch 94/200\n",
      "175341/175341 [==============================] - 152s 868us/step - loss: 1.2943 - accuracy: 0.5512 - val_loss: 1.2808 - val_accuracy: 0.5826\n",
      "Epoch 95/200\n",
      "175341/175341 [==============================] - 153s 872us/step - loss: 1.2950 - accuracy: 0.5511 - val_loss: 1.2684 - val_accuracy: 0.5801\n",
      "Epoch 96/200\n",
      "175341/175341 [==============================] - 151s 860us/step - loss: 1.2939 - accuracy: 0.5514 - val_loss: 1.2579 - val_accuracy: 0.5920\n",
      "Epoch 97/200\n",
      "175341/175341 [==============================] - 153s 875us/step - loss: 1.2939 - accuracy: 0.5515 - val_loss: 1.2519 - val_accuracy: 0.5932\n",
      "Epoch 98/200\n",
      "175341/175341 [==============================] - 151s 859us/step - loss: 1.2934 - accuracy: 0.5502 - val_loss: 1.2759 - val_accuracy: 0.5909\n",
      "Epoch 99/200\n",
      "175341/175341 [==============================] - 150s 857us/step - loss: 1.2927 - accuracy: 0.5514 - val_loss: 1.2639 - val_accuracy: 0.5761\n",
      "Epoch 100/200\n",
      "175341/175341 [==============================] - 152s 867us/step - loss: 1.2927 - accuracy: 0.5512 - val_loss: 1.2704 - val_accuracy: 0.5836\n",
      "Epoch 101/200\n",
      "175341/175341 [==============================] - 152s 867us/step - loss: 1.2922 - accuracy: 0.5516 - val_loss: 1.2685 - val_accuracy: 0.5847\n",
      "Epoch 102/200\n",
      "175341/175341 [==============================] - 152s 865us/step - loss: 1.2927 - accuracy: 0.5510 - val_loss: 1.2683 - val_accuracy: 0.5841\n",
      "Epoch 103/200\n",
      "175341/175341 [==============================] - 150s 856us/step - loss: 1.2924 - accuracy: 0.5517 - val_loss: 1.2781 - val_accuracy: 0.5797\n",
      "Epoch 104/200\n",
      "175341/175341 [==============================] - 151s 863us/step - loss: 1.2922 - accuracy: 0.5509 - val_loss: 1.2628 - val_accuracy: 0.5859\n",
      "Epoch 105/200\n",
      "175341/175341 [==============================] - 148s 841us/step - loss: 1.2926 - accuracy: 0.5507 - val_loss: 1.2792 - val_accuracy: 0.5860\n",
      "Epoch 106/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175341/175341 [==============================] - 152s 865us/step - loss: 1.2931 - accuracy: 0.5511 - val_loss: 1.2715 - val_accuracy: 0.5863\n",
      "Epoch 107/200\n",
      "175341/175341 [==============================] - 152s 868us/step - loss: 1.2919 - accuracy: 0.5503 - val_loss: 1.2787 - val_accuracy: 0.5830\n",
      "Epoch 108/200\n",
      "175341/175341 [==============================] - 152s 869us/step - loss: 1.2920 - accuracy: 0.5512 - val_loss: 1.2674 - val_accuracy: 0.5908\n",
      "Epoch 109/200\n",
      "175341/175341 [==============================] - 148s 845us/step - loss: 1.2915 - accuracy: 0.5502 - val_loss: 1.2834 - val_accuracy: 0.5726\n",
      "Epoch 110/200\n",
      "175341/175341 [==============================] - 153s 872us/step - loss: 1.2923 - accuracy: 0.5508 - val_loss: 1.2735 - val_accuracy: 0.5654\n",
      "Epoch 111/200\n",
      "175341/175341 [==============================] - 150s 856us/step - loss: 1.2914 - accuracy: 0.5514 - val_loss: 1.2634 - val_accuracy: 0.5824\n",
      "Epoch 112/200\n",
      "175341/175341 [==============================] - 150s 855us/step - loss: 1.2915 - accuracy: 0.5514 - val_loss: 1.2554 - val_accuracy: 0.5956\n",
      "Epoch 113/200\n",
      "175341/175341 [==============================] - 151s 861us/step - loss: 1.2920 - accuracy: 0.5511 - val_loss: 1.2786 - val_accuracy: 0.5848\n",
      "Epoch 114/200\n",
      "175341/175341 [==============================] - 150s 858us/step - loss: 1.2915 - accuracy: 0.5508 - val_loss: 1.2734 - val_accuracy: 0.5828\n",
      "Epoch 115/200\n",
      "175341/175341 [==============================] - 151s 861us/step - loss: 1.2915 - accuracy: 0.5501 - val_loss: 1.3128 - val_accuracy: 0.5740\n",
      "Epoch 116/200\n",
      "175341/175341 [==============================] - 150s 855us/step - loss: 1.2904 - accuracy: 0.5509 - val_loss: 1.2787 - val_accuracy: 0.5829\n",
      "Epoch 117/200\n",
      "175341/175341 [==============================] - 151s 861us/step - loss: 1.2920 - accuracy: 0.5496 - val_loss: 1.2731 - val_accuracy: 0.5782\n",
      "Epoch 118/200\n",
      "175341/175341 [==============================] - 150s 856us/step - loss: 1.2901 - accuracy: 0.5511 - val_loss: 1.2922 - val_accuracy: 0.5555\n",
      "Epoch 119/200\n",
      "175341/175341 [==============================] - 152s 865us/step - loss: 1.2900 - accuracy: 0.5504 - val_loss: 1.2833 - val_accuracy: 0.5757\n",
      "Epoch 120/200\n",
      "175341/175341 [==============================] - 152s 868us/step - loss: 1.2906 - accuracy: 0.5509 - val_loss: 1.2807 - val_accuracy: 0.5735\n",
      "Epoch 121/200\n",
      "175341/175341 [==============================] - 152s 869us/step - loss: 1.2903 - accuracy: 0.5511 - val_loss: 1.2515 - val_accuracy: 0.5918\n",
      "Epoch 122/200\n",
      "175341/175341 [==============================] - 148s 846us/step - loss: 1.2937 - accuracy: 0.5497 - val_loss: 1.2758 - val_accuracy: 0.5744\n",
      "Epoch 123/200\n",
      "175341/175341 [==============================] - 153s 870us/step - loss: 1.2886 - accuracy: 0.5510 - val_loss: 1.2687 - val_accuracy: 0.5816\n",
      "Epoch 124/200\n",
      "175341/175341 [==============================] - 151s 861us/step - loss: 1.2906 - accuracy: 0.5502 - val_loss: 1.2641 - val_accuracy: 0.5813\n",
      "Epoch 125/200\n",
      "175341/175341 [==============================] - 151s 860us/step - loss: 1.2891 - accuracy: 0.5510 - val_loss: 1.2641 - val_accuracy: 0.5856\n",
      "Epoch 126/200\n",
      "175341/175341 [==============================] - 152s 864us/step - loss: 1.2902 - accuracy: 0.5508 - val_loss: 1.2694 - val_accuracy: 0.5822\n",
      "Epoch 127/200\n",
      "175341/175341 [==============================] - 152s 866us/step - loss: 1.2892 - accuracy: 0.5500 - val_loss: 1.2596 - val_accuracy: 0.5874\n",
      "Epoch 128/200\n",
      "175341/175341 [==============================] - 151s 862us/step - loss: 1.2919 - accuracy: 0.5508 - val_loss: 1.2726 - val_accuracy: 0.5798\n",
      "Epoch 129/200\n",
      "175341/175341 [==============================] - 151s 859us/step - loss: 1.2908 - accuracy: 0.5498 - val_loss: 1.2593 - val_accuracy: 0.5852\n",
      "Epoch 130/200\n",
      "175341/175341 [==============================] - 152s 865us/step - loss: 1.2909 - accuracy: 0.5500 - val_loss: 1.2727 - val_accuracy: 0.5773\n",
      "Epoch 131/200\n",
      "175341/175341 [==============================] - 149s 850us/step - loss: 1.2899 - accuracy: 0.5508 - val_loss: 1.2604 - val_accuracy: 0.5857\n",
      "Epoch 132/200\n",
      "175341/175341 [==============================] - 152s 865us/step - loss: 1.2886 - accuracy: 0.5500 - val_loss: 1.2796 - val_accuracy: 0.5775\n",
      "Epoch 133/200\n",
      "175341/175341 [==============================] - 152s 868us/step - loss: 1.2884 - accuracy: 0.5504 - val_loss: 1.2728 - val_accuracy: 0.5841\n",
      "Epoch 134/200\n",
      "175341/175341 [==============================] - 152s 866us/step - loss: 1.2887 - accuracy: 0.5504 - val_loss: 1.2730 - val_accuracy: 0.5869\n",
      "Epoch 135/200\n",
      "175341/175341 [==============================] - 149s 852us/step - loss: 1.2892 - accuracy: 0.5507 - val_loss: 1.2754 - val_accuracy: 0.5596\n",
      "Epoch 136/200\n",
      "175341/175341 [==============================] - 153s 871us/step - loss: 1.2889 - accuracy: 0.5500 - val_loss: 1.2579 - val_accuracy: 0.5872\n",
      "Epoch 137/200\n",
      "175341/175341 [==============================] - 148s 843us/step - loss: 1.2897 - accuracy: 0.5501 - val_loss: 1.2769 - val_accuracy: 0.5805\n",
      "Epoch 138/200\n",
      "175341/175341 [==============================] - 152s 866us/step - loss: 1.2887 - accuracy: 0.5503 - val_loss: 1.3022 - val_accuracy: 0.5355\n",
      "Epoch 139/200\n",
      "175341/175341 [==============================] - 153s 870us/step - loss: 1.2888 - accuracy: 0.5499 - val_loss: 1.2786 - val_accuracy: 0.5766\n",
      "Epoch 140/200\n",
      "175341/175341 [==============================] - 152s 867us/step - loss: 1.2878 - accuracy: 0.5508 - val_loss: 1.2665 - val_accuracy: 0.5887\n",
      "Epoch 141/200\n",
      "175341/175341 [==============================] - 151s 864us/step - loss: 1.2888 - accuracy: 0.5507 - val_loss: 1.2759 - val_accuracy: 0.5867\n",
      "Epoch 142/200\n",
      "175341/175341 [==============================] - 151s 861us/step - loss: 1.2890 - accuracy: 0.5505 - val_loss: 1.2752 - val_accuracy: 0.5681\n",
      "Epoch 143/200\n",
      "175341/175341 [==============================] - 152s 867us/step - loss: 1.2889 - accuracy: 0.5499 - val_loss: 1.2674 - val_accuracy: 0.5772\n",
      "Epoch 144/200\n",
      "175341/175341 [==============================] - 149s 848us/step - loss: 1.2874 - accuracy: 0.5493 - val_loss: 1.2748 - val_accuracy: 0.5856\n",
      "Epoch 145/200\n",
      "175341/175341 [==============================] - 152s 867us/step - loss: 1.2884 - accuracy: 0.5497 - val_loss: 1.2900 - val_accuracy: 0.5546\n",
      "Epoch 146/200\n",
      "175341/175341 [==============================] - 153s 871us/step - loss: 1.2884 - accuracy: 0.5506 - val_loss: 1.2782 - val_accuracy: 0.5749\n",
      "Epoch 147/200\n",
      "175341/175341 [==============================] - 152s 865us/step - loss: 1.2878 - accuracy: 0.5505 - val_loss: 1.2623 - val_accuracy: 0.5918\n",
      "Epoch 148/200\n",
      "175341/175341 [==============================] - 149s 849us/step - loss: 1.2877 - accuracy: 0.5503 - val_loss: 1.2806 - val_accuracy: 0.5791\n",
      "Epoch 149/200\n",
      "175341/175341 [==============================] - 157s 894us/step - loss: 1.2882 - accuracy: 0.5501 - val_loss: 1.2756 - val_accuracy: 0.5714\n",
      "Epoch 150/200\n",
      "175341/175341 [==============================] - 153s 871us/step - loss: 1.2880 - accuracy: 0.5504 - val_loss: 1.2629 - val_accuracy: 0.5766\n",
      "Epoch 151/200\n",
      "175341/175341 [==============================] - 162s 924us/step - loss: 1.2882 - accuracy: 0.5501 - val_loss: 1.2985 - val_accuracy: 0.5198\n",
      "Epoch 152/200\n",
      "175341/175341 [==============================] - 160s 913us/step - loss: 1.2879 - accuracy: 0.5496 - val_loss: 1.2631 - val_accuracy: 0.5870\n",
      "Epoch 153/200\n",
      "175341/175341 [==============================] - 161s 917us/step - loss: 1.2880 - accuracy: 0.5505 - val_loss: 1.2672 - val_accuracy: 0.5714\n",
      "Epoch 154/200\n",
      "175341/175341 [==============================] - 160s 910us/step - loss: 1.2871 - accuracy: 0.5497 - val_loss: 1.2656 - val_accuracy: 0.5794\n",
      "Epoch 155/200\n",
      "175341/175341 [==============================] - 160s 913us/step - loss: 1.2871 - accuracy: 0.5508 - val_loss: 1.2664 - val_accuracy: 0.5829\n",
      "Epoch 156/200\n",
      "175341/175341 [==============================] - 158s 901us/step - loss: 1.2884 - accuracy: 0.5497 - val_loss: 1.2630 - val_accuracy: 0.5875\n",
      "Epoch 157/200\n",
      "175341/175341 [==============================] - 157s 893us/step - loss: 1.2876 - accuracy: 0.5500 - val_loss: 1.2818 - val_accuracy: 0.5856\n",
      "Epoch 158/200\n",
      "175341/175341 [==============================] - 161s 916us/step - loss: 1.2885 - accuracy: 0.5497 - val_loss: 1.2653 - val_accuracy: 0.5868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/200\n",
      "175341/175341 [==============================] - 162s 923us/step - loss: 1.2881 - accuracy: 0.5497 - val_loss: 1.2844 - val_accuracy: 0.5622\n",
      "Epoch 160/200\n",
      "175341/175341 [==============================] - 161s 919us/step - loss: 1.2886 - accuracy: 0.5494 - val_loss: 1.2740 - val_accuracy: 0.5745\n",
      "Epoch 161/200\n",
      "175341/175341 [==============================] - 159s 905us/step - loss: 1.2870 - accuracy: 0.5500 - val_loss: 1.2923 - val_accuracy: 0.5413\n",
      "Epoch 162/200\n",
      "175341/175341 [==============================] - 162s 924us/step - loss: 1.2884 - accuracy: 0.5502 - val_loss: 1.2726 - val_accuracy: 0.5830\n",
      "Epoch 163/200\n",
      "175341/175341 [==============================] - 153s 870us/step - loss: 1.2869 - accuracy: 0.5495 - val_loss: 1.2636 - val_accuracy: 0.5911\n",
      "Epoch 164/200\n",
      "175341/175341 [==============================] - 161s 920us/step - loss: 1.2876 - accuracy: 0.5506 - val_loss: 1.2598 - val_accuracy: 0.5921\n",
      "Epoch 165/200\n",
      "175341/175341 [==============================] - 161s 915us/step - loss: 1.2875 - accuracy: 0.5501 - val_loss: 1.2687 - val_accuracy: 0.5824\n",
      "Epoch 166/200\n",
      "175341/175341 [==============================] - 162s 922us/step - loss: 1.2868 - accuracy: 0.5502 - val_loss: 1.2737 - val_accuracy: 0.5875\n",
      "Epoch 167/200\n",
      "175341/175341 [==============================] - 160s 914us/step - loss: 1.2872 - accuracy: 0.5502 - val_loss: 1.2728 - val_accuracy: 0.5897\n",
      "Epoch 168/200\n",
      "175341/175341 [==============================] - 160s 913us/step - loss: 1.2880 - accuracy: 0.5498 - val_loss: 1.2706 - val_accuracy: 0.5748\n",
      "Epoch 169/200\n",
      "175341/175341 [==============================] - 155s 886us/step - loss: 1.2887 - accuracy: 0.5504 - val_loss: 1.2739 - val_accuracy: 0.5831\n",
      "Epoch 170/200\n",
      "175341/175341 [==============================] - 158s 901us/step - loss: 1.2883 - accuracy: 0.5498 - val_loss: 1.2935 - val_accuracy: 0.5764\n",
      "Epoch 171/200\n",
      "175341/175341 [==============================] - 160s 915us/step - loss: 1.2876 - accuracy: 0.5496 - val_loss: 1.2844 - val_accuracy: 0.5841\n",
      "Epoch 172/200\n",
      "175341/175341 [==============================] - 161s 920us/step - loss: 1.2894 - accuracy: 0.5495 - val_loss: 1.2735 - val_accuracy: 0.5794\n",
      "Epoch 173/200\n",
      "175341/175341 [==============================] - 158s 899us/step - loss: 1.2873 - accuracy: 0.5493 - val_loss: 1.2548 - val_accuracy: 0.5916\n",
      "Epoch 174/200\n",
      "175341/175341 [==============================] - 156s 887us/step - loss: 1.2910 - accuracy: 0.5489 - val_loss: 1.2682 - val_accuracy: 0.5884\n",
      "Epoch 175/200\n",
      "175341/175341 [==============================] - 158s 899us/step - loss: 1.2870 - accuracy: 0.5499 - val_loss: 1.2721 - val_accuracy: 0.5772\n",
      "Epoch 176/200\n",
      "175341/175341 [==============================] - 152s 865us/step - loss: 1.2871 - accuracy: 0.5488 - val_loss: 1.2581 - val_accuracy: 0.5872\n",
      "Epoch 177/200\n",
      "175341/175341 [==============================] - 158s 903us/step - loss: 1.2868 - accuracy: 0.5492 - val_loss: 1.2929 - val_accuracy: 0.5472\n",
      "Epoch 178/200\n",
      "175341/175341 [==============================] - 158s 902us/step - loss: 1.2871 - accuracy: 0.5493 - val_loss: 1.2436 - val_accuracy: 0.5991\n",
      "Epoch 179/200\n",
      "175341/175341 [==============================] - 159s 906us/step - loss: 1.2882 - accuracy: 0.5487 - val_loss: 1.2916 - val_accuracy: 0.5483\n",
      "Epoch 180/200\n",
      "175341/175341 [==============================] - 157s 895us/step - loss: 1.2877 - accuracy: 0.5491 - val_loss: 1.2869 - val_accuracy: 0.5785\n",
      "Epoch 181/200\n",
      "175341/175341 [==============================] - 157s 896us/step - loss: 1.2875 - accuracy: 0.5495 - val_loss: 1.2696 - val_accuracy: 0.5919\n",
      "Epoch 182/200\n",
      "175341/175341 [==============================] - 151s 862us/step - loss: 1.2859 - accuracy: 0.5495 - val_loss: 1.2808 - val_accuracy: 0.5556\n",
      "Epoch 183/200\n",
      "175341/175341 [==============================] - 157s 894us/step - loss: 1.2875 - accuracy: 0.5498 - val_loss: 1.2697 - val_accuracy: 0.5678\n",
      "Epoch 184/200\n",
      "175341/175341 [==============================] - 159s 905us/step - loss: 1.2890 - accuracy: 0.5491 - val_loss: 1.2800 - val_accuracy: 0.5830\n",
      "Epoch 185/200\n",
      "175341/175341 [==============================] - 159s 905us/step - loss: 1.2868 - accuracy: 0.5492 - val_loss: 1.2749 - val_accuracy: 0.5845\n",
      "Epoch 186/200\n",
      "175341/175341 [==============================] - 159s 906us/step - loss: 1.2867 - accuracy: 0.5497 - val_loss: 1.2436 - val_accuracy: 0.5925\n",
      "Epoch 187/200\n",
      "175341/175341 [==============================] - 156s 892us/step - loss: 1.2873 - accuracy: 0.5494 - val_loss: 1.2705 - val_accuracy: 0.5853\n",
      "Epoch 188/200\n",
      "175341/175341 [==============================] - 158s 900us/step - loss: 1.2887 - accuracy: 0.5495 - val_loss: 1.2742 - val_accuracy: 0.5717\n",
      "Epoch 189/200\n",
      "175341/175341 [==============================] - 151s 863us/step - loss: 1.2874 - accuracy: 0.5499 - val_loss: 1.2680 - val_accuracy: 0.5819\n",
      "Epoch 190/200\n",
      "175341/175341 [==============================] - 159s 904us/step - loss: 1.2864 - accuracy: 0.5499 - val_loss: 1.2889 - val_accuracy: 0.5730\n",
      "Epoch 191/200\n",
      "175341/175341 [==============================] - 159s 906us/step - loss: 1.2876 - accuracy: 0.5494 - val_loss: 1.2549 - val_accuracy: 0.5887\n",
      "Epoch 192/200\n",
      "175341/175341 [==============================] - 158s 899us/step - loss: 1.2874 - accuracy: 0.5498 - val_loss: 1.2637 - val_accuracy: 0.5922\n",
      "Epoch 193/200\n",
      "175341/175341 [==============================] - 158s 902us/step - loss: 1.2884 - accuracy: 0.5492 - val_loss: 1.2530 - val_accuracy: 0.5977\n",
      "Epoch 194/200\n",
      "175341/175341 [==============================] - 157s 896us/step - loss: 1.2868 - accuracy: 0.5501 - val_loss: 1.2855 - val_accuracy: 0.5533\n",
      "Epoch 195/200\n",
      "175341/175341 [==============================] - 150s 858us/step - loss: 1.2875 - accuracy: 0.5494 - val_loss: 1.2824 - val_accuracy: 0.5878\n",
      "Epoch 196/200\n",
      "175341/175341 [==============================] - 156s 889us/step - loss: 1.2873 - accuracy: 0.5499 - val_loss: 1.2739 - val_accuracy: 0.5723\n",
      "Epoch 197/200\n",
      "175341/175341 [==============================] - 155s 883us/step - loss: 1.2876 - accuracy: 0.5504 - val_loss: 1.2882 - val_accuracy: 0.5445\n",
      "Epoch 198/200\n",
      "175341/175341 [==============================] - 156s 892us/step - loss: 1.2867 - accuracy: 0.5493 - val_loss: 1.2631 - val_accuracy: 0.5899\n",
      "Epoch 199/200\n",
      "175341/175341 [==============================] - 157s 895us/step - loss: 1.2866 - accuracy: 0.5496 - val_loss: 1.2703 - val_accuracy: 0.5843\n",
      "Epoch 200/200\n",
      "175341/175341 [==============================] - 152s 870us/step - loss: 1.2878 - accuracy: 0.5489 - val_loss: 1.2539 - val_accuracy: 0.5872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f3f621b0b90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(XDelta1, Y, epochs=200, validation_data=(xdelta1, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predictions = cnn.predict([xdelta1])\n",
    "\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "#pred = np.zeros((82332,1), dtype=int)\n",
    "for i in range(0, predictions.shape[0]):\n",
    "    pred = np.where(predictions[i] == np.amax(predictions[i]))\n",
    "    for j in range(0,10):\n",
    "        predictions[i,j] = 0\n",
    "    predictions[i,pred]=1\n",
    "    \n",
    "print(predictions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5872078900063159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritika/.virtualenvs/pythree/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       677\n",
      "           1       0.33      0.00      0.01       583\n",
      "           2       0.79      0.01      0.01      4089\n",
      "           3       0.28      0.33      0.30     11132\n",
      "           4       0.46      0.01      0.01      6062\n",
      "           5       0.55      0.97      0.70     18871\n",
      "           6       0.74      0.71      0.73     37000\n",
      "           7       0.00      0.00      0.00      3496\n",
      "           8       0.00      0.00      0.00       378\n",
      "           9       0.00      0.00      0.00        44\n",
      "\n",
      "   micro avg       0.59      0.59      0.59     82332\n",
      "   macro avg       0.31      0.20      0.18     82332\n",
      "weighted avg       0.57      0.59      0.53     82332\n",
      " samples avg       0.59      0.59      0.59     82332\n",
      "\n",
      "[[    0     0     0    45     0   602    30     0     0     0]\n",
      " [    0     2     0    63     0   486    32     0     0     0]\n",
      " [    0     0    22   756     7  2649   655     0     0     0]\n",
      " [    0     1     3  3665     0  2902  4561     0     0     0]\n",
      " [    0     0     0  1593    33  2199  2237     0     0     0]\n",
      " [    0     0     2   262    11 18287   309     0     0     0]\n",
      " [    0     3     1  5989    21  4649 26337     0     0     0]\n",
      " [    0     0     0   848     0  1570  1078     0     0     0]\n",
      " [    0     0     0    79     0   185   114     0     0     0]\n",
      " [    0     0     0    12     0     6    26     0     0     0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Accuracy: \",accuracy_score(y,predictions))\n",
    "from sklearn.metrics import multilabel_confusion_matrix,classification_report,confusion_matrix\n",
    "print(\"Classification Report: \\n\",classification_report(y,predictions))\n",
    "conmat = np.array(confusion_matrix(y.argmax(axis=1), predictions.argmax(axis=1)))\n",
    "confusion = pd.DataFrame(conmat, index=['0','1','2','3','4','5','6','7','8','9'],\n",
    "                        columns=[['0','1','2','3','4','5','6','7','8','9']])\n",
    "print (conmat)\n",
    "conmat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
